Prerequisites
-------------
1) Boost (libbost<>-dev package)
2) Boost threads (libbost-thread-dev)
3) Thrift source installation (unlike Java, C++ needs extra header files).
Download this from Apache, then build. Requires Boost and python<version>-dev.
4) cmake
5) Hadoop
$HADOO_HOME must be set (e.g. mine is /home/abehm/hive/build/hadoopcore/hadoop-0.20.2-cdh3u1-SNAPSHOT).
To run an exec linked against libhdfs, the CLASSPATH must include 
all jars in $HADOOP_HOME and $HADOOP_HOME/lib, 
as well as the path with hdfs-site.xml.
6) Google command line flags library (gflags):
download the tarball from http://code.google.com/p/google-gflags, then install as per instructions. 
Static linking:
To enable linking the static library into shared libraries, you must compile the package with -fPIC.
You need to manually add -fPIC to the CXXFLAGS in the Makefile (line 318 in my version 0.3.1).
7) Google logging library (glog):
This depends on gflags; download from download link accessible from
http://code.google.com/p/google-glog, then install as per instructions.
Static linking:
To statically link this library, you must also statically link the gflags library.
To enable linking the static library into shared libraries, you must compile the package with -fPIC.
You need to manually add -fPIC to the CXXFLAGS in the Makefile (line 191 in my version 1.5).

Building
--------
1) You first need to generate the thrift structures, which are written into be/generated-sources/gen-cpp.  To do that:
	cd impala/fe
	mvn compile

2) Run 'cmake .' from impala/be; this will generated makefiles

3) From impala/be: make

Debugging
---------
1) To run the backend under gdb, start the runquery binary (build/service/runquery),
passing the query string as a parameter.
2) This binary uses a mock hdfs implementation that can only read local files and
requires that a version of the front be run as a demon. Run fe/bin/runplanservice
to start that demon process.
