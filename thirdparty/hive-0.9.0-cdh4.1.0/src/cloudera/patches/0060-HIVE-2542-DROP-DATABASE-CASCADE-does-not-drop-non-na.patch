From dba72bab06da619574593d72ec9780e031c2b539 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Fri, 18 May 2012 15:32:52 +0000
Subject: [PATCH 060/148] HIVE-2542 : DROP DATABASE CASCADE does not drop non-native tables ( Vandana Ayyalasomayajula via Ashutosh Chauhan)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1340130 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 4d8e438afac4292020e370543c4f951df89dae37)
---
 hbase-handler/build.xml                            |   17 +-
 .../src/test/queries/external_table_ppd.q          |   37 -
 .../queries/hbase_binary_external_table_queries.q  |   41 -
 .../src/test/queries/hbase_binary_map_queries.q    |  225 -----
 .../test/queries/hbase_binary_storage_queries.q    |  218 -----
 hbase-handler/src/test/queries/hbase_bulk.m        |   57 --
 hbase-handler/src/test/queries/hbase_joins.q       |   82 --
 .../src/test/queries/hbase_ppd_key_range.q         |   71 --
 hbase-handler/src/test/queries/hbase_pushdown.q    |   53 --
 hbase-handler/src/test/queries/hbase_queries.q     |  160 ----
 hbase-handler/src/test/queries/hbase_stats.q       |   30 -
 hbase-handler/src/test/queries/hbase_stats2.q      |   31 -
 .../src/test/queries/positive/external_table_ppd.q |   37 +
 .../positive/hbase_binary_external_table_queries.q |   41 +
 .../queries/positive/hbase_binary_map_queries.q    |  225 +++++
 .../positive/hbase_binary_storage_queries.q        |  218 +++++
 .../src/test/queries/positive/hbase_bulk.m         |   57 ++
 .../src/test/queries/positive/hbase_joins.q        |   82 ++
 .../test/queries/positive/hbase_ppd_key_range.q    |   71 ++
 .../src/test/queries/positive/hbase_pushdown.q     |   53 ++
 .../src/test/queries/positive/hbase_queries.q      |  160 ++++
 .../src/test/queries/positive/hbase_stats.q        |   30 +
 .../src/test/queries/positive/hbase_stats2.q       |   31 +
 .../src/test/queries/positive/ppd_key_ranges.q     |   22 +
 hbase-handler/src/test/queries/ppd_key_ranges.q    |   22 -
 .../src/test/results/external_table_ppd.q.out      |  186 ----
 .../hbase_binary_external_table_queries.q.out      |  131 ---
 .../test/results/hbase_binary_map_queries.q.out    |  869 -----------------
 .../results/hbase_binary_storage_queries.q.out     |  658 -------------
 hbase-handler/src/test/results/hbase_bulk.m.out    |  131 ---
 hbase-handler/src/test/results/hbase_joins.q.out   |  267 ------
 .../src/test/results/hbase_ppd_key_range.q.out     |  651 -------------
 .../src/test/results/hbase_pushdown.q.out          |  403 --------
 hbase-handler/src/test/results/hbase_queries.q.out |  980 --------------------
 hbase-handler/src/test/results/hbase_stats.q.out   |  390 --------
 hbase-handler/src/test/results/hbase_stats2.q.out  |  390 --------
 .../test/results/positive/external_table_ppd.q.out |  186 ++++
 .../hbase_binary_external_table_queries.q.out      |  131 +++
 .../positive/hbase_binary_map_queries.q.out        |  869 +++++++++++++++++
 .../positive/hbase_binary_storage_queries.q.out    |  658 +++++++++++++
 .../src/test/results/positive/hbase_bulk.m.out     |  131 +++
 .../src/test/results/positive/hbase_joins.q.out    |  267 ++++++
 .../results/positive/hbase_ppd_key_range.q.out     |  651 +++++++++++++
 .../src/test/results/positive/hbase_pushdown.q.out |  403 ++++++++
 .../src/test/results/positive/hbase_queries.q.out  |  980 ++++++++++++++++++++
 .../src/test/results/positive/hbase_stats.q.out    |  390 ++++++++
 .../src/test/results/positive/hbase_stats2.q.out   |  390 ++++++++
 .../src/test/results/positive/ppd_key_ranges.q.out |  251 +++++
 .../src/test/results/ppd_key_ranges.q.out          |  251 -----
 .../hadoop/hive/metastore/HiveMetaStoreClient.java |    7 +
 50 files changed, 6354 insertions(+), 6338 deletions(-)
 delete mode 100644 hbase-handler/src/test/queries/external_table_ppd.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_binary_external_table_queries.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_binary_map_queries.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_binary_storage_queries.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_bulk.m
 delete mode 100644 hbase-handler/src/test/queries/hbase_joins.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_ppd_key_range.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_pushdown.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_queries.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_stats.q
 delete mode 100644 hbase-handler/src/test/queries/hbase_stats2.q
 create mode 100644 hbase-handler/src/test/queries/positive/external_table_ppd.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_binary_external_table_queries.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_binary_map_queries.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_binary_storage_queries.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_bulk.m
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_joins.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_ppd_key_range.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_pushdown.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_queries.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_stats.q
 create mode 100644 hbase-handler/src/test/queries/positive/hbase_stats2.q
 create mode 100644 hbase-handler/src/test/queries/positive/ppd_key_ranges.q
 delete mode 100644 hbase-handler/src/test/queries/ppd_key_ranges.q
 delete mode 100644 hbase-handler/src/test/results/external_table_ppd.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_binary_external_table_queries.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_binary_map_queries.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_binary_storage_queries.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_bulk.m.out
 delete mode 100644 hbase-handler/src/test/results/hbase_joins.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_ppd_key_range.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_pushdown.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_queries.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_stats.q.out
 delete mode 100644 hbase-handler/src/test/results/hbase_stats2.q.out
 create mode 100644 hbase-handler/src/test/results/positive/external_table_ppd.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_binary_external_table_queries.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_binary_map_queries.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_binary_storage_queries.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_bulk.m.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_joins.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_ppd_key_range.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_pushdown.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_queries.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_stats.q.out
 create mode 100644 hbase-handler/src/test/results/positive/hbase_stats2.q.out
 create mode 100644 hbase-handler/src/test/results/positive/ppd_key_ranges.q.out
 delete mode 100644 hbase-handler/src/test/results/ppd_key_ranges.q.out

diff --git a/src/hbase-handler/build.xml b/src/hbase-handler/build.xml
index 048a29e..8676ca3 100644
--- a/src/hbase-handler/build.xml
+++ b/src/hbase-handler/build.xml
@@ -49,22 +49,31 @@
 
     <qtestgen outputDirectory="${test.build.src}/org/apache/hadoop/hive/cli" 
               templatePath="${ql.hbase.test.template.dir}" template="TestHBaseCliDriver.vm" 
-              queryDirectory="${hbase-handler.test.query.dir}" 
+              queryDirectory="${hbase-handler.test.query.dir}/positive" 
               queryFile="${qfile}"
               runDisabled="${run_disabled}"
               clusterMode="${clustermode}"
-              resultsDirectory="${hbase-handler.test.results.dir}" className="TestHBaseCliDriver"
+              resultsDirectory="${hbase-handler.test.results.dir}/positive" className="TestHBaseCliDriver"
               logFile="${test.log.dir}/testhbaseclidrivergen.log"
               logDirectory="${test.log.dir}/hbase-handler"/>
     <qtestgen outputDirectory="${test.build.src}/org/apache/hadoop/hive/cli" 
               templatePath="${ql.hbase.test.template.dir}" template="TestHBaseCliDriver.vm" 
-              queryDirectory="${hbase-handler.test.query.dir}" 
+              queryDirectory="${hbase-handler.test.query.dir}/positive" 
               queryFile="hbase_bulk.m"
               runDisabled="${run_disabled}"
               clusterMode="miniMR"
-              resultsDirectory="${hbase-handler.test.results.dir}" className="TestHBaseMinimrCliDriver"
+              resultsDirectory="${hbase-handler.test.results.dir}/positive" className="TestHBaseMinimrCliDriver"
               logFile="${test.log.dir}/testhbaseminimrclidrivergen.log"
               logDirectory="${test.log.dir}/hbase-handler"/>
+  <qtestgen outputDirectory="${test.build.src}/org/apache/hadoop/hive/cli"
+              templatePath="${ql.hbase.test.template.dir}" template="TestHBaseNegativeCliDriver.vm"
+              queryDirectory="${hbase-handler.test.query.dir}/negative"
+              queryFile="${qfile}"
+              runDisabled="${run_disabled}"
+              clusterMode="${clustermode}"
+              resultsDirectory="${hbase-handler.test.results.dir}/negative" className="TestHBaseNegativeCliDriver"
+              logFile="${test.log.dir}/testhbasenegativeclidrivergen.log"
+              logDirectory="${test.log.dir}/hbase-handler"/>
   </target>
 
 </project>
diff --git a/src/hbase-handler/src/test/queries/external_table_ppd.q b/src/hbase-handler/src/test/queries/external_table_ppd.q
deleted file mode 100644
index fbef4bb..0000000
--- a/src/hbase-handler/src/test/queries/external_table_ppd.q
+++ /dev/null
@@ -1,37 +0,0 @@
-DROP TABLE t_hbase;
-
-CREATE TABLE t_hbase(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive",
-               "hbase.table.default.storage.type" = "binary");
-
-DESCRIBE FORMATTED t_hbase;
-
-INSERT OVERWRITE TABLE t_hbase
-SELECT 'user1', 1, 11, 10, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-INSERT OVERWRITE TABLE t_hbase
-SELECT 'user2', 127, 327, 2147, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-INSERT OVERWRITE TABLE t_hbase
-SELECT 'user3', -128, -327, -214748, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-explain SELECT * FROM t_hbase where int_col > 0;
-SELECT * FROM t_hbase where int_col > 0;
-
-DROP TABLE t_hbase;
-
diff --git a/src/hbase-handler/src/test/queries/hbase_binary_external_table_queries.q b/src/hbase-handler/src/test/queries/hbase_binary_external_table_queries.q
deleted file mode 100644
index f7a1218..0000000
--- a/src/hbase-handler/src/test/queries/hbase_binary_external_table_queries.q
+++ /dev/null
@@ -1,41 +0,0 @@
-DROP TABLE t_ext_hbase_1;
-
-CREATE EXTERNAL TABLE t_ext_hbase_1
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
-TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable");
-
-SELECT * FROM t_ext_hbase_1;
-
-DROP TABLE t_ext_hbase_1;
-DROP TABLE t_ext_hbase_2;
-
-CREATE EXTERNAL TABLE t_ext_hbase_2
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:cq-boolean#b,cf:cq-byte#b,cf:cq-short#b,cf:cq-int#b,cf:cq-long#b,cf:cq-string#b,cf:cq-float#b,cf:cq-double#b")
-TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable");
-
-SELECT * FROM t_ext_hbase_2;
-
-DROP TABLE t_ext_hbase_2;
-DROP TABLE t_ext_hbase_3;
-
-CREATE EXTERNAL TABLE t_ext_hbase_3
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
-TBLPROPERTIES (
-"hbase.table.name" = "HiveExternalTable",
-"hbase.table.default.storage.type" = "binary");
-
-SELECT * from t_ext_hbase_3;
-
---HIVE-2958
-SELECT c_int, count(*) FROM t_ext_hbase_3 GROUP BY c_int;
-
-DROP table t_ext_hbase_3;
diff --git a/src/hbase-handler/src/test/queries/hbase_binary_map_queries.q b/src/hbase-handler/src/test/queries/hbase_binary_map_queries.q
deleted file mode 100644
index 255a2c7..0000000
--- a/src/hbase-handler/src/test/queries/hbase_binary_map_queries.q
+++ /dev/null
@@ -1,225 +0,0 @@
-DROP TABLE hbase_src;
-
-CREATE TABLE hbase_src(key STRING,
-                       tinyint_col TINYINT,
-                       smallint_col SMALLINT,
-                       int_col INT,
-                       bigint_col BIGINT,
-                       float_col FLOAT,
-                       double_col DOUBLE,
-                       string_col STRING);
-
-INSERT OVERWRITE TABLE hbase_src
-  SELECT key, key, key, key, key, key, key, value
-  FROM src
-  WHERE key = 125 OR key = 126 OR key = 127;
-
-DROP TABLE t_hbase_maps;
-
-CREATE TABLE t_hbase_maps(key STRING,
-                          tinyint_map_col MAP<TINYINT, TINYINT>,
-                          smallint_map_col MAP<SMALLINT, SMALLINT>,
-                          int_map_col MAP<INT, INT>,
-                          bigint_map_col MAP<BIGINT, BIGINT>,
-                          float_map_col MAP<FLOAT, FLOAT>,
-                          double_map_col MAP<DOUBLE, DOUBLE>,
-                          string_map_col MAP<STRING, STRING>,
-                          boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps");
-
-INSERT OVERWRITE TABLE t_hbase_maps
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(true, true)
-  FROM hbase_src
-  WHERE key = 125;
-
-INSERT OVERWRITE TABLE t_hbase_maps
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(false, false)
-  FROM hbase_src
-  WHERE key = 126;
-
-SELECT * FROM t_hbase_maps ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps;
-
-CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
-                                       tinyint_map_col MAP<TINYINT, TINYINT>,
-                                       smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                       int_map_col MAP<INT, INT>,
-                                       bigint_map_col MAP<BIGINT, BIGINT>,
-                                       float_map_col MAP<FLOAT, FLOAT>,
-                                       double_map_col MAP<DOUBLE, DOUBLE>,
-                                       string_map_col MAP<STRING, STRING>,
-                                       boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps");
-
-SELECT * FROM t_ext_hbase_maps ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps;
-
-DROP TABLE t_ext_hbase_maps_1;
-
-CREATE EXTERNAL TABLE t_ext_hbase_maps_1(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps");
-
-SELECT * FROM t_ext_hbase_maps_1 ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps_1;
-
-DROP TABLE t_ext_hbase_maps_2;
-
-CREATE EXTERNAL TABLE t_ext_hbase_maps_2(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES (
-"hbase.table.name"="t_hive_maps",
-"hbase.table.default.storage.type"="binary");
-
-SELECT * FROM t_ext_hbase_maps_2 ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps_2;
-
-DROP TABLE t_hbase_maps_1;
-
-CREATE TABLE t_hbase_maps_1(key STRING,
-                            tinyint_map_col MAP<TINYINT, TINYINT>,
-                            smallint_map_col MAP<SMALLINT, SMALLINT>,
-                            int_map_col MAP<INT, INT>,
-                            bigint_map_col MAP<BIGINT, BIGINT>,
-                            float_map_col MAP<FLOAT, FLOAT>,
-                            double_map_col MAP<DOUBLE, DOUBLE>,
-                            string_map_col MAP<STRING, STRING>,
-                            boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#b:b,cf-smallint:#b:b,cf-int:#b:b,cf-bigint:#b:b,cf-float:#b:b,cf-double:#b:b,cf-string:#b:b,cf-boolean:#b:b")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");
-
-INSERT OVERWRITE TABLE t_hbase_maps_1
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(true, true)
-  FROM hbase_src
-  WHERE key = 125;
-
-INSERT OVERWRITE TABLE t_hbase_maps_1
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(false, false)
-  FROM hbase_src
-  WHERE key = 126;
-
-SELECT * FROM t_hbase_maps_1 ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps_3;
-
-CREATE EXTERNAL TABLE t_ext_hbase_maps_3(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");
-
-SELECT * FROM t_ext_hbase_maps_3 ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps_3;
-
-DROP TABLE t_ext_hbase_maps_4;
-
-CREATE EXTERNAL TABLE t_ext_hbase_maps_4(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");
-
-SELECT * FROM t_ext_hbase_maps_4 ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps_4;
-
-DROP TABLE t_ext_hbase_maps_5;
-
-CREATE EXTERNAL TABLE t_ext_hbase_maps_5(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES (
-"hbase.table.name"="t_hive_maps_1",
-"hbase.table.default.storage.type"="binary");
-
-SELECT * FROM t_ext_hbase_maps_5 ORDER BY key;
-
-DROP TABLE t_ext_hbase_maps_5;
-
-DROP TABLE t_hbase_maps_1;
-
-DROP TABLE t_hbase_maps;
-
-DROP TABLE hbase_src;
diff --git a/src/hbase-handler/src/test/queries/hbase_binary_storage_queries.q b/src/hbase-handler/src/test/queries/hbase_binary_storage_queries.q
deleted file mode 100644
index b048871..0000000
--- a/src/hbase-handler/src/test/queries/hbase_binary_storage_queries.q
+++ /dev/null
@@ -1,218 +0,0 @@
-DROP TABLE t_hbase;
-
-CREATE TABLE t_hbase(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive",
-               "hbase.table.default.storage.type" = "binary");
-
-DESCRIBE FORMATTED t_hbase;
-
-INSERT OVERWRITE TABLE t_hbase
-SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-INSERT OVERWRITE TABLE t_hbase
-SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-INSERT OVERWRITE TABLE t_hbase
-SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-SELECT * FROM t_hbase;
-
-SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase
-WHERE key='user1' OR key='user2' OR key='user3';
-
-SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase;
-
-DROP TABLE t_hbase_1;
-
-CREATE EXTERNAL TABLE t_hbase_1(key STRING,
-                                tinyint_col TINYINT,
-                                smallint_col SMALLINT,
-                                int_col INT,
-                                bigint_col BIGINT,
-                                float_col FLOAT,
-                                double_col DOUBLE,
-                                boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
-TBLPROPERTIES ("hbase.table.name" = "t_hive");
-
-DESCRIBE FORMATTED t_hbase_1;
-
-SELECT * FROM t_hbase_1;
-
-SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_1
-WHERE key='user1' OR key='user2' OR key='user3';
-
-SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_1;
-
-DROP TABLE t_hbase_1;
-DROP TABLE t_hbase;
-DROP TABLE t_hbase_2;
-
-CREATE TABLE t_hbase_2(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive_2");
-
-INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126;
-
-SELECT * FROM t_hbase_2;
-
-SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_2
-WHERE key='user1' OR key='user2' OR key='user3';
-
-SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_2;
-
-DROP TABLE t_hbase_3;
-
-CREATE EXTERNAL TABLE t_hbase_3(key STRING,
-                                tinyint_col TINYINT,
-                                smallint_col SMALLINT,
-                                int_col INT,
-                                bigint_col BIGINT,
-                                float_col FLOAT,
-                                double_col DOUBLE,
-                                boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
-TBLPROPERTIES ("hbase.table.name" = "t_hive_2");
-
-SELECT * FROM t_hbase_3;
-
-SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_3
-WHERE key='user1' OR key='user2' OR key='user3';
-
-SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_3;
-
-DROP TABLE t_hbase_3;
-
-DROP TABLE t_hbase_4;
-
-CREATE EXTERNAL TABLE t_hbase_4(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES (
-"hbase.table.name" = "t_hive_2",
-"hbase.table.default.storage.type" = "binary");
-
-SELECT * FROM t_hbase_4;
-
-SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_4
-WHERE key='user1' OR key='user2' OR key='user3';
-
-SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_4;
-
-DROP TABLE t_hbase_4;
-DROP TABLE t_hbase_2;
diff --git a/src/hbase-handler/src/test/queries/hbase_bulk.m b/src/hbase-handler/src/test/queries/hbase_bulk.m
deleted file mode 100644
index 22683d9..0000000
--- a/src/hbase-handler/src/test/queries/hbase_bulk.m
+++ /dev/null
@@ -1,57 +0,0 @@
-drop table hbsort;
-drop table hbpartition;
-
-set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
-
--- this is a dummy table used for controlling how the HFiles are
--- created
-create table hbsort(key string, val string, val2 string)
-stored as
-INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
-OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
-TBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf');
-
--- this is a dummy table used for controlling how the input file
--- for TotalOrderPartitioner is created
-create table hbpartition(part_break string)
-row format serde
-'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
-stored as
-inputformat
-'org.apache.hadoop.mapred.TextInputFormat'
-outputformat
-'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat';
-
--- this should produce one file, but we do not
--- know what it will be called, so we will copy it to a well known
--- filename /tmp/hbpartition.lst
-insert overwrite table hbpartition
-select distinct value
-from src
-where value='val_100' or value='val_200';
-
-dfs -cp /build/ql/test/data/warehouse/hbpartition/* /tmp/hbpartition.lst;
-
-set mapred.reduce.tasks=3;
-set hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
-set total.order.partitioner.natural.order=false;
-set total.order.partitioner.path=/tmp/hbpartition.lst;
-
--- this should produce three files in /tmp/hbsort/cf
--- include some trailing blanks and nulls to make sure we handle them correctly
-insert overwrite table hbsort
-select distinct value,
-  case when key=103 then cast(null as string) else key end,
-  case when key=103 then ''
-       else cast(key+1 as string) end
-from src
-cluster by value;
-
-
--- To get the files out to your local filesystem for loading into
--- HBase, run mkdir -p /tmp/blah/cf, then uncomment and
--- semicolon-terminate the line below before running this test:
--- dfs -copyToLocal /tmp/hbsort/cf/* /tmp/blah/cf
-
-drop table hbsort;
-drop table hbpartition;
diff --git a/src/hbase-handler/src/test/queries/hbase_joins.q b/src/hbase-handler/src/test/queries/hbase_joins.q
deleted file mode 100644
index 1616adc..0000000
--- a/src/hbase-handler/src/test/queries/hbase_joins.q
+++ /dev/null
@@ -1,82 +0,0 @@
-DROP TABLE users;
-DROP TABLE states;
-DROP TABLE countries;
-DROP TABLE users_level;
-
--- From HIVE-1257
-
-CREATE TABLE users(key string, state string, country string, country_id int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "info:state,info:country,info:country_id"
-);
-
-CREATE TABLE states(key string, name string)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "state:name"
-);
-
-CREATE TABLE countries(key string, name string, country string, country_id int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "info:name,info:country,info:country_id"
-);
-
-INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
-FROM src WHERE key=100;
-
-INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
-FROM src WHERE key=100;
-
-INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
-FROM src WHERE key=100;
-
-set hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat;
-
-SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country = c.key);
-
-SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
-ON (u.country = c.country);
-
-SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country_id = c.country_id);
-
-SELECT u.key, u.state, s.name FROM users u JOIN states s 
-ON (u.state = s.key);
-
-set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
-
-SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country = c.key);
-
-SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
-ON (u.country = c.country);
-
-SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country_id = c.country_id);
-
-SELECT u.key, u.state, s.name FROM users u JOIN states s 
-ON (u.state = s.key);
-
-DROP TABLE users;
-DROP TABLE states;
-DROP TABLE countries;
-
-CREATE TABLE users(key int, userid int, username string, created int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:nickname,f:created");
-
-CREATE TABLE users_level(key int, userid int, level int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:level");
-
--- HIVE-1903:  the problem fixed here showed up even without any data,
--- so no need to load any to test it
-SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num 
- FROM users JOIN users_level ON (users.userid = users_level.userid) 
- GROUP BY year(from_unixtime(users.created)), level;
-
-DROP TABLE users;
-DROP TABLE users_level;
diff --git a/src/hbase-handler/src/test/queries/hbase_ppd_key_range.q b/src/hbase-handler/src/test/queries/hbase_ppd_key_range.q
deleted file mode 100644
index 59e724d..0000000
--- a/src/hbase-handler/src/test/queries/hbase_ppd_key_range.q
+++ /dev/null
@@ -1,71 +0,0 @@
-CREATE TABLE hbase_pushdown(key string, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string");
-
-INSERT OVERWRITE TABLE hbase_pushdown 
-SELECT cast(key as string), value
-FROM src;
-
--- with full pushdown
-explain select * from hbase_pushdown where key>'90';
-
-select * from hbase_pushdown where key>'90';
-select * from hbase_pushdown where key<'1';
-select * from hbase_pushdown where key<='2';
-select * from hbase_pushdown where key>='90';
-
--- with cnostant expressinon
-explain select * from hbase_pushdown where key>=cast(40 + 50 as string);
-select * from hbase_pushdown where key>=cast(40 + 50 as string);
-
--- with partial pushdown
-
-explain select * from hbase_pushdown where key>'90' and value like '%9%';
-
-select * from hbase_pushdown where key>'90' and value like '%9%';
-
--- with two residuals
-
-explain select * from hbase_pushdown
-where key>='90' and value like '%9%' and key=cast(value as int);
-
-select * from hbase_pushdown
-where key>='90' and value like '%9%' and key=cast(value as int);
-
-
--- with contradictory pushdowns
-
-explain select * from hbase_pushdown
-where key<'80' and key>'90' and value like '%90%';
-
-select * from hbase_pushdown
-where key<'80' and key>'90' and value like '%90%';
-
--- with nothing to push down
-
-explain select * from hbase_pushdown;
-
--- with a predicate which is not actually part of the filter, so
--- it should be ignored by pushdown
-
-explain select * from hbase_pushdown
-where (case when key<'90' then 2 else 4 end) > 3;
-
--- with a predicate which is under an OR, so it should
--- be ignored by pushdown
-
-explain select * from hbase_pushdown
-where key<='80' or value like '%90%';
-
--- following will get pushed into hbase after HIVE-2819
-explain select * from hbase_pushdown where key > '281' 
-and key < '287';
-
-select * from hbase_pushdown where key > '281' 
-and key < '287';
-
-set hive.optimize.ppd.storage=false;
-
--- with pushdown disabled
-
-explain select * from hbase_pushdown where key<='90';
diff --git a/src/hbase-handler/src/test/queries/hbase_pushdown.q b/src/hbase-handler/src/test/queries/hbase_pushdown.q
deleted file mode 100644
index 69a4536..0000000
--- a/src/hbase-handler/src/test/queries/hbase_pushdown.q
+++ /dev/null
@@ -1,53 +0,0 @@
-CREATE TABLE hbase_pushdown(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string");
-
-INSERT OVERWRITE TABLE hbase_pushdown 
-SELECT *
-FROM src;
-
--- with full pushdown
-explain select * from hbase_pushdown where key=90;
-
-select * from hbase_pushdown where key=90;
-
--- with partial pushdown
-
-explain select * from hbase_pushdown where key=90 and value like '%90%';
-
-select * from hbase_pushdown where key=90 and value like '%90%';
-
--- with two residuals
-
-explain select * from hbase_pushdown
-where key=90 and value like '%90%' and key=cast(value as int);
-
--- with contradictory pushdowns
-
-explain select * from hbase_pushdown
-where key=80 and key=90 and value like '%90%';
-
-select * from hbase_pushdown
-where key=80 and key=90 and value like '%90%';
-
--- with nothing to push down
-
-explain select * from hbase_pushdown;
-
--- with a predicate which is not actually part of the filter, so
--- it should be ignored by pushdown
-
-explain select * from hbase_pushdown
-where (case when key=90 then 2 else 4 end) > 3;
-
--- with a predicate which is under an OR, so it should
--- be ignored by pushdown
-
-explain select * from hbase_pushdown
-where key=80 or value like '%90%';
-
-set hive.optimize.ppd.storage=false;
-
--- with pushdown disabled
-
-explain select * from hbase_pushdown where key=90;
diff --git a/src/hbase-handler/src/test/queries/hbase_queries.q b/src/hbase-handler/src/test/queries/hbase_queries.q
deleted file mode 100644
index 0457566..0000000
--- a/src/hbase-handler/src/test/queries/hbase_queries.q
+++ /dev/null
@@ -1,160 +0,0 @@
-DROP TABLE hbase_table_1;
-CREATE TABLE hbase_table_1(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");
-
-DESCRIBE EXTENDED hbase_table_1;
-
-select * from hbase_table_1;
-
-EXPLAIN FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0;
-FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0;
-
-DROP TABLE hbase_table_2;
-CREATE EXTERNAL TABLE hbase_table_2(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");
-
-EXPLAIN 
-SELECT Y.* 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.* FROM src) Y
-ON (x.key = Y.key)
-ORDER BY key, value LIMIT 20;
-
-SELECT Y.* 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.* FROM src) Y
-ON (x.key = Y.key)
-ORDER BY key, value LIMIT 20;
-
-EXPLAIN 
-SELECT Y.*
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
-JOIN 
-(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
-ON (x.key = Y.key)
-ORDER BY key, value;
-
-SELECT Y.*
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
-JOIN 
-(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
-ON (x.key = Y.key)
-ORDER BY key,value;
-
-DROP TABLE empty_hbase_table;
-CREATE TABLE empty_hbase_table(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string");
-
-DROP TABLE empty_normal_table;
-CREATE TABLE empty_normal_table(key int, value string);
-
-select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_hbase_table) x order by c;
-select * from (select count(1) c from empty_normal_table union all select count(1) as c from hbase_table_1) x order by c;
-select * from (select count(1) c from src union all select count(1) as c from empty_hbase_table) x order by c;
-select * from (select count(1) c from src union all select count(1) as c from hbase_table_1) x order by c;
-
-CREATE TABLE hbase_table_3(key int, value string, count int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "cf:val,cf2:count"
-);
-
-EXPLAIN 
-INSERT OVERWRITE TABLE hbase_table_3
-SELECT x.key, x.value, Y.count 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
-ON (x.key = Y.key);
-
-INSERT OVERWRITE TABLE hbase_table_3
-SELECT x.key, x.value, Y.count 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
-ON (x.key = Y.key);
-
-select count(1) from hbase_table_3;
-select * from hbase_table_3 order by key, value limit 5;
-select key, count from hbase_table_3 order by key, count desc limit 5;
-
-DROP TABLE hbase_table_4;
-CREATE TABLE hbase_table_4(key int, value1 string, value2 int, value3 int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "a:b,a:c,d:e"
-);
-
-INSERT OVERWRITE TABLE hbase_table_4 SELECT key, value, key+1, key+2 
-FROM src WHERE key=98 OR key=100;
-
-SELECT * FROM hbase_table_4 ORDER BY key;
-
-DROP TABLE hbase_table_5;
-CREATE EXTERNAL TABLE hbase_table_5(key int, value map<string,string>) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "a:")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_4");
-
-SELECT * FROM hbase_table_5 ORDER BY key;
-
-DROP TABLE hbase_table_6;
-CREATE TABLE hbase_table_6(key int, value map<string,string>) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = ":key,cf:"
-);
-INSERT OVERWRITE TABLE hbase_table_6 SELECT key, map(value, key) FROM src
-WHERE key=98 OR key=100;
-
-SELECT * FROM hbase_table_6 ORDER BY key;
-
-DROP TABLE hbase_table_7;
-CREATE TABLE hbase_table_7(value map<string,string>, key int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "cf:,:key"
-);
-INSERT OVERWRITE TABLE hbase_table_7 
-SELECT map(value, key, upper(value), key+1), key FROM src
-WHERE key=98 OR key=100;
-
-SELECT * FROM hbase_table_7 ORDER BY key;
-
-set hive.hbase.wal.enabled=false;
-
-DROP TABLE hbase_table_8;
-CREATE TABLE hbase_table_8(key int, value1 string, value2 int, value3 int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "a:b,a:c,d:e"
-);
-
-INSERT OVERWRITE TABLE hbase_table_8 SELECT key, value, key+1, key+2 
-FROM src WHERE key=98 OR key=100;
-
-SELECT * FROM hbase_table_8 ORDER BY key;
-
-DROP TABLE hbase_table_1;
-DROP TABLE hbase_table_2;
-DROP TABLE hbase_table_3;
-DROP TABLE hbase_table_4;
-DROP TABLE hbase_table_5;
-DROP TABLE hbase_table_6;
-DROP TABLE hbase_table_7;
-DROP TABLE hbase_table_8;
-DROP TABLE empty_hbase_table;
-DROP TABLE empty_normal_table;
diff --git a/src/hbase-handler/src/test/queries/hbase_stats.q b/src/hbase-handler/src/test/queries/hbase_stats.q
deleted file mode 100644
index 52efef5..0000000
--- a/src/hbase-handler/src/test/queries/hbase_stats.q
+++ /dev/null
@@ -1,30 +0,0 @@
-set datanucleus.cache.collections=false;
-set hive.stats.autogather=true;
-set hive.ststs.atomic=false;
-
-set hive.stats.dbclass=hbase;
-
-create table stats_src like src;
-insert overwrite table stats_src select * from src;
-analyze table stats_src compute statistics;
-desc formatted stats_src;
-
-create table stats_part like srcpart;
-
-insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src;
-insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src;
-
-analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics;
-analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics;
-
-insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src;
-
-desc formatted stats_part;
-desc formatted stats_part partition (ds='2010-04-08', hr = '11');
-desc formatted stats_part partition (ds='2010-04-08', hr = '12');
-
-analyze table stats_part partition(ds, hr) compute statistics;
-desc formatted stats_part;
-
-drop table stats_src;
-drop table stats_part;
diff --git a/src/hbase-handler/src/test/queries/hbase_stats2.q b/src/hbase-handler/src/test/queries/hbase_stats2.q
deleted file mode 100644
index 520e003..0000000
--- a/src/hbase-handler/src/test/queries/hbase_stats2.q
+++ /dev/null
@@ -1,31 +0,0 @@
-set datanucleus.cache.collections=false;
-set hive.stats.autogather=true;
-set hive.stats.atomic=false;
-set hive.stats.collect.uncompressedsize=false;
-
-set hive.stats.dbclass=hbase;
-
-create table stats_src like src;
-insert overwrite table stats_src select * from src;
-analyze table stats_src compute statistics;
-desc formatted stats_src;
-
-create table stats_part like srcpart;
-
-insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src;
-insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src;
-
-analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics;
-analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics;
-
-insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src;
-
-desc formatted stats_part;
-desc formatted stats_part partition (ds='2010-04-08', hr = '11');
-desc formatted stats_part partition (ds='2010-04-08', hr = '12');
-
-analyze table stats_part partition(ds, hr) compute statistics;
-desc formatted stats_part;
-
-drop table stats_src;
-drop table stats_part;
diff --git a/src/hbase-handler/src/test/queries/positive/external_table_ppd.q b/src/hbase-handler/src/test/queries/positive/external_table_ppd.q
new file mode 100644
index 0000000..fbef4bb
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/external_table_ppd.q
@@ -0,0 +1,37 @@
+DROP TABLE t_hbase;
+
+CREATE TABLE t_hbase(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive",
+               "hbase.table.default.storage.type" = "binary");
+
+DESCRIBE FORMATTED t_hbase;
+
+INSERT OVERWRITE TABLE t_hbase
+SELECT 'user1', 1, 11, 10, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+INSERT OVERWRITE TABLE t_hbase
+SELECT 'user2', 127, 327, 2147, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+INSERT OVERWRITE TABLE t_hbase
+SELECT 'user3', -128, -327, -214748, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+explain SELECT * FROM t_hbase where int_col > 0;
+SELECT * FROM t_hbase where int_col > 0;
+
+DROP TABLE t_hbase;
+
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_binary_external_table_queries.q b/src/hbase-handler/src/test/queries/positive/hbase_binary_external_table_queries.q
new file mode 100644
index 0000000..f7a1218
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_binary_external_table_queries.q
@@ -0,0 +1,41 @@
+DROP TABLE t_ext_hbase_1;
+
+CREATE EXTERNAL TABLE t_ext_hbase_1
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
+TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable");
+
+SELECT * FROM t_ext_hbase_1;
+
+DROP TABLE t_ext_hbase_1;
+DROP TABLE t_ext_hbase_2;
+
+CREATE EXTERNAL TABLE t_ext_hbase_2
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:cq-boolean#b,cf:cq-byte#b,cf:cq-short#b,cf:cq-int#b,cf:cq-long#b,cf:cq-string#b,cf:cq-float#b,cf:cq-double#b")
+TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable");
+
+SELECT * FROM t_ext_hbase_2;
+
+DROP TABLE t_ext_hbase_2;
+DROP TABLE t_ext_hbase_3;
+
+CREATE EXTERNAL TABLE t_ext_hbase_3
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
+TBLPROPERTIES (
+"hbase.table.name" = "HiveExternalTable",
+"hbase.table.default.storage.type" = "binary");
+
+SELECT * from t_ext_hbase_3;
+
+--HIVE-2958
+SELECT c_int, count(*) FROM t_ext_hbase_3 GROUP BY c_int;
+
+DROP table t_ext_hbase_3;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_binary_map_queries.q b/src/hbase-handler/src/test/queries/positive/hbase_binary_map_queries.q
new file mode 100644
index 0000000..255a2c7
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_binary_map_queries.q
@@ -0,0 +1,225 @@
+DROP TABLE hbase_src;
+
+CREATE TABLE hbase_src(key STRING,
+                       tinyint_col TINYINT,
+                       smallint_col SMALLINT,
+                       int_col INT,
+                       bigint_col BIGINT,
+                       float_col FLOAT,
+                       double_col DOUBLE,
+                       string_col STRING);
+
+INSERT OVERWRITE TABLE hbase_src
+  SELECT key, key, key, key, key, key, key, value
+  FROM src
+  WHERE key = 125 OR key = 126 OR key = 127;
+
+DROP TABLE t_hbase_maps;
+
+CREATE TABLE t_hbase_maps(key STRING,
+                          tinyint_map_col MAP<TINYINT, TINYINT>,
+                          smallint_map_col MAP<SMALLINT, SMALLINT>,
+                          int_map_col MAP<INT, INT>,
+                          bigint_map_col MAP<BIGINT, BIGINT>,
+                          float_map_col MAP<FLOAT, FLOAT>,
+                          double_map_col MAP<DOUBLE, DOUBLE>,
+                          string_map_col MAP<STRING, STRING>,
+                          boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps");
+
+INSERT OVERWRITE TABLE t_hbase_maps
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(true, true)
+  FROM hbase_src
+  WHERE key = 125;
+
+INSERT OVERWRITE TABLE t_hbase_maps
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(false, false)
+  FROM hbase_src
+  WHERE key = 126;
+
+SELECT * FROM t_hbase_maps ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps;
+
+CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
+                                       tinyint_map_col MAP<TINYINT, TINYINT>,
+                                       smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                       int_map_col MAP<INT, INT>,
+                                       bigint_map_col MAP<BIGINT, BIGINT>,
+                                       float_map_col MAP<FLOAT, FLOAT>,
+                                       double_map_col MAP<DOUBLE, DOUBLE>,
+                                       string_map_col MAP<STRING, STRING>,
+                                       boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps");
+
+SELECT * FROM t_ext_hbase_maps ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps;
+
+DROP TABLE t_ext_hbase_maps_1;
+
+CREATE EXTERNAL TABLE t_ext_hbase_maps_1(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps");
+
+SELECT * FROM t_ext_hbase_maps_1 ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps_1;
+
+DROP TABLE t_ext_hbase_maps_2;
+
+CREATE EXTERNAL TABLE t_ext_hbase_maps_2(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES (
+"hbase.table.name"="t_hive_maps",
+"hbase.table.default.storage.type"="binary");
+
+SELECT * FROM t_ext_hbase_maps_2 ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps_2;
+
+DROP TABLE t_hbase_maps_1;
+
+CREATE TABLE t_hbase_maps_1(key STRING,
+                            tinyint_map_col MAP<TINYINT, TINYINT>,
+                            smallint_map_col MAP<SMALLINT, SMALLINT>,
+                            int_map_col MAP<INT, INT>,
+                            bigint_map_col MAP<BIGINT, BIGINT>,
+                            float_map_col MAP<FLOAT, FLOAT>,
+                            double_map_col MAP<DOUBLE, DOUBLE>,
+                            string_map_col MAP<STRING, STRING>,
+                            boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#b:b,cf-smallint:#b:b,cf-int:#b:b,cf-bigint:#b:b,cf-float:#b:b,cf-double:#b:b,cf-string:#b:b,cf-boolean:#b:b")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");
+
+INSERT OVERWRITE TABLE t_hbase_maps_1
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(true, true)
+  FROM hbase_src
+  WHERE key = 125;
+
+INSERT OVERWRITE TABLE t_hbase_maps_1
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(false, false)
+  FROM hbase_src
+  WHERE key = 126;
+
+SELECT * FROM t_hbase_maps_1 ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps_3;
+
+CREATE EXTERNAL TABLE t_ext_hbase_maps_3(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");
+
+SELECT * FROM t_ext_hbase_maps_3 ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps_3;
+
+DROP TABLE t_ext_hbase_maps_4;
+
+CREATE EXTERNAL TABLE t_ext_hbase_maps_4(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");
+
+SELECT * FROM t_ext_hbase_maps_4 ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps_4;
+
+DROP TABLE t_ext_hbase_maps_5;
+
+CREATE EXTERNAL TABLE t_ext_hbase_maps_5(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES (
+"hbase.table.name"="t_hive_maps_1",
+"hbase.table.default.storage.type"="binary");
+
+SELECT * FROM t_ext_hbase_maps_5 ORDER BY key;
+
+DROP TABLE t_ext_hbase_maps_5;
+
+DROP TABLE t_hbase_maps_1;
+
+DROP TABLE t_hbase_maps;
+
+DROP TABLE hbase_src;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_binary_storage_queries.q b/src/hbase-handler/src/test/queries/positive/hbase_binary_storage_queries.q
new file mode 100644
index 0000000..b048871
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_binary_storage_queries.q
@@ -0,0 +1,218 @@
+DROP TABLE t_hbase;
+
+CREATE TABLE t_hbase(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive",
+               "hbase.table.default.storage.type" = "binary");
+
+DESCRIBE FORMATTED t_hbase;
+
+INSERT OVERWRITE TABLE t_hbase
+SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+INSERT OVERWRITE TABLE t_hbase
+SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+INSERT OVERWRITE TABLE t_hbase
+SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+SELECT * FROM t_hbase;
+
+SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase
+WHERE key='user1' OR key='user2' OR key='user3';
+
+SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase;
+
+DROP TABLE t_hbase_1;
+
+CREATE EXTERNAL TABLE t_hbase_1(key STRING,
+                                tinyint_col TINYINT,
+                                smallint_col SMALLINT,
+                                int_col INT,
+                                bigint_col BIGINT,
+                                float_col FLOAT,
+                                double_col DOUBLE,
+                                boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
+TBLPROPERTIES ("hbase.table.name" = "t_hive");
+
+DESCRIBE FORMATTED t_hbase_1;
+
+SELECT * FROM t_hbase_1;
+
+SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_1
+WHERE key='user1' OR key='user2' OR key='user3';
+
+SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_1;
+
+DROP TABLE t_hbase_1;
+DROP TABLE t_hbase;
+DROP TABLE t_hbase_2;
+
+CREATE TABLE t_hbase_2(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive_2");
+
+INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126;
+
+SELECT * FROM t_hbase_2;
+
+SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_2
+WHERE key='user1' OR key='user2' OR key='user3';
+
+SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_2;
+
+DROP TABLE t_hbase_3;
+
+CREATE EXTERNAL TABLE t_hbase_3(key STRING,
+                                tinyint_col TINYINT,
+                                smallint_col SMALLINT,
+                                int_col INT,
+                                bigint_col BIGINT,
+                                float_col FLOAT,
+                                double_col DOUBLE,
+                                boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
+TBLPROPERTIES ("hbase.table.name" = "t_hive_2");
+
+SELECT * FROM t_hbase_3;
+
+SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_3
+WHERE key='user1' OR key='user2' OR key='user3';
+
+SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_3;
+
+DROP TABLE t_hbase_3;
+
+DROP TABLE t_hbase_4;
+
+CREATE EXTERNAL TABLE t_hbase_4(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES (
+"hbase.table.name" = "t_hive_2",
+"hbase.table.default.storage.type" = "binary");
+
+SELECT * FROM t_hbase_4;
+
+SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_4
+WHERE key='user1' OR key='user2' OR key='user3';
+
+SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_4;
+
+DROP TABLE t_hbase_4;
+DROP TABLE t_hbase_2;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_bulk.m b/src/hbase-handler/src/test/queries/positive/hbase_bulk.m
new file mode 100644
index 0000000..22683d9
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_bulk.m
@@ -0,0 +1,57 @@
+drop table hbsort;
+drop table hbpartition;
+
+set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
+
+-- this is a dummy table used for controlling how the HFiles are
+-- created
+create table hbsort(key string, val string, val2 string)
+stored as
+INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
+OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
+TBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf');
+
+-- this is a dummy table used for controlling how the input file
+-- for TotalOrderPartitioner is created
+create table hbpartition(part_break string)
+row format serde
+'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
+stored as
+inputformat
+'org.apache.hadoop.mapred.TextInputFormat'
+outputformat
+'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat';
+
+-- this should produce one file, but we do not
+-- know what it will be called, so we will copy it to a well known
+-- filename /tmp/hbpartition.lst
+insert overwrite table hbpartition
+select distinct value
+from src
+where value='val_100' or value='val_200';
+
+dfs -cp /build/ql/test/data/warehouse/hbpartition/* /tmp/hbpartition.lst;
+
+set mapred.reduce.tasks=3;
+set hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
+set total.order.partitioner.natural.order=false;
+set total.order.partitioner.path=/tmp/hbpartition.lst;
+
+-- this should produce three files in /tmp/hbsort/cf
+-- include some trailing blanks and nulls to make sure we handle them correctly
+insert overwrite table hbsort
+select distinct value,
+  case when key=103 then cast(null as string) else key end,
+  case when key=103 then ''
+       else cast(key+1 as string) end
+from src
+cluster by value;
+
+
+-- To get the files out to your local filesystem for loading into
+-- HBase, run mkdir -p /tmp/blah/cf, then uncomment and
+-- semicolon-terminate the line below before running this test:
+-- dfs -copyToLocal /tmp/hbsort/cf/* /tmp/blah/cf
+
+drop table hbsort;
+drop table hbpartition;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_joins.q b/src/hbase-handler/src/test/queries/positive/hbase_joins.q
new file mode 100644
index 0000000..1616adc
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_joins.q
@@ -0,0 +1,82 @@
+DROP TABLE users;
+DROP TABLE states;
+DROP TABLE countries;
+DROP TABLE users_level;
+
+-- From HIVE-1257
+
+CREATE TABLE users(key string, state string, country string, country_id int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "info:state,info:country,info:country_id"
+);
+
+CREATE TABLE states(key string, name string)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "state:name"
+);
+
+CREATE TABLE countries(key string, name string, country string, country_id int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "info:name,info:country,info:country_id"
+);
+
+INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
+FROM src WHERE key=100;
+
+INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
+FROM src WHERE key=100;
+
+INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
+FROM src WHERE key=100;
+
+set hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat;
+
+SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country = c.key);
+
+SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
+ON (u.country = c.country);
+
+SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country_id = c.country_id);
+
+SELECT u.key, u.state, s.name FROM users u JOIN states s 
+ON (u.state = s.key);
+
+set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+
+SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country = c.key);
+
+SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
+ON (u.country = c.country);
+
+SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country_id = c.country_id);
+
+SELECT u.key, u.state, s.name FROM users u JOIN states s 
+ON (u.state = s.key);
+
+DROP TABLE users;
+DROP TABLE states;
+DROP TABLE countries;
+
+CREATE TABLE users(key int, userid int, username string, created int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:nickname,f:created");
+
+CREATE TABLE users_level(key int, userid int, level int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:level");
+
+-- HIVE-1903:  the problem fixed here showed up even without any data,
+-- so no need to load any to test it
+SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num 
+ FROM users JOIN users_level ON (users.userid = users_level.userid) 
+ GROUP BY year(from_unixtime(users.created)), level;
+
+DROP TABLE users;
+DROP TABLE users_level;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_ppd_key_range.q b/src/hbase-handler/src/test/queries/positive/hbase_ppd_key_range.q
new file mode 100644
index 0000000..59e724d
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_ppd_key_range.q
@@ -0,0 +1,71 @@
+CREATE TABLE hbase_pushdown(key string, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string");
+
+INSERT OVERWRITE TABLE hbase_pushdown 
+SELECT cast(key as string), value
+FROM src;
+
+-- with full pushdown
+explain select * from hbase_pushdown where key>'90';
+
+select * from hbase_pushdown where key>'90';
+select * from hbase_pushdown where key<'1';
+select * from hbase_pushdown where key<='2';
+select * from hbase_pushdown where key>='90';
+
+-- with cnostant expressinon
+explain select * from hbase_pushdown where key>=cast(40 + 50 as string);
+select * from hbase_pushdown where key>=cast(40 + 50 as string);
+
+-- with partial pushdown
+
+explain select * from hbase_pushdown where key>'90' and value like '%9%';
+
+select * from hbase_pushdown where key>'90' and value like '%9%';
+
+-- with two residuals
+
+explain select * from hbase_pushdown
+where key>='90' and value like '%9%' and key=cast(value as int);
+
+select * from hbase_pushdown
+where key>='90' and value like '%9%' and key=cast(value as int);
+
+
+-- with contradictory pushdowns
+
+explain select * from hbase_pushdown
+where key<'80' and key>'90' and value like '%90%';
+
+select * from hbase_pushdown
+where key<'80' and key>'90' and value like '%90%';
+
+-- with nothing to push down
+
+explain select * from hbase_pushdown;
+
+-- with a predicate which is not actually part of the filter, so
+-- it should be ignored by pushdown
+
+explain select * from hbase_pushdown
+where (case when key<'90' then 2 else 4 end) > 3;
+
+-- with a predicate which is under an OR, so it should
+-- be ignored by pushdown
+
+explain select * from hbase_pushdown
+where key<='80' or value like '%90%';
+
+-- following will get pushed into hbase after HIVE-2819
+explain select * from hbase_pushdown where key > '281' 
+and key < '287';
+
+select * from hbase_pushdown where key > '281' 
+and key < '287';
+
+set hive.optimize.ppd.storage=false;
+
+-- with pushdown disabled
+
+explain select * from hbase_pushdown where key<='90';
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_pushdown.q b/src/hbase-handler/src/test/queries/positive/hbase_pushdown.q
new file mode 100644
index 0000000..69a4536
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_pushdown.q
@@ -0,0 +1,53 @@
+CREATE TABLE hbase_pushdown(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string");
+
+INSERT OVERWRITE TABLE hbase_pushdown 
+SELECT *
+FROM src;
+
+-- with full pushdown
+explain select * from hbase_pushdown where key=90;
+
+select * from hbase_pushdown where key=90;
+
+-- with partial pushdown
+
+explain select * from hbase_pushdown where key=90 and value like '%90%';
+
+select * from hbase_pushdown where key=90 and value like '%90%';
+
+-- with two residuals
+
+explain select * from hbase_pushdown
+where key=90 and value like '%90%' and key=cast(value as int);
+
+-- with contradictory pushdowns
+
+explain select * from hbase_pushdown
+where key=80 and key=90 and value like '%90%';
+
+select * from hbase_pushdown
+where key=80 and key=90 and value like '%90%';
+
+-- with nothing to push down
+
+explain select * from hbase_pushdown;
+
+-- with a predicate which is not actually part of the filter, so
+-- it should be ignored by pushdown
+
+explain select * from hbase_pushdown
+where (case when key=90 then 2 else 4 end) > 3;
+
+-- with a predicate which is under an OR, so it should
+-- be ignored by pushdown
+
+explain select * from hbase_pushdown
+where key=80 or value like '%90%';
+
+set hive.optimize.ppd.storage=false;
+
+-- with pushdown disabled
+
+explain select * from hbase_pushdown where key=90;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_queries.q b/src/hbase-handler/src/test/queries/positive/hbase_queries.q
new file mode 100644
index 0000000..0457566
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_queries.q
@@ -0,0 +1,160 @@
+DROP TABLE hbase_table_1;
+CREATE TABLE hbase_table_1(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");
+
+DESCRIBE EXTENDED hbase_table_1;
+
+select * from hbase_table_1;
+
+EXPLAIN FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0;
+FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0;
+
+DROP TABLE hbase_table_2;
+CREATE EXTERNAL TABLE hbase_table_2(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");
+
+EXPLAIN 
+SELECT Y.* 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.* FROM src) Y
+ON (x.key = Y.key)
+ORDER BY key, value LIMIT 20;
+
+SELECT Y.* 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.* FROM src) Y
+ON (x.key = Y.key)
+ORDER BY key, value LIMIT 20;
+
+EXPLAIN 
+SELECT Y.*
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
+JOIN 
+(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
+ON (x.key = Y.key)
+ORDER BY key, value;
+
+SELECT Y.*
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
+JOIN 
+(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
+ON (x.key = Y.key)
+ORDER BY key,value;
+
+DROP TABLE empty_hbase_table;
+CREATE TABLE empty_hbase_table(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string");
+
+DROP TABLE empty_normal_table;
+CREATE TABLE empty_normal_table(key int, value string);
+
+select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_hbase_table) x order by c;
+select * from (select count(1) c from empty_normal_table union all select count(1) as c from hbase_table_1) x order by c;
+select * from (select count(1) c from src union all select count(1) as c from empty_hbase_table) x order by c;
+select * from (select count(1) c from src union all select count(1) as c from hbase_table_1) x order by c;
+
+CREATE TABLE hbase_table_3(key int, value string, count int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "cf:val,cf2:count"
+);
+
+EXPLAIN 
+INSERT OVERWRITE TABLE hbase_table_3
+SELECT x.key, x.value, Y.count 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
+ON (x.key = Y.key);
+
+INSERT OVERWRITE TABLE hbase_table_3
+SELECT x.key, x.value, Y.count 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
+ON (x.key = Y.key);
+
+select count(1) from hbase_table_3;
+select * from hbase_table_3 order by key, value limit 5;
+select key, count from hbase_table_3 order by key, count desc limit 5;
+
+DROP TABLE hbase_table_4;
+CREATE TABLE hbase_table_4(key int, value1 string, value2 int, value3 int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "a:b,a:c,d:e"
+);
+
+INSERT OVERWRITE TABLE hbase_table_4 SELECT key, value, key+1, key+2 
+FROM src WHERE key=98 OR key=100;
+
+SELECT * FROM hbase_table_4 ORDER BY key;
+
+DROP TABLE hbase_table_5;
+CREATE EXTERNAL TABLE hbase_table_5(key int, value map<string,string>) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "a:")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_4");
+
+SELECT * FROM hbase_table_5 ORDER BY key;
+
+DROP TABLE hbase_table_6;
+CREATE TABLE hbase_table_6(key int, value map<string,string>) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = ":key,cf:"
+);
+INSERT OVERWRITE TABLE hbase_table_6 SELECT key, map(value, key) FROM src
+WHERE key=98 OR key=100;
+
+SELECT * FROM hbase_table_6 ORDER BY key;
+
+DROP TABLE hbase_table_7;
+CREATE TABLE hbase_table_7(value map<string,string>, key int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "cf:,:key"
+);
+INSERT OVERWRITE TABLE hbase_table_7 
+SELECT map(value, key, upper(value), key+1), key FROM src
+WHERE key=98 OR key=100;
+
+SELECT * FROM hbase_table_7 ORDER BY key;
+
+set hive.hbase.wal.enabled=false;
+
+DROP TABLE hbase_table_8;
+CREATE TABLE hbase_table_8(key int, value1 string, value2 int, value3 int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "a:b,a:c,d:e"
+);
+
+INSERT OVERWRITE TABLE hbase_table_8 SELECT key, value, key+1, key+2 
+FROM src WHERE key=98 OR key=100;
+
+SELECT * FROM hbase_table_8 ORDER BY key;
+
+DROP TABLE hbase_table_1;
+DROP TABLE hbase_table_2;
+DROP TABLE hbase_table_3;
+DROP TABLE hbase_table_4;
+DROP TABLE hbase_table_5;
+DROP TABLE hbase_table_6;
+DROP TABLE hbase_table_7;
+DROP TABLE hbase_table_8;
+DROP TABLE empty_hbase_table;
+DROP TABLE empty_normal_table;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_stats.q b/src/hbase-handler/src/test/queries/positive/hbase_stats.q
new file mode 100644
index 0000000..52efef5
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_stats.q
@@ -0,0 +1,30 @@
+set datanucleus.cache.collections=false;
+set hive.stats.autogather=true;
+set hive.ststs.atomic=false;
+
+set hive.stats.dbclass=hbase;
+
+create table stats_src like src;
+insert overwrite table stats_src select * from src;
+analyze table stats_src compute statistics;
+desc formatted stats_src;
+
+create table stats_part like srcpart;
+
+insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src;
+insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src;
+
+analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics;
+analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics;
+
+insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src;
+
+desc formatted stats_part;
+desc formatted stats_part partition (ds='2010-04-08', hr = '11');
+desc formatted stats_part partition (ds='2010-04-08', hr = '12');
+
+analyze table stats_part partition(ds, hr) compute statistics;
+desc formatted stats_part;
+
+drop table stats_src;
+drop table stats_part;
diff --git a/src/hbase-handler/src/test/queries/positive/hbase_stats2.q b/src/hbase-handler/src/test/queries/positive/hbase_stats2.q
new file mode 100644
index 0000000..520e003
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/hbase_stats2.q
@@ -0,0 +1,31 @@
+set datanucleus.cache.collections=false;
+set hive.stats.autogather=true;
+set hive.stats.atomic=false;
+set hive.stats.collect.uncompressedsize=false;
+
+set hive.stats.dbclass=hbase;
+
+create table stats_src like src;
+insert overwrite table stats_src select * from src;
+analyze table stats_src compute statistics;
+desc formatted stats_src;
+
+create table stats_part like srcpart;
+
+insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src;
+insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src;
+
+analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics;
+analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics;
+
+insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src;
+
+desc formatted stats_part;
+desc formatted stats_part partition (ds='2010-04-08', hr = '11');
+desc formatted stats_part partition (ds='2010-04-08', hr = '12');
+
+analyze table stats_part partition(ds, hr) compute statistics;
+desc formatted stats_part;
+
+drop table stats_src;
+drop table stats_part;
diff --git a/src/hbase-handler/src/test/queries/positive/ppd_key_ranges.q b/src/hbase-handler/src/test/queries/positive/ppd_key_ranges.q
new file mode 100644
index 0000000..0497d25
--- /dev/null
+++ b/src/hbase-handler/src/test/queries/positive/ppd_key_ranges.q
@@ -0,0 +1,22 @@
+CREATE TABLE hbase_ppd_keyrange(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#binary,cf:string");
+
+INSERT OVERWRITE TABLE hbase_ppd_keyrange 
+SELECT *
+FROM src;
+
+explain select * from hbase_ppd_keyrange where key > 8 and key < 21;
+select * from hbase_ppd_keyrange where key > 8 and key < 21;
+
+explain select * from hbase_ppd_keyrange where key > 8 and key <= 17;
+select * from hbase_ppd_keyrange where key > 8 and key <= 17;
+
+
+explain select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%';
+select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%';
+
+explain select * from hbase_ppd_keyrange where key >= 9 and key < 17 and key = 11;
+select * from hbase_ppd_keyrange where key >=9  and key < 17 and key = 11;
+
+drop table  hbase_ppd_keyrange;
diff --git a/src/hbase-handler/src/test/queries/ppd_key_ranges.q b/src/hbase-handler/src/test/queries/ppd_key_ranges.q
deleted file mode 100644
index 0497d25..0000000
--- a/src/hbase-handler/src/test/queries/ppd_key_ranges.q
+++ /dev/null
@@ -1,22 +0,0 @@
-CREATE TABLE hbase_ppd_keyrange(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#binary,cf:string");
-
-INSERT OVERWRITE TABLE hbase_ppd_keyrange 
-SELECT *
-FROM src;
-
-explain select * from hbase_ppd_keyrange where key > 8 and key < 21;
-select * from hbase_ppd_keyrange where key > 8 and key < 21;
-
-explain select * from hbase_ppd_keyrange where key > 8 and key <= 17;
-select * from hbase_ppd_keyrange where key > 8 and key <= 17;
-
-
-explain select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%';
-select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%';
-
-explain select * from hbase_ppd_keyrange where key >= 9 and key < 17 and key = 11;
-select * from hbase_ppd_keyrange where key >=9  and key < 17 and key = 11;
-
-drop table  hbase_ppd_keyrange;
diff --git a/src/hbase-handler/src/test/results/external_table_ppd.q.out b/src/hbase-handler/src/test/results/external_table_ppd.q.out
deleted file mode 100644
index 38d9175..0000000
--- a/src/hbase-handler/src/test/results/external_table_ppd.q.out
+++ /dev/null
@@ -1,186 +0,0 @@
-PREHOOK: query: DROP TABLE t_hbase
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE t_hbase(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive",
-               "hbase.table.default.storage.type" = "binary")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE t_hbase(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive",
-               "hbase.table.default.storage.type" = "binary")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: DESCRIBE FORMATTED t_hbase
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: DESCRIBE FORMATTED t_hbase
-POSTHOOK: type: DESCTABLE
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	from deserializer   
-tinyint_col         	tinyint             	from deserializer   
-smallint_col        	smallint            	from deserializer   
-int_col             	int                 	from deserializer   
-bigint_col          	bigint              	from deserializer   
-float_col           	float               	from deserializer   
-double_col          	double              	from deserializer   
-boolean_col         	boolean             	from deserializer   
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	hbase.table.default.storage.type	binary              
-	hbase.table.name    	t_hive              
-	storage_handler     	org.apache.hadoop.hive.hbase.HBaseStorageHandler
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.hbase.HBaseSerDe	 
-InputFormat:        	org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	hbase.columns.mapping	cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-
-	serialization.format	1                   
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user1', 1, 11, 10, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user1', 1, 11, 10, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user2', 127, 327, 2147, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user2', 127, 327, 2147, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user3', -128, -327, -214748, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user3', -128, -327, -214748, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: explain SELECT * FROM t_hbase where int_col > 0
-PREHOOK: type: QUERY
-POSTHOOK: query: explain SELECT * FROM t_hbase where int_col > 0
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME t_hbase))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_TABLE_OR_COL int_col) 0))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        t_hbase 
-          TableScan
-            alias: t_hbase
-            filterExpr:
-                expr: (int_col > 0)
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (int_col > 0)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: tinyint_col
-                      type: tinyint
-                      expr: smallint_col
-                      type: smallint
-                      expr: int_col
-                      type: int
-                      expr: bigint_col
-                      type: bigint
-                      expr: float_col
-                      type: float
-                      expr: double_col
-                      type: double
-                      expr: boolean_col
-                      type: boolean
-                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: SELECT * FROM t_hbase where int_col > 0
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase where int_col > 0
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-user1	1	11	10	1	1.0	1.0	true
-user2	127	327	2147	9223372036854775807	211.31	2.687465320571E8	false
-PREHOOK: query: DROP TABLE t_hbase
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: DROP TABLE t_hbase
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase
-POSTHOOK: Output: default@t_hbase
diff --git a/src/hbase-handler/src/test/results/hbase_binary_external_table_queries.q.out b/src/hbase-handler/src/test/results/hbase_binary_external_table_queries.q.out
deleted file mode 100644
index 524eef1..0000000
--- a/src/hbase-handler/src/test/results/hbase_binary_external_table_queries.q.out
+++ /dev/null
@@ -1,131 +0,0 @@
-PREHOOK: query: DROP TABLE t_ext_hbase_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_1
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
-TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_1
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
-TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_1
-PREHOOK: query: SELECT * FROM t_ext_hbase_1
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_1
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_1
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_1
-#### A masked pattern was here ####
-key-1	NULL	NULL	NULL	NULL	NULL	Hadoop, HBase,	NULL	NULL
-key-2	NULL	NULL	NULL	NULL	NULL	Hive	NULL	NULL
-key-3	NULL	NULL	NULL	NULL	NULL	Test Strings	NULL	NULL
-PREHOOK: query: DROP TABLE t_ext_hbase_1
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_1
-PREHOOK: Output: default@t_ext_hbase_1
-POSTHOOK: query: DROP TABLE t_ext_hbase_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_1
-POSTHOOK: Output: default@t_ext_hbase_1
-PREHOOK: query: DROP TABLE t_ext_hbase_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_2
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:cq-boolean#b,cf:cq-byte#b,cf:cq-short#b,cf:cq-int#b,cf:cq-long#b,cf:cq-string#b,cf:cq-float#b,cf:cq-double#b")
-TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_2
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:cq-boolean#b,cf:cq-byte#b,cf:cq-short#b,cf:cq-int#b,cf:cq-long#b,cf:cq-string#b,cf:cq-float#b,cf:cq-double#b")
-TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_2
-PREHOOK: query: SELECT * FROM t_ext_hbase_2
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_2
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_2
-#### A masked pattern was here ####
-key-1	true	-128	-32768	-2147483648	-9223372036854775808	Hadoop, HBase,	1.4E-45	4.9E-324
-key-2	false	-1	-1	-1	-1	Hive	-1.0	-1.0
-key-3	true	127	32767	2147483647	9223372036854775807	Test Strings	3.4028235E38	1.7976931348623157E308
-PREHOOK: query: DROP TABLE t_ext_hbase_2
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_2
-PREHOOK: Output: default@t_ext_hbase_2
-POSTHOOK: query: DROP TABLE t_ext_hbase_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_2
-POSTHOOK: Output: default@t_ext_hbase_2
-PREHOOK: query: DROP TABLE t_ext_hbase_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_3
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
-TBLPROPERTIES (
-"hbase.table.name" = "HiveExternalTable",
-"hbase.table.default.storage.type" = "binary")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_3
-(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
- c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
-TBLPROPERTIES (
-"hbase.table.name" = "HiveExternalTable",
-"hbase.table.default.storage.type" = "binary")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_3
-PREHOOK: query: SELECT * from t_ext_hbase_3
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_3
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * from t_ext_hbase_3
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_3
-#### A masked pattern was here ####
-key-1	true	-128	-32768	-2147483648	-9223372036854775808	Hadoop, HBase,	1.4E-45	4.9E-324
-key-2	false	-1	-1	-1	-1	Hive	-1.0	-1.0
-key-3	true	127	32767	2147483647	9223372036854775807	Test Strings	3.4028235E38	1.7976931348623157E308
-PREHOOK: query: --HIVE-2958
-SELECT c_int, count(*) FROM t_ext_hbase_3 GROUP BY c_int
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_3
-#### A masked pattern was here ####
-POSTHOOK: query: --HIVE-2958
-SELECT c_int, count(*) FROM t_ext_hbase_3 GROUP BY c_int
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_3
-#### A masked pattern was here ####
--2147483648	1
--1	1
-2147483647	1
-PREHOOK: query: DROP table t_ext_hbase_3
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_3
-PREHOOK: Output: default@t_ext_hbase_3
-POSTHOOK: query: DROP table t_ext_hbase_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_3
-POSTHOOK: Output: default@t_ext_hbase_3
diff --git a/src/hbase-handler/src/test/results/hbase_binary_map_queries.q.out b/src/hbase-handler/src/test/results/hbase_binary_map_queries.q.out
deleted file mode 100644
index 8984da1..0000000
--- a/src/hbase-handler/src/test/results/hbase_binary_map_queries.q.out
+++ /dev/null
@@ -1,869 +0,0 @@
-PREHOOK: query: DROP TABLE hbase_src
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_src
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE hbase_src(key STRING,
-                       tinyint_col TINYINT,
-                       smallint_col SMALLINT,
-                       int_col INT,
-                       bigint_col BIGINT,
-                       float_col FLOAT,
-                       double_col DOUBLE,
-                       string_col STRING)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_src(key STRING,
-                       tinyint_col TINYINT,
-                       smallint_col SMALLINT,
-                       int_col INT,
-                       bigint_col BIGINT,
-                       float_col FLOAT,
-                       double_col DOUBLE,
-                       string_col STRING)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_src
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_src
-  SELECT key, key, key, key, key, key, key, value
-  FROM src
-  WHERE key = 125 OR key = 126 OR key = 127
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_src
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_src
-  SELECT key, key, key, key, key, key, key, value
-  FROM src
-  WHERE key = 125 OR key = 126 OR key = 127
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_src
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_hbase_maps
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase_maps
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE TABLE t_hbase_maps(key STRING,
-                          tinyint_map_col MAP<TINYINT, TINYINT>,
-                          smallint_map_col MAP<SMALLINT, SMALLINT>,
-                          int_map_col MAP<INT, INT>,
-                          bigint_map_col MAP<BIGINT, BIGINT>,
-                          float_map_col MAP<FLOAT, FLOAT>,
-                          double_map_col MAP<DOUBLE, DOUBLE>,
-                          string_map_col MAP<STRING, STRING>,
-                          boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE t_hbase_maps(key STRING,
-                          tinyint_map_col MAP<TINYINT, TINYINT>,
-                          smallint_map_col MAP<SMALLINT, SMALLINT>,
-                          int_map_col MAP<INT, INT>,
-                          bigint_map_col MAP<BIGINT, BIGINT>,
-                          float_map_col MAP<FLOAT, FLOAT>,
-                          double_map_col MAP<DOUBLE, DOUBLE>,
-                          string_map_col MAP<STRING, STRING>,
-                          boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase_maps
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(true, true)
-  FROM hbase_src
-  WHERE key = 125
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_src
-PREHOOK: Output: default@t_hbase_maps
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(true, true)
-  FROM hbase_src
-  WHERE key = 125
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_src
-POSTHOOK: Output: default@t_hbase_maps
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(false, false)
-  FROM hbase_src
-  WHERE key = 126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_src
-PREHOOK: Output: default@t_hbase_maps
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(false, false)
-  FROM hbase_src
-  WHERE key = 126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_src
-POSTHOOK: Output: default@t_hbase_maps
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_hbase_maps ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_maps
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase_maps ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_maps
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
-126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
-                                       tinyint_map_col MAP<TINYINT, TINYINT>,
-                                       smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                       int_map_col MAP<INT, INT>,
-                                       bigint_map_col MAP<BIGINT, BIGINT>,
-                                       float_map_col MAP<FLOAT, FLOAT>,
-                                       double_map_col MAP<DOUBLE, DOUBLE>,
-                                       string_map_col MAP<STRING, STRING>,
-                                       boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
-                                       tinyint_map_col MAP<TINYINT, TINYINT>,
-                                       smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                       int_map_col MAP<INT, INT>,
-                                       bigint_map_col MAP<BIGINT, BIGINT>,
-                                       float_map_col MAP<FLOAT, FLOAT>,
-                                       double_map_col MAP<DOUBLE, DOUBLE>,
-                                       string_map_col MAP<STRING, STRING>,
-                                       boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_maps
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_ext_hbase_maps ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_maps
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_maps ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_maps
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
-126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_maps
-PREHOOK: Output: default@t_ext_hbase_maps
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_maps
-POSTHOOK: Output: default@t_ext_hbase_maps
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_1(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_1(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_maps_1
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_ext_hbase_maps_1 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_maps_1
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_1 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_maps_1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{49:49}	{12594:12594}	{}	{}	{2.5932638E-9:2.5932638E-9}	{}	{"125":"val_125"}	{true:true}
-126	{49:49}	{12594:12594}	{}	{}	{2.5933207E-9:2.5933207E-9}	{}	{"126":"val_126"}	{true:true}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_1
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_maps_1
-PREHOOK: Output: default@t_ext_hbase_maps_1
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_maps_1
-POSTHOOK: Output: default@t_ext_hbase_maps_1
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_2(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES (
-"hbase.table.name"="t_hive_maps",
-"hbase.table.default.storage.type"="binary")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_2(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES (
-"hbase.table.name"="t_hive_maps",
-"hbase.table.default.storage.type"="binary")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_maps_2
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_ext_hbase_maps_2 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_maps_2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_2 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_maps_2
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{49:49}	{12594:12594}	{}	{}	{2.5932638E-9:2.5932638E-9}	{}	{"125":"val_125"}	{true:true}
-126	{49:49}	{12594:12594}	{}	{}	{2.5933207E-9:2.5933207E-9}	{}	{"126":"val_126"}	{true:true}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_2
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_maps_2
-PREHOOK: Output: default@t_ext_hbase_maps_2
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_maps_2
-POSTHOOK: Output: default@t_ext_hbase_maps_2
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_hbase_maps_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase_maps_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE TABLE t_hbase_maps_1(key STRING,
-                            tinyint_map_col MAP<TINYINT, TINYINT>,
-                            smallint_map_col MAP<SMALLINT, SMALLINT>,
-                            int_map_col MAP<INT, INT>,
-                            bigint_map_col MAP<BIGINT, BIGINT>,
-                            float_map_col MAP<FLOAT, FLOAT>,
-                            double_map_col MAP<DOUBLE, DOUBLE>,
-                            string_map_col MAP<STRING, STRING>,
-                            boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#b:b,cf-smallint:#b:b,cf-int:#b:b,cf-bigint:#b:b,cf-float:#b:b,cf-double:#b:b,cf-string:#b:b,cf-boolean:#b:b")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE t_hbase_maps_1(key STRING,
-                            tinyint_map_col MAP<TINYINT, TINYINT>,
-                            smallint_map_col MAP<SMALLINT, SMALLINT>,
-                            int_map_col MAP<INT, INT>,
-                            bigint_map_col MAP<BIGINT, BIGINT>,
-                            float_map_col MAP<FLOAT, FLOAT>,
-                            double_map_col MAP<DOUBLE, DOUBLE>,
-                            string_map_col MAP<STRING, STRING>,
-                            boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#b:b,cf-smallint:#b:b,cf-int:#b:b,cf-bigint:#b:b,cf-float:#b:b,cf-double:#b:b,cf-string:#b:b,cf-boolean:#b:b")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase_maps_1
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(true, true)
-  FROM hbase_src
-  WHERE key = 125
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_src
-PREHOOK: Output: default@t_hbase_maps_1
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(true, true)
-  FROM hbase_src
-  WHERE key = 125
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_src
-POSTHOOK: Output: default@t_hbase_maps_1
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(false, false)
-  FROM hbase_src
-  WHERE key = 126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_src
-PREHOOK: Output: default@t_hbase_maps_1
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
-  SELECT key,
-         map(tinyint_col, tinyint_col),
-         map(smallint_col, smallint_col),
-         map(int_col, int_col),
-         map(bigint_col, bigint_col),
-         map(float_col, float_col),
-         map(double_col, double_col),
-         map(key, string_col),
-         map(false, false)
-  FROM hbase_src
-  WHERE key = 126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_src
-POSTHOOK: Output: default@t_hbase_maps_1
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_hbase_maps_1 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_maps_1
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase_maps_1 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_maps_1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
-126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_3(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_3(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_maps_3
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_ext_hbase_maps_3 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_maps_3
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_3 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_maps_3
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
-126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_3
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_maps_3
-PREHOOK: Output: default@t_ext_hbase_maps_3
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_maps_3
-POSTHOOK: Output: default@t_ext_hbase_maps_3
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_4(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_4(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_maps_4
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_ext_hbase_maps_4 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_maps_4
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_4 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_maps_4
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{}	{}	{}	{}	{}	{}	{"125":"val_125"}	{}
-126	{}	{}	{}	{}	{}	{}	{"126":"val_126"}	{}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_4
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_maps_4
-PREHOOK: Output: default@t_ext_hbase_maps_4
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_maps_4
-POSTHOOK: Output: default@t_ext_hbase_maps_4
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_5
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_5(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES (
-"hbase.table.name"="t_hive_maps_1",
-"hbase.table.default.storage.type"="binary")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_5(key STRING,
-                                         tinyint_map_col MAP<TINYINT, TINYINT>,
-                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
-                                         int_map_col MAP<INT, INT>,
-                                         bigint_map_col MAP<BIGINT, BIGINT>,
-                                         float_map_col MAP<FLOAT, FLOAT>,
-                                         double_map_col MAP<DOUBLE, DOUBLE>,
-                                         string_map_col MAP<STRING, STRING>,
-                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
-TBLPROPERTIES (
-"hbase.table.name"="t_hive_maps_1",
-"hbase.table.default.storage.type"="binary")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_ext_hbase_maps_5
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: SELECT * FROM t_ext_hbase_maps_5 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_ext_hbase_maps_5
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_5 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_ext_hbase_maps_5
-#### A masked pattern was here ####
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
-126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
-PREHOOK: query: DROP TABLE t_ext_hbase_maps_5
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_ext_hbase_maps_5
-PREHOOK: Output: default@t_ext_hbase_maps_5
-POSTHOOK: query: DROP TABLE t_ext_hbase_maps_5
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_ext_hbase_maps_5
-POSTHOOK: Output: default@t_ext_hbase_maps_5
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_hbase_maps_1
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase_maps_1
-PREHOOK: Output: default@t_hbase_maps_1
-POSTHOOK: query: DROP TABLE t_hbase_maps_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase_maps_1
-POSTHOOK: Output: default@t_hbase_maps_1
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE t_hbase_maps
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase_maps
-PREHOOK: Output: default@t_hbase_maps
-POSTHOOK: query: DROP TABLE t_hbase_maps
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase_maps
-POSTHOOK: Output: default@t_hbase_maps
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: DROP TABLE hbase_src
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_src
-PREHOOK: Output: default@hbase_src
-POSTHOOK: query: DROP TABLE hbase_src
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_src
-POSTHOOK: Output: default@hbase_src
-POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/hbase_binary_storage_queries.q.out b/src/hbase-handler/src/test/results/hbase_binary_storage_queries.q.out
deleted file mode 100644
index 306e666..0000000
--- a/src/hbase-handler/src/test/results/hbase_binary_storage_queries.q.out
+++ /dev/null
@@ -1,658 +0,0 @@
-PREHOOK: query: DROP TABLE t_hbase
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE t_hbase(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive",
-               "hbase.table.default.storage.type" = "binary")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE t_hbase(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive",
-               "hbase.table.default.storage.type" = "binary")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: DESCRIBE FORMATTED t_hbase
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: DESCRIBE FORMATTED t_hbase
-POSTHOOK: type: DESCTABLE
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	from deserializer   
-tinyint_col         	tinyint             	from deserializer   
-smallint_col        	smallint            	from deserializer   
-int_col             	int                 	from deserializer   
-bigint_col          	bigint              	from deserializer   
-float_col           	float               	from deserializer   
-double_col          	double              	from deserializer   
-boolean_col         	boolean             	from deserializer   
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	hbase.table.default.storage.type	binary              
-	hbase.table.name    	t_hive              
-	storage_handler     	org.apache.hadoop.hive.hbase.HBaseStorageHandler
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.hbase.HBaseSerDe	 
-InputFormat:        	org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	hbase.columns.mapping	:key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-
-	serialization.format	1                   
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
-SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: SELECT * FROM t_hbase
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-user1	1	1	1	1	1.0	1.0	true
-user2	127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
-user3	-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
-PREHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase
-WHERE key='user1' OR key='user2' OR key='user3'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase
-WHERE key='user1' OR key='user2' OR key='user3'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-1	1	1	1	1.0	1.0	true
-127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
--128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
-PREHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase
-#### A masked pattern was here ####
-0	0	0	0	11.139999389648438	2.6663574368565E8	3
-PREHOOK: query: DROP TABLE t_hbase_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE t_hbase_1(key STRING,
-                                tinyint_col TINYINT,
-                                smallint_col SMALLINT,
-                                int_col INT,
-                                bigint_col BIGINT,
-                                float_col FLOAT,
-                                double_col DOUBLE,
-                                boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
-TBLPROPERTIES ("hbase.table.name" = "t_hive")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_hbase_1(key STRING,
-                                tinyint_col TINYINT,
-                                smallint_col SMALLINT,
-                                int_col INT,
-                                bigint_col BIGINT,
-                                float_col FLOAT,
-                                double_col DOUBLE,
-                                boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
-TBLPROPERTIES ("hbase.table.name" = "t_hive")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase_1
-PREHOOK: query: DESCRIBE FORMATTED t_hbase_1
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: DESCRIBE FORMATTED t_hbase_1
-POSTHOOK: type: DESCTABLE
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	from deserializer   
-tinyint_col         	tinyint             	from deserializer   
-smallint_col        	smallint            	from deserializer   
-int_col             	int                 	from deserializer   
-bigint_col          	bigint              	from deserializer   
-float_col           	float               	from deserializer   
-double_col          	double              	from deserializer   
-boolean_col         	boolean             	from deserializer   
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	EXTERNAL_TABLE      	 
-Table Parameters:	 	 
-	EXTERNAL            	TRUE                
-	hbase.table.name    	t_hive              
-	storage_handler     	org.apache.hadoop.hive.hbase.HBaseStorageHandler
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.hbase.HBaseSerDe	 
-InputFormat:        	org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	hbase.columns.mapping	:key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b
-	serialization.format	1                   
-PREHOOK: query: SELECT * FROM t_hbase_1
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_1
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase_1
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_1
-#### A masked pattern was here ####
-user1	1	1	1	1	1.0	1.0	true
-user2	127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
-user3	-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
-PREHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_1
-WHERE key='user1' OR key='user2' OR key='user3'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_1
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_1
-WHERE key='user1' OR key='user2' OR key='user3'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_1
-#### A masked pattern was here ####
-1	1	1	1	1.0	1.0	true
-127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
--128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
-PREHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_1
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_1
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_1
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_1
-#### A masked pattern was here ####
-0	0	0	0	11.139999389648438	2.6663574368565E8	3
-PREHOOK: query: DROP TABLE t_hbase_1
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase_1
-PREHOOK: Output: default@t_hbase_1
-POSTHOOK: query: DROP TABLE t_hbase_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase_1
-POSTHOOK: Output: default@t_hbase_1
-PREHOOK: query: DROP TABLE t_hbase
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase
-PREHOOK: Output: default@t_hbase
-POSTHOOK: query: DROP TABLE t_hbase
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase
-POSTHOOK: Output: default@t_hbase
-PREHOOK: query: DROP TABLE t_hbase_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE t_hbase_2(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE t_hbase_2(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase_2
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase_2
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase_2
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase_2
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase_2
-PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@t_hbase_2
-POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
-SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
-FROM src
-WHERE key=100 OR key=125 OR key=126
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@t_hbase_2
-PREHOOK: query: SELECT * FROM t_hbase_2
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase_2
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_2
-#### A masked pattern was here ####
-user1	1	1	1	1	1.0	1.0	true
-user2	127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
-user3	-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
-PREHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_2
-WHERE key='user1' OR key='user2' OR key='user3'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_2
-WHERE key='user1' OR key='user2' OR key='user3'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_2
-#### A masked pattern was here ####
-1	1	1	1	1.0	1.0	true
-127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
--128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
-PREHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_2
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_2
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_2
-#### A masked pattern was here ####
-0	0	0	0	11.139999389648438	2.6663574368565E8	3
-PREHOOK: query: DROP TABLE t_hbase_3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase_3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE t_hbase_3(key STRING,
-                                tinyint_col TINYINT,
-                                smallint_col SMALLINT,
-                                int_col INT,
-                                bigint_col BIGINT,
-                                float_col FLOAT,
-                                double_col DOUBLE,
-                                boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
-TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_hbase_3(key STRING,
-                                tinyint_col TINYINT,
-                                smallint_col SMALLINT,
-                                int_col INT,
-                                bigint_col BIGINT,
-                                float_col FLOAT,
-                                double_col DOUBLE,
-                                boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
-TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase_3
-PREHOOK: query: SELECT * FROM t_hbase_3
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_3
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase_3
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_3
-#### A masked pattern was here ####
-user1	49	NULL	NULL	NULL	NULL	NULL	true
-user2	49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
-user3	45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
-PREHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_3
-WHERE key='user1' OR key='user2' OR key='user3'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_3
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_3
-WHERE key='user1' OR key='user2' OR key='user3'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_3
-#### A masked pattern was here ####
-49	NULL	NULL	NULL	NULL	NULL	true
-49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
-45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
-PREHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_3
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_3
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_3
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_3
-#### A masked pattern was here ####
-143	24677	1600349547	7380102801049413986	1.0324066977186741E-8	5.6030888442763564E-67	3
-PREHOOK: query: DROP TABLE t_hbase_3
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase_3
-PREHOOK: Output: default@t_hbase_3
-POSTHOOK: query: DROP TABLE t_hbase_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase_3
-POSTHOOK: Output: default@t_hbase_3
-PREHOOK: query: DROP TABLE t_hbase_4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE t_hbase_4
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE t_hbase_4(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES (
-"hbase.table.name" = "t_hive_2",
-"hbase.table.default.storage.type" = "binary")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE t_hbase_4(key STRING,
-                     tinyint_col TINYINT,
-                     smallint_col SMALLINT,
-                     int_col INT,
-                     bigint_col BIGINT,
-                     float_col FLOAT,
-                     double_col DOUBLE,
-                     boolean_col BOOLEAN)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
-TBLPROPERTIES (
-"hbase.table.name" = "t_hive_2",
-"hbase.table.default.storage.type" = "binary")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@t_hbase_4
-PREHOOK: query: SELECT * FROM t_hbase_4
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_4
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM t_hbase_4
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_4
-#### A masked pattern was here ####
-user1	49	NULL	NULL	NULL	NULL	NULL	true
-user2	49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
-user3	45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
-PREHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_4
-WHERE key='user1' OR key='user2' OR key='user3'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_4
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT tinyint_col,
-       smallint_col,
-       int_col,
-       bigint_col,
-       float_col,
-       double_col,
-       boolean_col
-FROM t_hbase_4
-WHERE key='user1' OR key='user2' OR key='user3'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_4
-#### A masked pattern was here ####
-49	NULL	NULL	NULL	NULL	NULL	true
-49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
-45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
-PREHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_4
-PREHOOK: type: QUERY
-PREHOOK: Input: default@t_hbase_4
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT sum(tinyint_col),
-       sum(smallint_col),
-       sum(int_col),
-       sum(bigint_col),
-       sum(float_col),
-       sum(double_col),
-       count(boolean_col)
-FROM t_hbase_4
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@t_hbase_4
-#### A masked pattern was here ####
-143	24677	1600349547	7380102801049413986	1.0324066977186741E-8	5.6030888442763564E-67	3
-PREHOOK: query: DROP TABLE t_hbase_4
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase_4
-PREHOOK: Output: default@t_hbase_4
-POSTHOOK: query: DROP TABLE t_hbase_4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase_4
-POSTHOOK: Output: default@t_hbase_4
-PREHOOK: query: DROP TABLE t_hbase_2
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@t_hbase_2
-PREHOOK: Output: default@t_hbase_2
-POSTHOOK: query: DROP TABLE t_hbase_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@t_hbase_2
-POSTHOOK: Output: default@t_hbase_2
diff --git a/src/hbase-handler/src/test/results/hbase_bulk.m.out b/src/hbase-handler/src/test/results/hbase_bulk.m.out
deleted file mode 100644
index 21f4e10..0000000
--- a/src/hbase-handler/src/test/results/hbase_bulk.m.out
+++ /dev/null
@@ -1,131 +0,0 @@
-PREHOOK: query: drop table hbsort
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hbsort
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table hbpartition
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table hbpartition
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: -- this is a dummy table used for controlling how the HFiles are
--- created
-create table hbsort(key string, val string, val2 string)
-stored as
-INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
-OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
-#### A masked pattern was here ####
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: -- this is a dummy table used for controlling how the HFiles are
--- created
-create table hbsort(key string, val string, val2 string)
-stored as
-INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
-OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
-#### A masked pattern was here ####
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbsort
-PREHOOK: query: -- this is a dummy table used for controlling how the input file
--- for TotalOrderPartitioner is created
-create table hbpartition(part_break string)
-row format serde
-'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
-stored as
-inputformat
-'org.apache.hadoop.mapred.TextInputFormat'
-outputformat
-'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat'
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: -- this is a dummy table used for controlling how the input file
--- for TotalOrderPartitioner is created
-create table hbpartition(part_break string)
-row format serde
-'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
-stored as
-inputformat
-'org.apache.hadoop.mapred.TextInputFormat'
-outputformat
-'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat'
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbpartition
-PREHOOK: query: -- this should produce one file, but we do not
--- know what it will be called, so we will copy it to a well known
-#### A masked pattern was here ####
-insert overwrite table hbpartition
-select distinct value
-from src
-where value='val_100' or value='val_200'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbpartition
-POSTHOOK: query: -- this should produce one file, but we do not
--- know what it will be called, so we will copy it to a well known
-#### A masked pattern was here ####
-insert overwrite table hbpartition
-select distinct value
-from src
-where value='val_100' or value='val_200'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbpartition
-POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-#### A masked pattern was here ####
--- include some trailing blanks and nulls to make sure we handle them correctly
-insert overwrite table hbsort
-select distinct value,
-  case when key=103 then cast(null as string) else key end,
-  case when key=103 then ''
-       else cast(key+1 as string) end
-from src
-cluster by value
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbsort
-#### A masked pattern was here ####
--- include some trailing blanks and nulls to make sure we handle them correctly
-insert overwrite table hbsort
-select distinct value,
-  case when key=103 then cast(null as string) else key end,
-  case when key=103 then ''
-       else cast(key+1 as string) end
-from src
-cluster by value
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbsort
-POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: -- To get the files out to your local filesystem for loading into
-#### A masked pattern was here ####
--- semicolon-terminate the line below before running this test:
-#### A masked pattern was here ####
-
-drop table hbsort
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbsort
-PREHOOK: Output: default@hbsort
-POSTHOOK: query: -- To get the files out to your local filesystem for loading into
-#### A masked pattern was here ####
--- semicolon-terminate the line below before running this test:
-#### A masked pattern was here ####
-
-drop table hbsort
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbsort
-POSTHOOK: Output: default@hbsort
-POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: drop table hbpartition
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbpartition
-PREHOOK: Output: default@hbpartition
-POSTHOOK: query: drop table hbpartition
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbpartition
-POSTHOOK: Output: default@hbpartition
-POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/hbase_joins.q.out b/src/hbase-handler/src/test/results/hbase_joins.q.out
deleted file mode 100644
index 2c2f89d..0000000
--- a/src/hbase-handler/src/test/results/hbase_joins.q.out
+++ /dev/null
@@ -1,267 +0,0 @@
-PREHOOK: query: DROP TABLE users
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE users
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE states
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE states
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE countries
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE countries
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: DROP TABLE users_level
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE users_level
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: -- From HIVE-1257
-
-CREATE TABLE users(key string, state string, country string, country_id int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "info:state,info:country,info:country_id"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: -- From HIVE-1257
-
-CREATE TABLE users(key string, state string, country string, country_id int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "info:state,info:country,info:country_id"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@users
-PREHOOK: query: CREATE TABLE states(key string, name string)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "state:name"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE states(key string, name string)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "state:name"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@states
-PREHOOK: query: CREATE TABLE countries(key string, name string, country string, country_id int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "info:name,info:country,info:country_id"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE countries(key string, name string, country string, country_id int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "info:name,info:country,info:country_id"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@countries
-PREHOOK: query: INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
-FROM src WHERE key=100
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@users
-POSTHOOK: query: INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
-FROM src WHERE key=100
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@users
-PREHOOK: query: INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
-FROM src WHERE key=100
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@states
-POSTHOOK: query: INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
-FROM src WHERE key=100
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@states
-PREHOOK: query: INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
-FROM src WHERE key=100
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@countries
-POSTHOOK: query: INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
-FROM src WHERE key=100
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@countries
-PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country = c.key)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@countries
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country = c.key)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@countries
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-user1	USA	United States	USA
-PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
-ON (u.country = c.country)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@countries
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
-ON (u.country = c.country)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@countries
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-user1	USA	United States	USA
-PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country_id = c.country_id)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@countries
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country_id = c.country_id)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@countries
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-PREHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
-ON (u.state = s.key)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@states
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
-ON (u.state = s.key)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@states
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-user1	IA	Iowa
-PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country = c.key)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@countries
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country = c.key)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@countries
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-user1	USA	United States	USA
-PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
-ON (u.country = c.country)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@countries
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
-ON (u.country = c.country)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@countries
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-user1	USA	United States	USA
-PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country_id = c.country_id)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@countries
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
-ON (u.country_id = c.country_id)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@countries
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-PREHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
-ON (u.state = s.key)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@states
-PREHOOK: Input: default@users
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
-ON (u.state = s.key)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@states
-POSTHOOK: Input: default@users
-#### A masked pattern was here ####
-user1	IA	Iowa
-PREHOOK: query: DROP TABLE users
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@users
-PREHOOK: Output: default@users
-POSTHOOK: query: DROP TABLE users
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@users
-POSTHOOK: Output: default@users
-PREHOOK: query: DROP TABLE states
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@states
-PREHOOK: Output: default@states
-POSTHOOK: query: DROP TABLE states
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@states
-POSTHOOK: Output: default@states
-PREHOOK: query: DROP TABLE countries
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@countries
-PREHOOK: Output: default@countries
-POSTHOOK: query: DROP TABLE countries
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@countries
-POSTHOOK: Output: default@countries
-PREHOOK: query: CREATE TABLE users(key int, userid int, username string, created int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:nickname,f:created")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE users(key int, userid int, username string, created int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:nickname,f:created")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@users
-PREHOOK: query: CREATE TABLE users_level(key int, userid int, level int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:level")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE users_level(key int, userid int, level int)
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:level")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@users_level
-PREHOOK: query: -- HIVE-1903:  the problem fixed here showed up even without any data,
--- so no need to load any to test it
-SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num 
- FROM users JOIN users_level ON (users.userid = users_level.userid) 
- GROUP BY year(from_unixtime(users.created)), level
-PREHOOK: type: QUERY
-PREHOOK: Input: default@users
-PREHOOK: Input: default@users_level
-#### A masked pattern was here ####
-POSTHOOK: query: -- HIVE-1903:  the problem fixed here showed up even without any data,
--- so no need to load any to test it
-SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num 
- FROM users JOIN users_level ON (users.userid = users_level.userid) 
- GROUP BY year(from_unixtime(users.created)), level
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@users
-POSTHOOK: Input: default@users_level
-#### A masked pattern was here ####
-PREHOOK: query: DROP TABLE users
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@users
-PREHOOK: Output: default@users
-POSTHOOK: query: DROP TABLE users
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@users
-POSTHOOK: Output: default@users
-PREHOOK: query: DROP TABLE users_level
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@users_level
-PREHOOK: Output: default@users_level
-POSTHOOK: query: DROP TABLE users_level
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@users_level
-POSTHOOK: Output: default@users_level
diff --git a/src/hbase-handler/src/test/results/hbase_ppd_key_range.q.out b/src/hbase-handler/src/test/results/hbase_ppd_key_range.q.out
deleted file mode 100644
index b35860f..0000000
--- a/src/hbase-handler/src/test/results/hbase_ppd_key_range.q.out
+++ /dev/null
@@ -1,651 +0,0 @@
-PREHOOK: query: CREATE TABLE hbase_pushdown(key string, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_pushdown(key string, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_pushdown
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
-SELECT cast(key as string), value
-FROM src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_pushdown
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
-SELECT cast(key as string), value
-FROM src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_pushdown
-PREHOOK: query: -- with full pushdown
-explain select * from hbase_pushdown where key>'90'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with full pushdown
-explain select * from hbase_pushdown where key>'90'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_TABLE_OR_COL key) '90'))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: (key > '90')
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (key > '90')
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown where key>'90'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key>'90'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-92	val_92
-95	val_95
-96	val_96
-97	val_97
-98	val_98
-PREHOOK: query: select * from hbase_pushdown where key<'1'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key<'1'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-0	val_0
-PREHOOK: query: select * from hbase_pushdown where key<='2'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key<='2'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-0	val_0
-10	val_10
-100	val_100
-103	val_103
-104	val_104
-105	val_105
-11	val_11
-111	val_111
-113	val_113
-114	val_114
-116	val_116
-118	val_118
-119	val_119
-12	val_12
-120	val_120
-125	val_125
-126	val_126
-128	val_128
-129	val_129
-131	val_131
-133	val_133
-134	val_134
-136	val_136
-137	val_137
-138	val_138
-143	val_143
-145	val_145
-146	val_146
-149	val_149
-15	val_15
-150	val_150
-152	val_152
-153	val_153
-155	val_155
-156	val_156
-157	val_157
-158	val_158
-160	val_160
-162	val_162
-163	val_163
-164	val_164
-165	val_165
-166	val_166
-167	val_167
-168	val_168
-169	val_169
-17	val_17
-170	val_170
-172	val_172
-174	val_174
-175	val_175
-176	val_176
-177	val_177
-178	val_178
-179	val_179
-18	val_18
-180	val_180
-181	val_181
-183	val_183
-186	val_186
-187	val_187
-189	val_189
-19	val_19
-190	val_190
-191	val_191
-192	val_192
-193	val_193
-194	val_194
-195	val_195
-196	val_196
-197	val_197
-199	val_199
-2	val_2
-PREHOOK: query: select * from hbase_pushdown where key>='90'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key>='90'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-90	val_90
-92	val_92
-95	val_95
-96	val_96
-97	val_97
-98	val_98
-PREHOOK: query: -- with cnostant expressinon
-explain select * from hbase_pushdown where key>=cast(40 + 50 as string)
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with cnostant expressinon
-explain select * from hbase_pushdown where key>=cast(40 + 50 as string)
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (>= (TOK_TABLE_OR_COL key) (TOK_FUNCTION TOK_STRING (+ 40 50))))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: (key >= UDFToString((40 + 50)))
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (key >= UDFToString((40 + 50)))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown where key>=cast(40 + 50 as string)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key>=cast(40 + 50 as string)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-90	val_90
-92	val_92
-95	val_95
-96	val_96
-97	val_97
-98	val_98
-PREHOOK: query: -- with partial pushdown
-
-explain select * from hbase_pushdown where key>'90' and value like '%9%'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with partial pushdown
-
-explain select * from hbase_pushdown where key>'90' and value like '%9%'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) '90') (like (TOK_TABLE_OR_COL value) '%9%')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: (key > '90')
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (value like '%9%')
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown where key>'90' and value like '%9%'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key>'90' and value like '%9%'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-92	val_92
-95	val_95
-96	val_96
-97	val_97
-98	val_98
-PREHOOK: query: -- with two residuals
-
-explain select * from hbase_pushdown
-where key>='90' and value like '%9%' and key=cast(value as int)
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with two residuals
-
-explain select * from hbase_pushdown
-where key>='90' and value like '%9%' and key=cast(value as int)
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (>= (TOK_TABLE_OR_COL key) '90') (like (TOK_TABLE_OR_COL value) '%9%')) (= (TOK_TABLE_OR_COL key) (TOK_FUNCTION TOK_INT (TOK_TABLE_OR_COL value)))))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: (key >= '90')
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: ((value like '%9%') and (key = UDFToInteger(value)))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown
-where key>='90' and value like '%9%' and key=cast(value as int)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown
-where key>='90' and value like '%9%' and key=cast(value as int)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-PREHOOK: query: -- with contradictory pushdowns
-
-explain select * from hbase_pushdown
-where key<'80' and key>'90' and value like '%90%'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with contradictory pushdowns
-
-explain select * from hbase_pushdown
-where key<'80' and key>'90' and value like '%90%'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (< (TOK_TABLE_OR_COL key) '80') (> (TOK_TABLE_OR_COL key) '90')) (like (TOK_TABLE_OR_COL value) '%90%')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: ((key < '80') and (key > '90'))
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (value like '%90%')
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown
-where key<'80' and key>'90' and value like '%90%'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown
-where key<'80' and key>'90' and value like '%90%'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-PREHOOK: query: -- with nothing to push down
-
-explain select * from hbase_pushdown
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with nothing to push down
-
-explain select * from hbase_pushdown
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))
-
-STAGE DEPENDENCIES:
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: -- with a predicate which is not actually part of the filter, so
--- it should be ignored by pushdown
-
-explain select * from hbase_pushdown
-where (case when key<'90' then 2 else 4 end) > 3
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with a predicate which is not actually part of the filter, so
--- it should be ignored by pushdown
-
-explain select * from hbase_pushdown
-where (case when key<'90' then 2 else 4 end) > 3
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_FUNCTION when (< (TOK_TABLE_OR_COL key) '90') 2 4) 3))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            Filter Operator
-              predicate:
-                  expr: (CASE WHEN ((key < '90')) THEN (2) ELSE (4) END > 3)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: -- with a predicate which is under an OR, so it should
--- be ignored by pushdown
-
-explain select * from hbase_pushdown
-where key<='80' or value like '%90%'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with a predicate which is under an OR, so it should
--- be ignored by pushdown
-
-explain select * from hbase_pushdown
-where key<='80' or value like '%90%'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (<= (TOK_TABLE_OR_COL key) '80') (like (TOK_TABLE_OR_COL value) '%90%')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            Filter Operator
-              predicate:
-                  expr: ((key <= '80') or (value like '%90%'))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: -- following will get pushed into hbase after HIVE-2819
-explain select * from hbase_pushdown where key > '281' 
-and key < '287'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- following will get pushed into hbase after HIVE-2819
-explain select * from hbase_pushdown where key > '281' 
-and key < '287'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) '281') (< (TOK_TABLE_OR_COL key) '287')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: ((key > '281') and (key < '287'))
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: ((key > '281') and (key < '287'))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown where key > '281' 
-and key < '287'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key > '281' 
-and key < '287'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-282	val_282
-283	val_283
-284	val_284
-285	val_285
-286	val_286
-PREHOOK: query: -- with pushdown disabled
-
-explain select * from hbase_pushdown where key<='90'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with pushdown disabled
-
-explain select * from hbase_pushdown where key<='90'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (<= (TOK_TABLE_OR_COL key) '90'))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            Filter Operator
-              predicate:
-                  expr: (key <= '90')
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
diff --git a/src/hbase-handler/src/test/results/hbase_pushdown.q.out b/src/hbase-handler/src/test/results/hbase_pushdown.q.out
deleted file mode 100644
index 45927dd..0000000
--- a/src/hbase-handler/src/test/results/hbase_pushdown.q.out
+++ /dev/null
@@ -1,403 +0,0 @@
-PREHOOK: query: CREATE TABLE hbase_pushdown(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_pushdown(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_pushdown
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
-SELECT *
-FROM src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_pushdown
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
-SELECT *
-FROM src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_pushdown
-PREHOOK: query: -- with full pushdown
-explain select * from hbase_pushdown where key=90
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with full pushdown
-explain select * from hbase_pushdown where key=90
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (TOK_TABLE_OR_COL key) 90))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: (key = 90)
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (key = 90)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown where key=90
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key=90
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-90	val_90
-PREHOOK: query: -- with partial pushdown
-
-explain select * from hbase_pushdown where key=90 and value like '%90%'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with partial pushdown
-
-explain select * from hbase_pushdown where key=90 and value like '%90%'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (= (TOK_TABLE_OR_COL key) 90) (like (TOK_TABLE_OR_COL value) '%90%')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: (key = 90)
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (value like '%90%')
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown where key=90 and value like '%90%'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown where key=90 and value like '%90%'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-90	val_90
-PREHOOK: query: -- with two residuals
-
-explain select * from hbase_pushdown
-where key=90 and value like '%90%' and key=cast(value as int)
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with two residuals
-
-explain select * from hbase_pushdown
-where key=90 and value like '%90%' and key=cast(value as int)
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL key) 90) (like (TOK_TABLE_OR_COL value) '%90%')) (= (TOK_TABLE_OR_COL key) (TOK_FUNCTION TOK_INT (TOK_TABLE_OR_COL value)))))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            filterExpr:
-                expr: (key = 90)
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: ((value like '%90%') and (key = UDFToInteger(value)))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: -- with contradictory pushdowns
-
-explain select * from hbase_pushdown
-where key=80 and key=90 and value like '%90%'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with contradictory pushdowns
-
-explain select * from hbase_pushdown
-where key=80 and key=90 and value like '%90%'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL key) 80) (= (TOK_TABLE_OR_COL key) 90)) (like (TOK_TABLE_OR_COL value) '%90%')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            Filter Operator
-              predicate:
-                  expr: (((key = 80) and (key = 90)) and (value like '%90%'))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_pushdown
-where key=80 and key=90 and value like '%90%'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_pushdown
-where key=80 and key=90 and value like '%90%'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_pushdown
-#### A masked pattern was here ####
-PREHOOK: query: -- with nothing to push down
-
-explain select * from hbase_pushdown
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with nothing to push down
-
-explain select * from hbase_pushdown
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))
-
-STAGE DEPENDENCIES:
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: -- with a predicate which is not actually part of the filter, so
--- it should be ignored by pushdown
-
-explain select * from hbase_pushdown
-where (case when key=90 then 2 else 4 end) > 3
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with a predicate which is not actually part of the filter, so
--- it should be ignored by pushdown
-
-explain select * from hbase_pushdown
-where (case when key=90 then 2 else 4 end) > 3
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_FUNCTION when (= (TOK_TABLE_OR_COL key) 90) 2 4) 3))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            Filter Operator
-              predicate:
-                  expr: (CASE WHEN ((key = 90)) THEN (2) ELSE (4) END > 3)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: -- with a predicate which is under an OR, so it should
--- be ignored by pushdown
-
-explain select * from hbase_pushdown
-where key=80 or value like '%90%'
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with a predicate which is under an OR, so it should
--- be ignored by pushdown
-
-explain select * from hbase_pushdown
-where key=80 or value like '%90%'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (= (TOK_TABLE_OR_COL key) 80) (like (TOK_TABLE_OR_COL value) '%90%')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            Filter Operator
-              predicate:
-                  expr: ((key = 80) or (value like '%90%'))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: -- with pushdown disabled
-
-explain select * from hbase_pushdown where key=90
-PREHOOK: type: QUERY
-POSTHOOK: query: -- with pushdown disabled
-
-explain select * from hbase_pushdown where key=90
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (TOK_TABLE_OR_COL key) 90))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_pushdown 
-          TableScan
-            alias: hbase_pushdown
-            Filter Operator
-              predicate:
-                  expr: (key = 90)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
diff --git a/src/hbase-handler/src/test/results/hbase_queries.q.out b/src/hbase-handler/src/test/results/hbase_queries.q.out
deleted file mode 100644
index 6a4f68b..0000000
--- a/src/hbase-handler/src/test/results/hbase_queries.q.out
+++ /dev/null
@@ -1,980 +0,0 @@
-PREHOOK: query: DROP TABLE hbase_table_1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_table_1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE hbase_table_1(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_table_1(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_1
-PREHOOK: query: DESCRIBE EXTENDED hbase_table_1
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: DESCRIBE EXTENDED hbase_table_1
-POSTHOOK: type: DESCTABLE
-key	int	from deserializer
-value	string	from deserializer
-	 	 
-#### A masked pattern was here ####
-PREHOOK: query: select * from hbase_table_1
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_1
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_table_1
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_1
-#### A masked pattern was here ####
-PREHOOK: query: EXPLAIN FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME hbase_table_1))) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (% (TOK_TABLE_OR_COL key) 2) 0))))
-
-STAGE DEPENDENCIES:
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-0
-    Map Reduce
-      Alias -> Map Operator Tree:
-        src 
-          TableScan
-            alias: src
-            Filter Operator
-              predicate:
-                  expr: ((key % 2) = 0)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                Select Operator
-                  expressions:
-                        expr: UDFToInteger(_col0)
-                        type: int
-                        expr: _col1
-                        type: string
-                  outputColumnNames: _col0, _col1
-                  File Output Operator
-                    compressed: false
-                    GlobalTableId: 1
-                    table:
-                        input format: org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat
-                        output format: org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat
-                        serde: org.apache.hadoop.hive.hbase.HBaseSerDe
-                        name: default.hbase_table_1
-
-
-PREHOOK: query: FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_table_1
-POSTHOOK: query: FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_table_1
-PREHOOK: query: DROP TABLE hbase_table_2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_table_2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE hbase_table_2(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE hbase_table_2(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_2
-PREHOOK: query: EXPLAIN 
-SELECT Y.* 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.* FROM src) Y
-ON (x.key = Y.key)
-ORDER BY key, value LIMIT 20
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN 
-SELECT Y.* 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.* FROM src) Y
-ON (x.key = Y.key)
-ORDER BY key, value LIMIT 20
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_1)))))) x) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME src)))))) Y) (= (. (TOK_TABLE_OR_COL x) key) (. (TOK_TABLE_OR_COL Y) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME Y)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 20)))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-2 depends on stages: Stage-1
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        x:hbase_table_1 
-          TableScan
-            alias: hbase_table_1
-            Select Operator
-              expressions:
-                    expr: key
-                    type: int
-              outputColumnNames: _col0
-              Reduce Output Operator
-                key expressions:
-                      expr: UDFToDouble(_col0)
-                      type: double
-                sort order: +
-                Map-reduce partition columns:
-                      expr: UDFToDouble(_col0)
-                      type: double
-                tag: 0
-        y:src 
-          TableScan
-            alias: src
-            Select Operator
-              expressions:
-                    expr: key
-                    type: string
-                    expr: value
-                    type: string
-              outputColumnNames: _col0, _col1
-              Reduce Output Operator
-                key expressions:
-                      expr: UDFToDouble(_col0)
-                      type: double
-                sort order: +
-                Map-reduce partition columns:
-                      expr: UDFToDouble(_col0)
-                      type: double
-                tag: 1
-                value expressions:
-                      expr: _col0
-                      type: string
-                      expr: _col1
-                      type: string
-      Reduce Operator Tree:
-        Join Operator
-          condition map:
-               Inner Join 0 to 1
-          condition expressions:
-            0 
-            1 {VALUE._col0} {VALUE._col1}
-          handleSkewJoin: false
-          outputColumnNames: _col2, _col3
-          Select Operator
-            expressions:
-                  expr: _col2
-                  type: string
-                  expr: _col3
-                  type: string
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 0
-              table:
-                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-
-  Stage: Stage-2
-    Map Reduce
-      Alias -> Map Operator Tree:
-#### A masked pattern was here ####
-            Reduce Output Operator
-              key expressions:
-                    expr: _col0
-                    type: string
-                    expr: _col1
-                    type: string
-              sort order: ++
-              tag: -1
-              value expressions:
-                    expr: _col0
-                    type: string
-                    expr: _col1
-                    type: string
-      Reduce Operator Tree:
-        Extract
-          Limit
-            File Output Operator
-              compressed: false
-              GlobalTableId: 0
-              table:
-                  input format: org.apache.hadoop.mapred.TextInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: 20
-
-
-PREHOOK: query: SELECT Y.* 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.* FROM src) Y
-ON (x.key = Y.key)
-ORDER BY key, value LIMIT 20
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_1
-PREHOOK: Input: default@src
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT Y.* 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.* FROM src) Y
-ON (x.key = Y.key)
-ORDER BY key, value LIMIT 20
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_1
-POSTHOOK: Input: default@src
-#### A masked pattern was here ####
-0	val_0
-0	val_0
-0	val_0
-10	val_10
-100	val_100
-100	val_100
-104	val_104
-104	val_104
-114	val_114
-116	val_116
-118	val_118
-118	val_118
-12	val_12
-12	val_12
-120	val_120
-120	val_120
-126	val_126
-128	val_128
-128	val_128
-128	val_128
-PREHOOK: query: EXPLAIN 
-SELECT Y.*
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
-JOIN 
-(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
-ON (x.key = Y.key)
-ORDER BY key, value
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN 
-SELECT Y.*
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
-JOIN 
-(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
-ON (x.key = Y.key)
-ORDER BY key, value
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_1)))) (TOK_WHERE (> (. (TOK_TABLE_OR_COL hbase_table_1) key) 100)))) x) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_2))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_2)))) (TOK_WHERE (< (. (TOK_TABLE_OR_COL hbase_table_2) key) 120)))) Y) (= (. (TOK_TABLE_OR_COL x) key) (. (TOK_TABLE_OR_COL Y) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME Y)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-2 depends on stages: Stage-1
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        x:hbase_table_1 
-          TableScan
-            alias: hbase_table_1
-            Filter Operator
-              predicate:
-                  expr: (key > 100)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                outputColumnNames: _col0
-                Reduce Output Operator
-                  key expressions:
-                        expr: _col0
-                        type: int
-                  sort order: +
-                  Map-reduce partition columns:
-                        expr: _col0
-                        type: int
-                  tag: 0
-        y:hbase_table_2 
-          TableScan
-            alias: hbase_table_2
-            Filter Operator
-              predicate:
-                  expr: (key < 120)
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                Reduce Output Operator
-                  key expressions:
-                        expr: _col0
-                        type: int
-                  sort order: +
-                  Map-reduce partition columns:
-                        expr: _col0
-                        type: int
-                  tag: 1
-                  value expressions:
-                        expr: _col0
-                        type: int
-                        expr: _col1
-                        type: string
-      Reduce Operator Tree:
-        Join Operator
-          condition map:
-               Inner Join 0 to 1
-          condition expressions:
-            0 
-            1 {VALUE._col0} {VALUE._col1}
-          handleSkewJoin: false
-          outputColumnNames: _col2, _col3
-          Select Operator
-            expressions:
-                  expr: _col2
-                  type: int
-                  expr: _col3
-                  type: string
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 0
-              table:
-                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-
-  Stage: Stage-2
-    Map Reduce
-      Alias -> Map Operator Tree:
-#### A masked pattern was here ####
-            Reduce Output Operator
-              key expressions:
-                    expr: _col0
-                    type: int
-                    expr: _col1
-                    type: string
-              sort order: ++
-              tag: -1
-              value expressions:
-                    expr: _col0
-                    type: int
-                    expr: _col1
-                    type: string
-      Reduce Operator Tree:
-        Extract
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: SELECT Y.*
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
-JOIN 
-(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
-ON (x.key = Y.key)
-ORDER BY key,value
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_1
-PREHOOK: Input: default@hbase_table_2
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT Y.*
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
-JOIN 
-(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
-ON (x.key = Y.key)
-ORDER BY key,value
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_1
-POSTHOOK: Input: default@hbase_table_2
-#### A masked pattern was here ####
-104	val_104
-114	val_114
-116	val_116
-118	val_118
-PREHOOK: query: DROP TABLE empty_hbase_table
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE empty_hbase_table
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE empty_hbase_table(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE empty_hbase_table(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@empty_hbase_table
-PREHOOK: query: DROP TABLE empty_normal_table
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE empty_normal_table
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE empty_normal_table(key int, value string)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE empty_normal_table(key int, value string)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@empty_normal_table
-PREHOOK: query: select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_hbase_table) x order by c
-PREHOOK: type: QUERY
-PREHOOK: Input: default@empty_hbase_table
-PREHOOK: Input: default@empty_normal_table
-#### A masked pattern was here ####
-POSTHOOK: query: select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_hbase_table) x order by c
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@empty_hbase_table
-POSTHOOK: Input: default@empty_normal_table
-#### A masked pattern was here ####
-0
-0
-PREHOOK: query: select * from (select count(1) c from empty_normal_table union all select count(1) as c from hbase_table_1) x order by c
-PREHOOK: type: QUERY
-PREHOOK: Input: default@empty_normal_table
-PREHOOK: Input: default@hbase_table_1
-#### A masked pattern was here ####
-POSTHOOK: query: select * from (select count(1) c from empty_normal_table union all select count(1) as c from hbase_table_1) x order by c
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@empty_normal_table
-POSTHOOK: Input: default@hbase_table_1
-#### A masked pattern was here ####
-0
-155
-PREHOOK: query: select * from (select count(1) c from src union all select count(1) as c from empty_hbase_table) x order by c
-PREHOOK: type: QUERY
-PREHOOK: Input: default@empty_hbase_table
-PREHOOK: Input: default@src
-#### A masked pattern was here ####
-POSTHOOK: query: select * from (select count(1) c from src union all select count(1) as c from empty_hbase_table) x order by c
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@empty_hbase_table
-POSTHOOK: Input: default@src
-#### A masked pattern was here ####
-0
-500
-PREHOOK: query: select * from (select count(1) c from src union all select count(1) as c from hbase_table_1) x order by c
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_1
-PREHOOK: Input: default@src
-#### A masked pattern was here ####
-POSTHOOK: query: select * from (select count(1) c from src union all select count(1) as c from hbase_table_1) x order by c
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_1
-POSTHOOK: Input: default@src
-#### A masked pattern was here ####
-155
-500
-PREHOOK: query: CREATE TABLE hbase_table_3(key int, value string, count int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "cf:val,cf2:count"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_table_3(key int, value string, count int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "cf:val,cf2:count"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_3
-PREHOOK: query: EXPLAIN 
-INSERT OVERWRITE TABLE hbase_table_3
-SELECT x.key, x.value, Y.count 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
-ON (x.key = Y.key)
-PREHOOK: type: QUERY
-POSTHOOK: query: EXPLAIN 
-INSERT OVERWRITE TABLE hbase_table_3
-SELECT x.key, x.value, Y.count 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
-ON (x.key = Y.key)
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_1)))))) x) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) key)) (TOK_SELEXPR (TOK_FUNCTION count (. (TOK_TABLE_OR_COL src) key)) count)) (TOK_GROUPBY (. (TOK_TABLE_OR_COL src) key)))) Y) (= (. (TOK_TABLE_OR_COL x) key) (. (TOK_TABLE_OR_COL Y) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME hbase_table_3))) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL x) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL x) value)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL Y) count)))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        y:src 
-          TableScan
-            alias: src
-            Select Operator
-              expressions:
-                    expr: key
-                    type: string
-              outputColumnNames: key
-              Group By Operator
-                aggregations:
-                      expr: count(key)
-                bucketGroup: false
-                keys:
-                      expr: key
-                      type: string
-                mode: hash
-                outputColumnNames: _col0, _col1
-                Reduce Output Operator
-                  key expressions:
-                        expr: _col0
-                        type: string
-                  sort order: +
-                  Map-reduce partition columns:
-                        expr: _col0
-                        type: string
-                  tag: -1
-                  value expressions:
-                        expr: _col1
-                        type: bigint
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations:
-                expr: count(VALUE._col0)
-          bucketGroup: false
-          keys:
-                expr: KEY._col0
-                type: string
-          mode: mergepartial
-          outputColumnNames: _col0, _col1
-          Select Operator
-            expressions:
-                  expr: _col0
-                  type: string
-                  expr: _col1
-                  type: bigint
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 0
-              table:
-                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-
-  Stage: Stage-0
-    Map Reduce
-      Alias -> Map Operator Tree:
-        $INTNAME 
-            Reduce Output Operator
-              key expressions:
-                    expr: UDFToDouble(_col0)
-                    type: double
-              sort order: +
-              Map-reduce partition columns:
-                    expr: UDFToDouble(_col0)
-                    type: double
-              tag: 1
-              value expressions:
-                    expr: _col1
-                    type: bigint
-        x:hbase_table_1 
-          TableScan
-            alias: hbase_table_1
-            Select Operator
-              expressions:
-                    expr: key
-                    type: int
-                    expr: value
-                    type: string
-              outputColumnNames: _col0, _col1
-              Reduce Output Operator
-                key expressions:
-                      expr: UDFToDouble(_col0)
-                      type: double
-                sort order: +
-                Map-reduce partition columns:
-                      expr: UDFToDouble(_col0)
-                      type: double
-                tag: 0
-                value expressions:
-                      expr: _col0
-                      type: int
-                      expr: _col1
-                      type: string
-      Reduce Operator Tree:
-        Join Operator
-          condition map:
-               Inner Join 0 to 1
-          condition expressions:
-            0 {VALUE._col0} {VALUE._col1}
-            1 {VALUE._col1}
-          handleSkewJoin: false
-          outputColumnNames: _col0, _col1, _col3
-          Select Operator
-            expressions:
-                  expr: _col0
-                  type: int
-                  expr: _col1
-                  type: string
-                  expr: _col3
-                  type: bigint
-            outputColumnNames: _col0, _col1, _col2
-            Select Operator
-              expressions:
-                    expr: _col0
-                    type: int
-                    expr: _col1
-                    type: string
-                    expr: UDFToInteger(_col2)
-                    type: int
-              outputColumnNames: _col0, _col1, _col2
-              File Output Operator
-                compressed: false
-                GlobalTableId: 1
-                table:
-                    input format: org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat
-                    output format: org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat
-                    serde: org.apache.hadoop.hive.hbase.HBaseSerDe
-                    name: default.hbase_table_3
-
-
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_3
-SELECT x.key, x.value, Y.count 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
-ON (x.key = Y.key)
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_1
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_table_3
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_3
-SELECT x.key, x.value, Y.count 
-FROM 
-(SELECT hbase_table_1.* FROM hbase_table_1) x
-JOIN 
-(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
-ON (x.key = Y.key)
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_1
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_table_3
-PREHOOK: query: select count(1) from hbase_table_3
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_3
-#### A masked pattern was here ####
-POSTHOOK: query: select count(1) from hbase_table_3
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_3
-#### A masked pattern was here ####
-155
-PREHOOK: query: select * from hbase_table_3 order by key, value limit 5
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_3
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_table_3 order by key, value limit 5
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_3
-#### A masked pattern was here ####
-0	val_0	3
-2	val_2	1
-4	val_4	1
-8	val_8	1
-10	val_10	1
-PREHOOK: query: select key, count from hbase_table_3 order by key, count desc limit 5
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_3
-#### A masked pattern was here ####
-POSTHOOK: query: select key, count from hbase_table_3 order by key, count desc limit 5
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_3
-#### A masked pattern was here ####
-0	3
-2	1
-4	1
-8	1
-10	1
-PREHOOK: query: DROP TABLE hbase_table_4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_table_4
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE hbase_table_4(key int, value1 string, value2 int, value3 int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "a:b,a:c,d:e"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_table_4(key int, value1 string, value2 int, value3 int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "a:b,a:c,d:e"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_4
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_4 SELECT key, value, key+1, key+2 
-FROM src WHERE key=98 OR key=100
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_table_4
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_4 SELECT key, value, key+1, key+2 
-FROM src WHERE key=98 OR key=100
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_table_4
-PREHOOK: query: SELECT * FROM hbase_table_4 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_4
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM hbase_table_4 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_4
-#### A masked pattern was here ####
-98	val_98	99	100
-100	val_100	101	102
-PREHOOK: query: DROP TABLE hbase_table_5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_table_5
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE EXTERNAL TABLE hbase_table_5(key int, value map<string,string>) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "a:")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_4")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE EXTERNAL TABLE hbase_table_5(key int, value map<string,string>) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = "a:")
-TBLPROPERTIES ("hbase.table.name" = "hbase_table_4")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_5
-PREHOOK: query: SELECT * FROM hbase_table_5 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_5
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM hbase_table_5 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_5
-#### A masked pattern was here ####
-98	{"b":"val_98","c":"99"}
-100	{"b":"val_100","c":"101"}
-PREHOOK: query: DROP TABLE hbase_table_6
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_table_6
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE hbase_table_6(key int, value map<string,string>) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = ":key,cf:"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_table_6(key int, value map<string,string>) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = ":key,cf:"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_6
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_6 SELECT key, map(value, key) FROM src
-WHERE key=98 OR key=100
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_table_6
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_6 SELECT key, map(value, key) FROM src
-WHERE key=98 OR key=100
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_table_6
-PREHOOK: query: SELECT * FROM hbase_table_6 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_6
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM hbase_table_6 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_6
-#### A masked pattern was here ####
-98	{"val_98":"98"}
-100	{"val_100":"100"}
-PREHOOK: query: DROP TABLE hbase_table_7
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_table_7
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE hbase_table_7(value map<string,string>, key int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "cf:,:key"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_table_7(value map<string,string>, key int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "cf:,:key"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_7
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_7 
-SELECT map(value, key, upper(value), key+1), key FROM src
-WHERE key=98 OR key=100
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_table_7
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_7 
-SELECT map(value, key, upper(value), key+1), key FROM src
-WHERE key=98 OR key=100
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_table_7
-PREHOOK: query: SELECT * FROM hbase_table_7 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_7
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM hbase_table_7 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_7
-#### A masked pattern was here ####
-{"VAL_98":"99.0","val_98":"98"}	98
-{"VAL_100":"101.0","val_100":"100"}	100
-PREHOOK: query: DROP TABLE hbase_table_8
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: DROP TABLE hbase_table_8
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE hbase_table_8(key int, value1 string, value2 int, value3 int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "a:b,a:c,d:e"
-)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_table_8(key int, value1 string, value2 int, value3 int) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES (
-"hbase.columns.mapping" = "a:b,a:c,d:e"
-)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_table_8
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_8 SELECT key, value, key+1, key+2 
-FROM src WHERE key=98 OR key=100
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_table_8
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_8 SELECT key, value, key+1, key+2 
-FROM src WHERE key=98 OR key=100
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_table_8
-PREHOOK: query: SELECT * FROM hbase_table_8 ORDER BY key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_table_8
-#### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM hbase_table_8 ORDER BY key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_table_8
-#### A masked pattern was here ####
-98	val_98	99	100
-100	val_100	101	102
-PREHOOK: query: DROP TABLE hbase_table_1
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_1
-PREHOOK: Output: default@hbase_table_1
-POSTHOOK: query: DROP TABLE hbase_table_1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_1
-POSTHOOK: Output: default@hbase_table_1
-PREHOOK: query: DROP TABLE hbase_table_2
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_2
-PREHOOK: Output: default@hbase_table_2
-POSTHOOK: query: DROP TABLE hbase_table_2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_2
-POSTHOOK: Output: default@hbase_table_2
-PREHOOK: query: DROP TABLE hbase_table_3
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_3
-PREHOOK: Output: default@hbase_table_3
-POSTHOOK: query: DROP TABLE hbase_table_3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_3
-POSTHOOK: Output: default@hbase_table_3
-PREHOOK: query: DROP TABLE hbase_table_4
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_4
-PREHOOK: Output: default@hbase_table_4
-POSTHOOK: query: DROP TABLE hbase_table_4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_4
-POSTHOOK: Output: default@hbase_table_4
-PREHOOK: query: DROP TABLE hbase_table_5
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_5
-PREHOOK: Output: default@hbase_table_5
-POSTHOOK: query: DROP TABLE hbase_table_5
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_5
-POSTHOOK: Output: default@hbase_table_5
-PREHOOK: query: DROP TABLE hbase_table_6
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_6
-PREHOOK: Output: default@hbase_table_6
-POSTHOOK: query: DROP TABLE hbase_table_6
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_6
-POSTHOOK: Output: default@hbase_table_6
-PREHOOK: query: DROP TABLE hbase_table_7
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_7
-PREHOOK: Output: default@hbase_table_7
-POSTHOOK: query: DROP TABLE hbase_table_7
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_7
-POSTHOOK: Output: default@hbase_table_7
-PREHOOK: query: DROP TABLE hbase_table_8
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_table_8
-PREHOOK: Output: default@hbase_table_8
-POSTHOOK: query: DROP TABLE hbase_table_8
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_table_8
-POSTHOOK: Output: default@hbase_table_8
-PREHOOK: query: DROP TABLE empty_hbase_table
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@empty_hbase_table
-PREHOOK: Output: default@empty_hbase_table
-POSTHOOK: query: DROP TABLE empty_hbase_table
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@empty_hbase_table
-POSTHOOK: Output: default@empty_hbase_table
-PREHOOK: query: DROP TABLE empty_normal_table
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@empty_normal_table
-PREHOOK: Output: default@empty_normal_table
-POSTHOOK: query: DROP TABLE empty_normal_table
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@empty_normal_table
-POSTHOOK: Output: default@empty_normal_table
diff --git a/src/hbase-handler/src/test/results/hbase_stats.q.out b/src/hbase-handler/src/test/results/hbase_stats.q.out
deleted file mode 100644
index c8cac9e..0000000
--- a/src/hbase-handler/src/test/results/hbase_stats.q.out
+++ /dev/null
@@ -1,390 +0,0 @@
-PREHOOK: query: create table stats_src like src
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table stats_src like src
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@stats_src
-PREHOOK: query: insert overwrite table stats_src select * from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_src
-POSTHOOK: query: insert overwrite table stats_src select * from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_src
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: analyze table stats_src compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_src
-PREHOOK: Output: default@stats_src
-POSTHOOK: query: analyze table stats_src compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_src
-POSTHOOK: Output: default@stats_src
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: desc formatted stats_src
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_src
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	numFiles            	1                   
-	numPartitions       	0                   
-	numRows             	500                 
-	rawDataSize         	5312                
-	totalSize           	5812                
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: create table stats_part like srcpart
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table stats_part like srcpart
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-PREHOOK: Output: default@stats_part
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-PREHOOK: Output: default@stats_part
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: desc formatted stats_part
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	numFiles            	3                   
-	numPartitions       	3                   
-	numRows             	1500                
-	rawDataSize         	15936               
-	totalSize           	17436               
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Partition Information	 	 
-Partition Value:    	[2010-04-08, 11]    	 
-Database:           	default             	 
-Table:              	stats_part          	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-#### A masked pattern was here ####
-Partition Parameters:	 	 
-	numFiles            	1                   
-	numRows             	500                 
-	rawDataSize         	5312                
-	totalSize           	5812                
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Partition Information	 	 
-Partition Value:    	[2010-04-08, 12]    	 
-Database:           	default             	 
-Table:              	stats_part          	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-#### A masked pattern was here ####
-Partition Parameters:	 	 
-	numFiles            	1                   
-	numRows             	500                 
-	rawDataSize         	5312                
-	totalSize           	5812                
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
-PREHOOK: Output: default@stats_part
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: desc formatted stats_part
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	numFiles            	3                   
-	numPartitions       	3                   
-	numRows             	1500                
-	rawDataSize         	15936               
-	totalSize           	17436               
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: drop table stats_src
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@stats_src
-PREHOOK: Output: default@stats_src
-POSTHOOK: query: drop table stats_src
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@stats_src
-POSTHOOK: Output: default@stats_src
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table stats_part
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@stats_part
-PREHOOK: Output: default@stats_part
-POSTHOOK: query: drop table stats_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@stats_part
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/hbase_stats2.q.out b/src/hbase-handler/src/test/results/hbase_stats2.q.out
deleted file mode 100644
index c8cac9e..0000000
--- a/src/hbase-handler/src/test/results/hbase_stats2.q.out
+++ /dev/null
@@ -1,390 +0,0 @@
-PREHOOK: query: create table stats_src like src
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table stats_src like src
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@stats_src
-PREHOOK: query: insert overwrite table stats_src select * from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_src
-POSTHOOK: query: insert overwrite table stats_src select * from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_src
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: analyze table stats_src compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_src
-PREHOOK: Output: default@stats_src
-POSTHOOK: query: analyze table stats_src compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_src
-POSTHOOK: Output: default@stats_src
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: desc formatted stats_src
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_src
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	numFiles            	1                   
-	numPartitions       	0                   
-	numRows             	500                 
-	rawDataSize         	5312                
-	totalSize           	5812                
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: create table stats_part like srcpart
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table stats_part like srcpart
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-PREHOOK: Output: default@stats_part
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-PREHOOK: Output: default@stats_part
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: desc formatted stats_part
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	numFiles            	3                   
-	numPartitions       	3                   
-	numRows             	1500                
-	rawDataSize         	15936               
-	totalSize           	17436               
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Partition Information	 	 
-Partition Value:    	[2010-04-08, 11]    	 
-Database:           	default             	 
-Table:              	stats_part          	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-#### A masked pattern was here ####
-Partition Parameters:	 	 
-	numFiles            	1                   
-	numRows             	500                 
-	rawDataSize         	5312                
-	totalSize           	5812                
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Partition Information	 	 
-Partition Value:    	[2010-04-08, 12]    	 
-Database:           	default             	 
-Table:              	stats_part          	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-#### A masked pattern was here ####
-Partition Parameters:	 	 
-	numFiles            	1                   
-	numRows             	500                 
-	rawDataSize         	5312                
-	totalSize           	5812                
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
-PREHOOK: type: QUERY
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
-PREHOOK: Output: default@stats_part
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
-POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: desc formatted stats_part
-PREHOOK: type: DESCTABLE
-POSTHOOK: query: desc formatted stats_part
-POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-# col_name            	data_type           	comment             
-	 	 
-key                 	string              	default             
-value               	string              	default             
-	 	 
-# Partition Information	 	 
-# col_name            	data_type           	comment             
-	 	 
-ds                  	string              	None                
-hr                  	string              	None                
-	 	 
-# Detailed Table Information	 	 
-Database:           	default             	 
-#### A masked pattern was here ####
-Protect Mode:       	None                	 
-Retention:          	0                   	 
-#### A masked pattern was here ####
-Table Type:         	MANAGED_TABLE       	 
-Table Parameters:	 	 
-	numFiles            	3                   
-	numPartitions       	3                   
-	numRows             	1500                
-	rawDataSize         	15936               
-	totalSize           	17436               
-#### A masked pattern was here ####
-	 	 
-# Storage Information	 	 
-SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
-InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
-OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
-Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
-Storage Desc Params:	 	 
-	serialization.format	1                   
-PREHOOK: query: drop table stats_src
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@stats_src
-PREHOOK: Output: default@stats_src
-POSTHOOK: query: drop table stats_src
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@stats_src
-POSTHOOK: Output: default@stats_src
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table stats_part
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@stats_part
-PREHOOK: Output: default@stats_part
-POSTHOOK: query: drop table stats_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@stats_part
-POSTHOOK: Output: default@stats_part
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/positive/external_table_ppd.q.out b/src/hbase-handler/src/test/results/positive/external_table_ppd.q.out
new file mode 100644
index 0000000..38d9175
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/external_table_ppd.q.out
@@ -0,0 +1,186 @@
+PREHOOK: query: DROP TABLE t_hbase
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE t_hbase(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive",
+               "hbase.table.default.storage.type" = "binary")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE t_hbase(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive",
+               "hbase.table.default.storage.type" = "binary")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: DESCRIBE FORMATTED t_hbase
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE FORMATTED t_hbase
+POSTHOOK: type: DESCTABLE
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	from deserializer   
+tinyint_col         	tinyint             	from deserializer   
+smallint_col        	smallint            	from deserializer   
+int_col             	int                 	from deserializer   
+bigint_col          	bigint              	from deserializer   
+float_col           	float               	from deserializer   
+double_col          	double              	from deserializer   
+boolean_col         	boolean             	from deserializer   
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	hbase.table.default.storage.type	binary              
+	hbase.table.name    	t_hive              
+	storage_handler     	org.apache.hadoop.hive.hbase.HBaseStorageHandler
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.hbase.HBaseSerDe	 
+InputFormat:        	org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	hbase.columns.mapping	cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-
+	serialization.format	1                   
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user1', 1, 11, 10, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user1', 1, 11, 10, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user2', 127, 327, 2147, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user2', 127, 327, 2147, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user3', -128, -327, -214748, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user3', -128, -327, -214748, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: explain SELECT * FROM t_hbase where int_col > 0
+PREHOOK: type: QUERY
+POSTHOOK: query: explain SELECT * FROM t_hbase where int_col > 0
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME t_hbase))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_TABLE_OR_COL int_col) 0))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        t_hbase 
+          TableScan
+            alias: t_hbase
+            filterExpr:
+                expr: (int_col > 0)
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (int_col > 0)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: tinyint_col
+                      type: tinyint
+                      expr: smallint_col
+                      type: smallint
+                      expr: int_col
+                      type: int
+                      expr: bigint_col
+                      type: bigint
+                      expr: float_col
+                      type: float
+                      expr: double_col
+                      type: double
+                      expr: boolean_col
+                      type: boolean
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: SELECT * FROM t_hbase where int_col > 0
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase where int_col > 0
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+user1	1	11	10	1	1.0	1.0	true
+user2	127	327	2147	9223372036854775807	211.31	2.687465320571E8	false
+PREHOOK: query: DROP TABLE t_hbase
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: DROP TABLE t_hbase
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase
+POSTHOOK: Output: default@t_hbase
diff --git a/src/hbase-handler/src/test/results/positive/hbase_binary_external_table_queries.q.out b/src/hbase-handler/src/test/results/positive/hbase_binary_external_table_queries.q.out
new file mode 100644
index 0000000..524eef1
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_binary_external_table_queries.q.out
@@ -0,0 +1,131 @@
+PREHOOK: query: DROP TABLE t_ext_hbase_1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_1
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
+TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_1
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
+TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_1
+PREHOOK: query: SELECT * FROM t_ext_hbase_1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_1
+#### A masked pattern was here ####
+key-1	NULL	NULL	NULL	NULL	NULL	Hadoop, HBase,	NULL	NULL
+key-2	NULL	NULL	NULL	NULL	NULL	Hive	NULL	NULL
+key-3	NULL	NULL	NULL	NULL	NULL	Test Strings	NULL	NULL
+PREHOOK: query: DROP TABLE t_ext_hbase_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_1
+PREHOOK: Output: default@t_ext_hbase_1
+POSTHOOK: query: DROP TABLE t_ext_hbase_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_1
+POSTHOOK: Output: default@t_ext_hbase_1
+PREHOOK: query: DROP TABLE t_ext_hbase_2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_2
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_2
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:cq-boolean#b,cf:cq-byte#b,cf:cq-short#b,cf:cq-int#b,cf:cq-long#b,cf:cq-string#b,cf:cq-float#b,cf:cq-double#b")
+TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_2
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:cq-boolean#b,cf:cq-byte#b,cf:cq-short#b,cf:cq-int#b,cf:cq-long#b,cf:cq-string#b,cf:cq-float#b,cf:cq-double#b")
+TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_2
+PREHOOK: query: SELECT * FROM t_ext_hbase_2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_2
+#### A masked pattern was here ####
+key-1	true	-128	-32768	-2147483648	-9223372036854775808	Hadoop, HBase,	1.4E-45	4.9E-324
+key-2	false	-1	-1	-1	-1	Hive	-1.0	-1.0
+key-3	true	127	32767	2147483647	9223372036854775807	Test Strings	3.4028235E38	1.7976931348623157E308
+PREHOOK: query: DROP TABLE t_ext_hbase_2
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_2
+PREHOOK: Output: default@t_ext_hbase_2
+POSTHOOK: query: DROP TABLE t_ext_hbase_2
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_2
+POSTHOOK: Output: default@t_ext_hbase_2
+PREHOOK: query: DROP TABLE t_ext_hbase_3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_3
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_3
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
+TBLPROPERTIES (
+"hbase.table.name" = "HiveExternalTable",
+"hbase.table.default.storage.type" = "binary")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_3
+(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
+ c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
+TBLPROPERTIES (
+"hbase.table.name" = "HiveExternalTable",
+"hbase.table.default.storage.type" = "binary")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_3
+PREHOOK: query: SELECT * from t_ext_hbase_3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_3
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * from t_ext_hbase_3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_3
+#### A masked pattern was here ####
+key-1	true	-128	-32768	-2147483648	-9223372036854775808	Hadoop, HBase,	1.4E-45	4.9E-324
+key-2	false	-1	-1	-1	-1	Hive	-1.0	-1.0
+key-3	true	127	32767	2147483647	9223372036854775807	Test Strings	3.4028235E38	1.7976931348623157E308
+PREHOOK: query: --HIVE-2958
+SELECT c_int, count(*) FROM t_ext_hbase_3 GROUP BY c_int
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_3
+#### A masked pattern was here ####
+POSTHOOK: query: --HIVE-2958
+SELECT c_int, count(*) FROM t_ext_hbase_3 GROUP BY c_int
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_3
+#### A masked pattern was here ####
+-2147483648	1
+-1	1
+2147483647	1
+PREHOOK: query: DROP table t_ext_hbase_3
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_3
+PREHOOK: Output: default@t_ext_hbase_3
+POSTHOOK: query: DROP table t_ext_hbase_3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_3
+POSTHOOK: Output: default@t_ext_hbase_3
diff --git a/src/hbase-handler/src/test/results/positive/hbase_binary_map_queries.q.out b/src/hbase-handler/src/test/results/positive/hbase_binary_map_queries.q.out
new file mode 100644
index 0000000..8984da1
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_binary_map_queries.q.out
@@ -0,0 +1,869 @@
+PREHOOK: query: DROP TABLE hbase_src
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_src
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE hbase_src(key STRING,
+                       tinyint_col TINYINT,
+                       smallint_col SMALLINT,
+                       int_col INT,
+                       bigint_col BIGINT,
+                       float_col FLOAT,
+                       double_col DOUBLE,
+                       string_col STRING)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_src(key STRING,
+                       tinyint_col TINYINT,
+                       smallint_col SMALLINT,
+                       int_col INT,
+                       bigint_col BIGINT,
+                       float_col FLOAT,
+                       double_col DOUBLE,
+                       string_col STRING)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_src
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_src
+  SELECT key, key, key, key, key, key, key, value
+  FROM src
+  WHERE key = 125 OR key = 126 OR key = 127
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_src
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_src
+  SELECT key, key, key, key, key, key, key, value
+  FROM src
+  WHERE key = 125 OR key = 126 OR key = 127
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_src
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_hbase_maps
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase_maps
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE TABLE t_hbase_maps(key STRING,
+                          tinyint_map_col MAP<TINYINT, TINYINT>,
+                          smallint_map_col MAP<SMALLINT, SMALLINT>,
+                          int_map_col MAP<INT, INT>,
+                          bigint_map_col MAP<BIGINT, BIGINT>,
+                          float_map_col MAP<FLOAT, FLOAT>,
+                          double_map_col MAP<DOUBLE, DOUBLE>,
+                          string_map_col MAP<STRING, STRING>,
+                          boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE t_hbase_maps(key STRING,
+                          tinyint_map_col MAP<TINYINT, TINYINT>,
+                          smallint_map_col MAP<SMALLINT, SMALLINT>,
+                          int_map_col MAP<INT, INT>,
+                          bigint_map_col MAP<BIGINT, BIGINT>,
+                          float_map_col MAP<FLOAT, FLOAT>,
+                          double_map_col MAP<DOUBLE, DOUBLE>,
+                          string_map_col MAP<STRING, STRING>,
+                          boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase_maps
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(true, true)
+  FROM hbase_src
+  WHERE key = 125
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_src
+PREHOOK: Output: default@t_hbase_maps
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(true, true)
+  FROM hbase_src
+  WHERE key = 125
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_src
+POSTHOOK: Output: default@t_hbase_maps
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(false, false)
+  FROM hbase_src
+  WHERE key = 126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_src
+PREHOOK: Output: default@t_hbase_maps
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(false, false)
+  FROM hbase_src
+  WHERE key = 126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_src
+POSTHOOK: Output: default@t_hbase_maps
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_hbase_maps ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_maps
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase_maps ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_maps
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
+126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
+                                       tinyint_map_col MAP<TINYINT, TINYINT>,
+                                       smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                       int_map_col MAP<INT, INT>,
+                                       bigint_map_col MAP<BIGINT, BIGINT>,
+                                       float_map_col MAP<FLOAT, FLOAT>,
+                                       double_map_col MAP<DOUBLE, DOUBLE>,
+                                       string_map_col MAP<STRING, STRING>,
+                                       boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
+                                       tinyint_map_col MAP<TINYINT, TINYINT>,
+                                       smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                       int_map_col MAP<INT, INT>,
+                                       bigint_map_col MAP<BIGINT, BIGINT>,
+                                       float_map_col MAP<FLOAT, FLOAT>,
+                                       double_map_col MAP<DOUBLE, DOUBLE>,
+                                       string_map_col MAP<STRING, STRING>,
+                                       boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_maps
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_ext_hbase_maps ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_maps
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_maps ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_maps
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
+126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_maps
+PREHOOK: Output: default@t_ext_hbase_maps
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_maps
+POSTHOOK: Output: default@t_ext_hbase_maps
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_1(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_1(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_maps_1
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_ext_hbase_maps_1 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_maps_1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_1 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_maps_1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{49:49}	{12594:12594}	{}	{}	{2.5932638E-9:2.5932638E-9}	{}	{"125":"val_125"}	{true:true}
+126	{49:49}	{12594:12594}	{}	{}	{2.5933207E-9:2.5933207E-9}	{}	{"126":"val_126"}	{true:true}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_maps_1
+PREHOOK: Output: default@t_ext_hbase_maps_1
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_maps_1
+POSTHOOK: Output: default@t_ext_hbase_maps_1
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_2
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_2(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES (
+"hbase.table.name"="t_hive_maps",
+"hbase.table.default.storage.type"="binary")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_2(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES (
+"hbase.table.name"="t_hive_maps",
+"hbase.table.default.storage.type"="binary")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_maps_2
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_ext_hbase_maps_2 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_maps_2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_2 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_maps_2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{49:49}	{12594:12594}	{}	{}	{2.5932638E-9:2.5932638E-9}	{}	{"125":"val_125"}	{true:true}
+126	{49:49}	{12594:12594}	{}	{}	{2.5933207E-9:2.5933207E-9}	{}	{"126":"val_126"}	{true:true}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_2
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_maps_2
+PREHOOK: Output: default@t_ext_hbase_maps_2
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_2
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_maps_2
+POSTHOOK: Output: default@t_ext_hbase_maps_2
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_hbase_maps_1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase_maps_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE TABLE t_hbase_maps_1(key STRING,
+                            tinyint_map_col MAP<TINYINT, TINYINT>,
+                            smallint_map_col MAP<SMALLINT, SMALLINT>,
+                            int_map_col MAP<INT, INT>,
+                            bigint_map_col MAP<BIGINT, BIGINT>,
+                            float_map_col MAP<FLOAT, FLOAT>,
+                            double_map_col MAP<DOUBLE, DOUBLE>,
+                            string_map_col MAP<STRING, STRING>,
+                            boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#b:b,cf-smallint:#b:b,cf-int:#b:b,cf-bigint:#b:b,cf-float:#b:b,cf-double:#b:b,cf-string:#b:b,cf-boolean:#b:b")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE t_hbase_maps_1(key STRING,
+                            tinyint_map_col MAP<TINYINT, TINYINT>,
+                            smallint_map_col MAP<SMALLINT, SMALLINT>,
+                            int_map_col MAP<INT, INT>,
+                            bigint_map_col MAP<BIGINT, BIGINT>,
+                            float_map_col MAP<FLOAT, FLOAT>,
+                            double_map_col MAP<DOUBLE, DOUBLE>,
+                            string_map_col MAP<STRING, STRING>,
+                            boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#b:b,cf-smallint:#b:b,cf-int:#b:b,cf-bigint:#b:b,cf-float:#b:b,cf-double:#b:b,cf-string:#b:b,cf-boolean:#b:b")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase_maps_1
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(true, true)
+  FROM hbase_src
+  WHERE key = 125
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_src
+PREHOOK: Output: default@t_hbase_maps_1
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(true, true)
+  FROM hbase_src
+  WHERE key = 125
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_src
+POSTHOOK: Output: default@t_hbase_maps_1
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(false, false)
+  FROM hbase_src
+  WHERE key = 126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_src
+PREHOOK: Output: default@t_hbase_maps_1
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_maps_1
+  SELECT key,
+         map(tinyint_col, tinyint_col),
+         map(smallint_col, smallint_col),
+         map(int_col, int_col),
+         map(bigint_col, bigint_col),
+         map(float_col, float_col),
+         map(double_col, double_col),
+         map(key, string_col),
+         map(false, false)
+  FROM hbase_src
+  WHERE key = 126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_src
+POSTHOOK: Output: default@t_hbase_maps_1
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_hbase_maps_1 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_maps_1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase_maps_1 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_maps_1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
+126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_3(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_3(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_maps_3
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_ext_hbase_maps_3 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_maps_3
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_3 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_maps_3
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
+126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_3
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_maps_3
+PREHOOK: Output: default@t_ext_hbase_maps_3
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_maps_3
+POSTHOOK: Output: default@t_ext_hbase_maps_3
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_4
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_4
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_4(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_4(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_maps_4
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_ext_hbase_maps_4 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_maps_4
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_4 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_maps_4
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{}	{}	{}	{}	{}	{}	{"125":"val_125"}	{}
+126	{}	{}	{}	{}	{}	{}	{"126":"val_126"}	{}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_4
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_maps_4
+PREHOOK: Output: default@t_ext_hbase_maps_4
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_4
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_maps_4
+POSTHOOK: Output: default@t_ext_hbase_maps_4
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_5
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_5
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_5(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES (
+"hbase.table.name"="t_hive_maps_1",
+"hbase.table.default.storage.type"="binary")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_ext_hbase_maps_5(key STRING,
+                                         tinyint_map_col MAP<TINYINT, TINYINT>,
+                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
+                                         int_map_col MAP<INT, INT>,
+                                         bigint_map_col MAP<BIGINT, BIGINT>,
+                                         float_map_col MAP<FLOAT, FLOAT>,
+                                         double_map_col MAP<DOUBLE, DOUBLE>,
+                                         string_map_col MAP<STRING, STRING>,
+                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
+TBLPROPERTIES (
+"hbase.table.name"="t_hive_maps_1",
+"hbase.table.default.storage.type"="binary")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_ext_hbase_maps_5
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: SELECT * FROM t_ext_hbase_maps_5 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_ext_hbase_maps_5
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_ext_hbase_maps_5 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_ext_hbase_maps_5
+#### A masked pattern was here ####
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+125	{125:125}	{125:125}	{125:125}	{125:125}	{125.0:125.0}	{125.0:125.0}	{"125":"val_125"}	{true:true}
+126	{126:126}	{126:126}	{126:126}	{126:126}	{126.0:126.0}	{126.0:126.0}	{"126":"val_126"}	{false:false}
+PREHOOK: query: DROP TABLE t_ext_hbase_maps_5
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_ext_hbase_maps_5
+PREHOOK: Output: default@t_ext_hbase_maps_5
+POSTHOOK: query: DROP TABLE t_ext_hbase_maps_5
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_ext_hbase_maps_5
+POSTHOOK: Output: default@t_ext_hbase_maps_5
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_hbase_maps_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase_maps_1
+PREHOOK: Output: default@t_hbase_maps_1
+POSTHOOK: query: DROP TABLE t_hbase_maps_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase_maps_1
+POSTHOOK: Output: default@t_hbase_maps_1
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE t_hbase_maps
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase_maps
+PREHOOK: Output: default@t_hbase_maps
+POSTHOOK: query: DROP TABLE t_hbase_maps
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase_maps
+POSTHOOK: Output: default@t_hbase_maps
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: DROP TABLE hbase_src
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_src
+PREHOOK: Output: default@hbase_src
+POSTHOOK: query: DROP TABLE hbase_src
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_src
+POSTHOOK: Output: default@hbase_src
+POSTHOOK: Lineage: hbase_src.bigint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.double_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.float_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.int_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.smallint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.string_col SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbase_src.tinyint_col EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/positive/hbase_binary_storage_queries.q.out b/src/hbase-handler/src/test/results/positive/hbase_binary_storage_queries.q.out
new file mode 100644
index 0000000..306e666
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_binary_storage_queries.q.out
@@ -0,0 +1,658 @@
+PREHOOK: query: DROP TABLE t_hbase
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE t_hbase(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive",
+               "hbase.table.default.storage.type" = "binary")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE t_hbase(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive",
+               "hbase.table.default.storage.type" = "binary")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: DESCRIBE FORMATTED t_hbase
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE FORMATTED t_hbase
+POSTHOOK: type: DESCTABLE
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	from deserializer   
+tinyint_col         	tinyint             	from deserializer   
+smallint_col        	smallint            	from deserializer   
+int_col             	int                 	from deserializer   
+bigint_col          	bigint              	from deserializer   
+float_col           	float               	from deserializer   
+double_col          	double              	from deserializer   
+boolean_col         	boolean             	from deserializer   
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	hbase.table.default.storage.type	binary              
+	hbase.table.name    	t_hive              
+	storage_handler     	org.apache.hadoop.hive.hbase.HBaseStorageHandler
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.hbase.HBaseSerDe	 
+InputFormat:        	org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	hbase.columns.mapping	:key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-
+	serialization.format	1                   
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase
+SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: SELECT * FROM t_hbase
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+user1	1	1	1	1	1.0	1.0	true
+user2	127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
+user3	-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
+PREHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase
+WHERE key='user1' OR key='user2' OR key='user3'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase
+WHERE key='user1' OR key='user2' OR key='user3'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+1	1	1	1	1.0	1.0	true
+127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
+-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
+PREHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase
+#### A masked pattern was here ####
+0	0	0	0	11.139999389648438	2.6663574368565E8	3
+PREHOOK: query: DROP TABLE t_hbase_1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase_1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE t_hbase_1(key STRING,
+                                tinyint_col TINYINT,
+                                smallint_col SMALLINT,
+                                int_col INT,
+                                bigint_col BIGINT,
+                                float_col FLOAT,
+                                double_col DOUBLE,
+                                boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
+TBLPROPERTIES ("hbase.table.name" = "t_hive")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_hbase_1(key STRING,
+                                tinyint_col TINYINT,
+                                smallint_col SMALLINT,
+                                int_col INT,
+                                bigint_col BIGINT,
+                                float_col FLOAT,
+                                double_col DOUBLE,
+                                boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
+TBLPROPERTIES ("hbase.table.name" = "t_hive")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase_1
+PREHOOK: query: DESCRIBE FORMATTED t_hbase_1
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE FORMATTED t_hbase_1
+POSTHOOK: type: DESCTABLE
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	from deserializer   
+tinyint_col         	tinyint             	from deserializer   
+smallint_col        	smallint            	from deserializer   
+int_col             	int                 	from deserializer   
+bigint_col          	bigint              	from deserializer   
+float_col           	float               	from deserializer   
+double_col          	double              	from deserializer   
+boolean_col         	boolean             	from deserializer   
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	EXTERNAL_TABLE      	 
+Table Parameters:	 	 
+	EXTERNAL            	TRUE                
+	hbase.table.name    	t_hive              
+	storage_handler     	org.apache.hadoop.hive.hbase.HBaseStorageHandler
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.hbase.HBaseSerDe	 
+InputFormat:        	org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	hbase.columns.mapping	:key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b
+	serialization.format	1                   
+PREHOOK: query: SELECT * FROM t_hbase_1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase_1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_1
+#### A masked pattern was here ####
+user1	1	1	1	1	1.0	1.0	true
+user2	127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
+user3	-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
+PREHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_1
+WHERE key='user1' OR key='user2' OR key='user3'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_1
+WHERE key='user1' OR key='user2' OR key='user3'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_1
+#### A masked pattern was here ####
+1	1	1	1	1.0	1.0	true
+127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
+-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
+PREHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_1
+#### A masked pattern was here ####
+0	0	0	0	11.139999389648438	2.6663574368565E8	3
+PREHOOK: query: DROP TABLE t_hbase_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase_1
+PREHOOK: Output: default@t_hbase_1
+POSTHOOK: query: DROP TABLE t_hbase_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase_1
+POSTHOOK: Output: default@t_hbase_1
+PREHOOK: query: DROP TABLE t_hbase
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase
+PREHOOK: Output: default@t_hbase
+POSTHOOK: query: DROP TABLE t_hbase
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase
+POSTHOOK: Output: default@t_hbase
+PREHOOK: query: DROP TABLE t_hbase_2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase_2
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE t_hbase_2(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE t_hbase_2(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase_2
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase_2
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase_2
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase_2
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase_2
+PREHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@t_hbase_2
+POSTHOOK: query: INSERT OVERWRITE TABLE t_hbase_2
+SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
+FROM src
+WHERE key=100 OR key=125 OR key=126
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@t_hbase_2
+PREHOOK: query: SELECT * FROM t_hbase_2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase_2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_2
+#### A masked pattern was here ####
+user1	1	1	1	1	1.0	1.0	true
+user2	127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
+user3	-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
+PREHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_2
+WHERE key='user1' OR key='user2' OR key='user3'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_2
+WHERE key='user1' OR key='user2' OR key='user3'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_2
+#### A masked pattern was here ####
+1	1	1	1	1.0	1.0	true
+127	32767	2147483647	9223372036854775807	211.31	2.687465320571E8	false
+-128	-32768	-2147483648	-9223372036854775808	-201.17	-2110789.37145	true
+PREHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_2
+#### A masked pattern was here ####
+0	0	0	0	11.139999389648438	2.6663574368565E8	3
+PREHOOK: query: DROP TABLE t_hbase_3
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase_3
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE t_hbase_3(key STRING,
+                                tinyint_col TINYINT,
+                                smallint_col SMALLINT,
+                                int_col INT,
+                                bigint_col BIGINT,
+                                float_col FLOAT,
+                                double_col DOUBLE,
+                                boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
+TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_hbase_3(key STRING,
+                                tinyint_col TINYINT,
+                                smallint_col SMALLINT,
+                                int_col INT,
+                                bigint_col BIGINT,
+                                float_col FLOAT,
+                                double_col DOUBLE,
+                                boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
+TBLPROPERTIES ("hbase.table.name" = "t_hive_2")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase_3
+PREHOOK: query: SELECT * FROM t_hbase_3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_3
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase_3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_3
+#### A masked pattern was here ####
+user1	49	NULL	NULL	NULL	NULL	NULL	true
+user2	49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
+user3	45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
+PREHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_3
+WHERE key='user1' OR key='user2' OR key='user3'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_3
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_3
+WHERE key='user1' OR key='user2' OR key='user3'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_3
+#### A masked pattern was here ####
+49	NULL	NULL	NULL	NULL	NULL	true
+49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
+45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
+PREHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_3
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_3
+#### A masked pattern was here ####
+143	24677	1600349547	7380102801049413986	1.0324066977186741E-8	5.6030888442763564E-67	3
+PREHOOK: query: DROP TABLE t_hbase_3
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase_3
+PREHOOK: Output: default@t_hbase_3
+POSTHOOK: query: DROP TABLE t_hbase_3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase_3
+POSTHOOK: Output: default@t_hbase_3
+PREHOOK: query: DROP TABLE t_hbase_4
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE t_hbase_4
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE t_hbase_4(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES (
+"hbase.table.name" = "t_hive_2",
+"hbase.table.default.storage.type" = "binary")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE t_hbase_4(key STRING,
+                     tinyint_col TINYINT,
+                     smallint_col SMALLINT,
+                     int_col INT,
+                     bigint_col BIGINT,
+                     float_col FLOAT,
+                     double_col DOUBLE,
+                     boolean_col BOOLEAN)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
+TBLPROPERTIES (
+"hbase.table.name" = "t_hive_2",
+"hbase.table.default.storage.type" = "binary")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@t_hbase_4
+PREHOOK: query: SELECT * FROM t_hbase_4
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_4
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM t_hbase_4
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_4
+#### A masked pattern was here ####
+user1	49	NULL	NULL	NULL	NULL	NULL	true
+user2	49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
+user3	45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
+PREHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_4
+WHERE key='user1' OR key='user2' OR key='user3'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_4
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT tinyint_col,
+       smallint_col,
+       int_col,
+       bigint_col,
+       float_col,
+       double_col,
+       boolean_col
+FROM t_hbase_4
+WHERE key='user1' OR key='user2' OR key='user3'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_4
+#### A masked pattern was here ####
+49	NULL	NULL	NULL	NULL	NULL	true
+49	13106	842085431	4121411804481401392	1.0313938E-8	5.6030888442763564E-67	true
+45	11571	758264116	3258690996568012594	1.0128829E-11	5.581687380553606E-91	true
+PREHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_4
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t_hbase_4
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT sum(tinyint_col),
+       sum(smallint_col),
+       sum(int_col),
+       sum(bigint_col),
+       sum(float_col),
+       sum(double_col),
+       count(boolean_col)
+FROM t_hbase_4
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t_hbase_4
+#### A masked pattern was here ####
+143	24677	1600349547	7380102801049413986	1.0324066977186741E-8	5.6030888442763564E-67	3
+PREHOOK: query: DROP TABLE t_hbase_4
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase_4
+PREHOOK: Output: default@t_hbase_4
+POSTHOOK: query: DROP TABLE t_hbase_4
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase_4
+POSTHOOK: Output: default@t_hbase_4
+PREHOOK: query: DROP TABLE t_hbase_2
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@t_hbase_2
+PREHOOK: Output: default@t_hbase_2
+POSTHOOK: query: DROP TABLE t_hbase_2
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@t_hbase_2
+POSTHOOK: Output: default@t_hbase_2
diff --git a/src/hbase-handler/src/test/results/positive/hbase_bulk.m.out b/src/hbase-handler/src/test/results/positive/hbase_bulk.m.out
new file mode 100644
index 0000000..21f4e10
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_bulk.m.out
@@ -0,0 +1,131 @@
+PREHOOK: query: drop table hbsort
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table hbsort
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table hbpartition
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table hbpartition
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: -- this is a dummy table used for controlling how the HFiles are
+-- created
+create table hbsort(key string, val string, val2 string)
+stored as
+INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
+OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
+#### A masked pattern was here ####
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- this is a dummy table used for controlling how the HFiles are
+-- created
+create table hbsort(key string, val string, val2 string)
+stored as
+INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
+OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
+#### A masked pattern was here ####
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbsort
+PREHOOK: query: -- this is a dummy table used for controlling how the input file
+-- for TotalOrderPartitioner is created
+create table hbpartition(part_break string)
+row format serde
+'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
+stored as
+inputformat
+'org.apache.hadoop.mapred.TextInputFormat'
+outputformat
+'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat'
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- this is a dummy table used for controlling how the input file
+-- for TotalOrderPartitioner is created
+create table hbpartition(part_break string)
+row format serde
+'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
+stored as
+inputformat
+'org.apache.hadoop.mapred.TextInputFormat'
+outputformat
+'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbpartition
+PREHOOK: query: -- this should produce one file, but we do not
+-- know what it will be called, so we will copy it to a well known
+#### A masked pattern was here ####
+insert overwrite table hbpartition
+select distinct value
+from src
+where value='val_100' or value='val_200'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbpartition
+POSTHOOK: query: -- this should produce one file, but we do not
+-- know what it will be called, so we will copy it to a well known
+#### A masked pattern was here ####
+insert overwrite table hbpartition
+select distinct value
+from src
+where value='val_100' or value='val_200'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbpartition
+POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+#### A masked pattern was here ####
+-- include some trailing blanks and nulls to make sure we handle them correctly
+insert overwrite table hbsort
+select distinct value,
+  case when key=103 then cast(null as string) else key end,
+  case when key=103 then ''
+       else cast(key+1 as string) end
+from src
+cluster by value
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbsort
+#### A masked pattern was here ####
+-- include some trailing blanks and nulls to make sure we handle them correctly
+insert overwrite table hbsort
+select distinct value,
+  case when key=103 then cast(null as string) else key end,
+  case when key=103 then ''
+       else cast(key+1 as string) end
+from src
+cluster by value
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbsort
+POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: -- To get the files out to your local filesystem for loading into
+#### A masked pattern was here ####
+-- semicolon-terminate the line below before running this test:
+#### A masked pattern was here ####
+
+drop table hbsort
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbsort
+PREHOOK: Output: default@hbsort
+POSTHOOK: query: -- To get the files out to your local filesystem for loading into
+#### A masked pattern was here ####
+-- semicolon-terminate the line below before running this test:
+#### A masked pattern was here ####
+
+drop table hbsort
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbsort
+POSTHOOK: Output: default@hbsort
+POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: drop table hbpartition
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbpartition
+PREHOOK: Output: default@hbpartition
+POSTHOOK: query: drop table hbpartition
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbpartition
+POSTHOOK: Output: default@hbpartition
+POSTHOOK: Lineage: hbpartition.part_break SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hbsort.val2 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/positive/hbase_joins.q.out b/src/hbase-handler/src/test/results/positive/hbase_joins.q.out
new file mode 100644
index 0000000..2c2f89d
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_joins.q.out
@@ -0,0 +1,267 @@
+PREHOOK: query: DROP TABLE users
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE users
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE states
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE states
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE countries
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE countries
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE users_level
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE users_level
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: -- From HIVE-1257
+
+CREATE TABLE users(key string, state string, country string, country_id int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "info:state,info:country,info:country_id"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- From HIVE-1257
+
+CREATE TABLE users(key string, state string, country string, country_id int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "info:state,info:country,info:country_id"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@users
+PREHOOK: query: CREATE TABLE states(key string, name string)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "state:name"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE states(key string, name string)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "state:name"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@states
+PREHOOK: query: CREATE TABLE countries(key string, name string, country string, country_id int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "info:name,info:country,info:country_id"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE countries(key string, name string, country string, country_id int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "info:name,info:country,info:country_id"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@countries
+PREHOOK: query: INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
+FROM src WHERE key=100
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@users
+POSTHOOK: query: INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
+FROM src WHERE key=100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@users
+PREHOOK: query: INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
+FROM src WHERE key=100
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@states
+POSTHOOK: query: INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
+FROM src WHERE key=100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@states
+PREHOOK: query: INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
+FROM src WHERE key=100
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@countries
+POSTHOOK: query: INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
+FROM src WHERE key=100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@countries
+PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country = c.key)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@countries
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country = c.key)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@countries
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+user1	USA	United States	USA
+PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
+ON (u.country = c.country)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@countries
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
+ON (u.country = c.country)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@countries
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+user1	USA	United States	USA
+PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country_id = c.country_id)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@countries
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country_id = c.country_id)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@countries
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+PREHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
+ON (u.state = s.key)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@states
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
+ON (u.state = s.key)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@states
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+user1	IA	Iowa
+PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country = c.key)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@countries
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country = c.key)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@countries
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+user1	USA	United States	USA
+PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
+ON (u.country = c.country)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@countries
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
+ON (u.country = c.country)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@countries
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+user1	USA	United States	USA
+PREHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country_id = c.country_id)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@countries
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c 
+ON (u.country_id = c.country_id)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@countries
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+PREHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
+ON (u.state = s.key)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@states
+PREHOOK: Input: default@users
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT u.key, u.state, s.name FROM users u JOIN states s 
+ON (u.state = s.key)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@states
+POSTHOOK: Input: default@users
+#### A masked pattern was here ####
+user1	IA	Iowa
+PREHOOK: query: DROP TABLE users
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@users
+PREHOOK: Output: default@users
+POSTHOOK: query: DROP TABLE users
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@users
+POSTHOOK: Output: default@users
+PREHOOK: query: DROP TABLE states
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@states
+PREHOOK: Output: default@states
+POSTHOOK: query: DROP TABLE states
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@states
+POSTHOOK: Output: default@states
+PREHOOK: query: DROP TABLE countries
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@countries
+PREHOOK: Output: default@countries
+POSTHOOK: query: DROP TABLE countries
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@countries
+POSTHOOK: Output: default@countries
+PREHOOK: query: CREATE TABLE users(key int, userid int, username string, created int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:nickname,f:created")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE users(key int, userid int, username string, created int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:nickname,f:created")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@users
+PREHOOK: query: CREATE TABLE users_level(key int, userid int, level int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:level")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE users_level(key int, userid int, level int)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:level")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@users_level
+PREHOOK: query: -- HIVE-1903:  the problem fixed here showed up even without any data,
+-- so no need to load any to test it
+SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num 
+ FROM users JOIN users_level ON (users.userid = users_level.userid) 
+ GROUP BY year(from_unixtime(users.created)), level
+PREHOOK: type: QUERY
+PREHOOK: Input: default@users
+PREHOOK: Input: default@users_level
+#### A masked pattern was here ####
+POSTHOOK: query: -- HIVE-1903:  the problem fixed here showed up even without any data,
+-- so no need to load any to test it
+SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num 
+ FROM users JOIN users_level ON (users.userid = users_level.userid) 
+ GROUP BY year(from_unixtime(users.created)), level
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@users
+POSTHOOK: Input: default@users_level
+#### A masked pattern was here ####
+PREHOOK: query: DROP TABLE users
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@users
+PREHOOK: Output: default@users
+POSTHOOK: query: DROP TABLE users
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@users
+POSTHOOK: Output: default@users
+PREHOOK: query: DROP TABLE users_level
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@users_level
+PREHOOK: Output: default@users_level
+POSTHOOK: query: DROP TABLE users_level
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@users_level
+POSTHOOK: Output: default@users_level
diff --git a/src/hbase-handler/src/test/results/positive/hbase_ppd_key_range.q.out b/src/hbase-handler/src/test/results/positive/hbase_ppd_key_range.q.out
new file mode 100644
index 0000000..b35860f
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_ppd_key_range.q.out
@@ -0,0 +1,651 @@
+PREHOOK: query: CREATE TABLE hbase_pushdown(key string, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_pushdown(key string, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_pushdown
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
+SELECT cast(key as string), value
+FROM src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_pushdown
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
+SELECT cast(key as string), value
+FROM src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_pushdown
+PREHOOK: query: -- with full pushdown
+explain select * from hbase_pushdown where key>'90'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with full pushdown
+explain select * from hbase_pushdown where key>'90'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_TABLE_OR_COL key) '90'))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: (key > '90')
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (key > '90')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown where key>'90'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key>'90'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+92	val_92
+95	val_95
+96	val_96
+97	val_97
+98	val_98
+PREHOOK: query: select * from hbase_pushdown where key<'1'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key<'1'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+0	val_0
+PREHOOK: query: select * from hbase_pushdown where key<='2'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key<='2'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+0	val_0
+10	val_10
+100	val_100
+103	val_103
+104	val_104
+105	val_105
+11	val_11
+111	val_111
+113	val_113
+114	val_114
+116	val_116
+118	val_118
+119	val_119
+12	val_12
+120	val_120
+125	val_125
+126	val_126
+128	val_128
+129	val_129
+131	val_131
+133	val_133
+134	val_134
+136	val_136
+137	val_137
+138	val_138
+143	val_143
+145	val_145
+146	val_146
+149	val_149
+15	val_15
+150	val_150
+152	val_152
+153	val_153
+155	val_155
+156	val_156
+157	val_157
+158	val_158
+160	val_160
+162	val_162
+163	val_163
+164	val_164
+165	val_165
+166	val_166
+167	val_167
+168	val_168
+169	val_169
+17	val_17
+170	val_170
+172	val_172
+174	val_174
+175	val_175
+176	val_176
+177	val_177
+178	val_178
+179	val_179
+18	val_18
+180	val_180
+181	val_181
+183	val_183
+186	val_186
+187	val_187
+189	val_189
+19	val_19
+190	val_190
+191	val_191
+192	val_192
+193	val_193
+194	val_194
+195	val_195
+196	val_196
+197	val_197
+199	val_199
+2	val_2
+PREHOOK: query: select * from hbase_pushdown where key>='90'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key>='90'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+90	val_90
+92	val_92
+95	val_95
+96	val_96
+97	val_97
+98	val_98
+PREHOOK: query: -- with cnostant expressinon
+explain select * from hbase_pushdown where key>=cast(40 + 50 as string)
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with cnostant expressinon
+explain select * from hbase_pushdown where key>=cast(40 + 50 as string)
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (>= (TOK_TABLE_OR_COL key) (TOK_FUNCTION TOK_STRING (+ 40 50))))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: (key >= UDFToString((40 + 50)))
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (key >= UDFToString((40 + 50)))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown where key>=cast(40 + 50 as string)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key>=cast(40 + 50 as string)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+90	val_90
+92	val_92
+95	val_95
+96	val_96
+97	val_97
+98	val_98
+PREHOOK: query: -- with partial pushdown
+
+explain select * from hbase_pushdown where key>'90' and value like '%9%'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with partial pushdown
+
+explain select * from hbase_pushdown where key>'90' and value like '%9%'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) '90') (like (TOK_TABLE_OR_COL value) '%9%')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: (key > '90')
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (value like '%9%')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown where key>'90' and value like '%9%'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key>'90' and value like '%9%'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+92	val_92
+95	val_95
+96	val_96
+97	val_97
+98	val_98
+PREHOOK: query: -- with two residuals
+
+explain select * from hbase_pushdown
+where key>='90' and value like '%9%' and key=cast(value as int)
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with two residuals
+
+explain select * from hbase_pushdown
+where key>='90' and value like '%9%' and key=cast(value as int)
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (>= (TOK_TABLE_OR_COL key) '90') (like (TOK_TABLE_OR_COL value) '%9%')) (= (TOK_TABLE_OR_COL key) (TOK_FUNCTION TOK_INT (TOK_TABLE_OR_COL value)))))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: (key >= '90')
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: ((value like '%9%') and (key = UDFToInteger(value)))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown
+where key>='90' and value like '%9%' and key=cast(value as int)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown
+where key>='90' and value like '%9%' and key=cast(value as int)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+PREHOOK: query: -- with contradictory pushdowns
+
+explain select * from hbase_pushdown
+where key<'80' and key>'90' and value like '%90%'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with contradictory pushdowns
+
+explain select * from hbase_pushdown
+where key<'80' and key>'90' and value like '%90%'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (< (TOK_TABLE_OR_COL key) '80') (> (TOK_TABLE_OR_COL key) '90')) (like (TOK_TABLE_OR_COL value) '%90%')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: ((key < '80') and (key > '90'))
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (value like '%90%')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown
+where key<'80' and key>'90' and value like '%90%'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown
+where key<'80' and key>'90' and value like '%90%'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+PREHOOK: query: -- with nothing to push down
+
+explain select * from hbase_pushdown
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with nothing to push down
+
+explain select * from hbase_pushdown
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))
+
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: -- with a predicate which is not actually part of the filter, so
+-- it should be ignored by pushdown
+
+explain select * from hbase_pushdown
+where (case when key<'90' then 2 else 4 end) > 3
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with a predicate which is not actually part of the filter, so
+-- it should be ignored by pushdown
+
+explain select * from hbase_pushdown
+where (case when key<'90' then 2 else 4 end) > 3
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_FUNCTION when (< (TOK_TABLE_OR_COL key) '90') 2 4) 3))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            Filter Operator
+              predicate:
+                  expr: (CASE WHEN ((key < '90')) THEN (2) ELSE (4) END > 3)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: -- with a predicate which is under an OR, so it should
+-- be ignored by pushdown
+
+explain select * from hbase_pushdown
+where key<='80' or value like '%90%'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with a predicate which is under an OR, so it should
+-- be ignored by pushdown
+
+explain select * from hbase_pushdown
+where key<='80' or value like '%90%'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (<= (TOK_TABLE_OR_COL key) '80') (like (TOK_TABLE_OR_COL value) '%90%')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            Filter Operator
+              predicate:
+                  expr: ((key <= '80') or (value like '%90%'))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: -- following will get pushed into hbase after HIVE-2819
+explain select * from hbase_pushdown where key > '281' 
+and key < '287'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- following will get pushed into hbase after HIVE-2819
+explain select * from hbase_pushdown where key > '281' 
+and key < '287'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) '281') (< (TOK_TABLE_OR_COL key) '287')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: ((key > '281') and (key < '287'))
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: ((key > '281') and (key < '287'))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown where key > '281' 
+and key < '287'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key > '281' 
+and key < '287'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+282	val_282
+283	val_283
+284	val_284
+285	val_285
+286	val_286
+PREHOOK: query: -- with pushdown disabled
+
+explain select * from hbase_pushdown where key<='90'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with pushdown disabled
+
+explain select * from hbase_pushdown where key<='90'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (<= (TOK_TABLE_OR_COL key) '90'))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            Filter Operator
+              predicate:
+                  expr: (key <= '90')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
diff --git a/src/hbase-handler/src/test/results/positive/hbase_pushdown.q.out b/src/hbase-handler/src/test/results/positive/hbase_pushdown.q.out
new file mode 100644
index 0000000..45927dd
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_pushdown.q.out
@@ -0,0 +1,403 @@
+PREHOOK: query: CREATE TABLE hbase_pushdown(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_pushdown(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_pushdown
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
+SELECT *
+FROM src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_pushdown
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_pushdown 
+SELECT *
+FROM src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_pushdown
+PREHOOK: query: -- with full pushdown
+explain select * from hbase_pushdown where key=90
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with full pushdown
+explain select * from hbase_pushdown where key=90
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (TOK_TABLE_OR_COL key) 90))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: (key = 90)
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (key = 90)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown where key=90
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key=90
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+90	val_90
+PREHOOK: query: -- with partial pushdown
+
+explain select * from hbase_pushdown where key=90 and value like '%90%'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with partial pushdown
+
+explain select * from hbase_pushdown where key=90 and value like '%90%'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (= (TOK_TABLE_OR_COL key) 90) (like (TOK_TABLE_OR_COL value) '%90%')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: (key = 90)
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (value like '%90%')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown where key=90 and value like '%90%'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown where key=90 and value like '%90%'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+90	val_90
+PREHOOK: query: -- with two residuals
+
+explain select * from hbase_pushdown
+where key=90 and value like '%90%' and key=cast(value as int)
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with two residuals
+
+explain select * from hbase_pushdown
+where key=90 and value like '%90%' and key=cast(value as int)
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL key) 90) (like (TOK_TABLE_OR_COL value) '%90%')) (= (TOK_TABLE_OR_COL key) (TOK_FUNCTION TOK_INT (TOK_TABLE_OR_COL value)))))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            filterExpr:
+                expr: (key = 90)
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: ((value like '%90%') and (key = UDFToInteger(value)))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: -- with contradictory pushdowns
+
+explain select * from hbase_pushdown
+where key=80 and key=90 and value like '%90%'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with contradictory pushdowns
+
+explain select * from hbase_pushdown
+where key=80 and key=90 and value like '%90%'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL key) 80) (= (TOK_TABLE_OR_COL key) 90)) (like (TOK_TABLE_OR_COL value) '%90%')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            Filter Operator
+              predicate:
+                  expr: (((key = 80) and (key = 90)) and (value like '%90%'))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_pushdown
+where key=80 and key=90 and value like '%90%'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_pushdown
+where key=80 and key=90 and value like '%90%'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_pushdown
+#### A masked pattern was here ####
+PREHOOK: query: -- with nothing to push down
+
+explain select * from hbase_pushdown
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with nothing to push down
+
+explain select * from hbase_pushdown
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))
+
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: -- with a predicate which is not actually part of the filter, so
+-- it should be ignored by pushdown
+
+explain select * from hbase_pushdown
+where (case when key=90 then 2 else 4 end) > 3
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with a predicate which is not actually part of the filter, so
+-- it should be ignored by pushdown
+
+explain select * from hbase_pushdown
+where (case when key=90 then 2 else 4 end) > 3
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (> (TOK_FUNCTION when (= (TOK_TABLE_OR_COL key) 90) 2 4) 3))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            Filter Operator
+              predicate:
+                  expr: (CASE WHEN ((key = 90)) THEN (2) ELSE (4) END > 3)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: -- with a predicate which is under an OR, so it should
+-- be ignored by pushdown
+
+explain select * from hbase_pushdown
+where key=80 or value like '%90%'
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with a predicate which is under an OR, so it should
+-- be ignored by pushdown
+
+explain select * from hbase_pushdown
+where key=80 or value like '%90%'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (or (= (TOK_TABLE_OR_COL key) 80) (like (TOK_TABLE_OR_COL value) '%90%')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            Filter Operator
+              predicate:
+                  expr: ((key = 80) or (value like '%90%'))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: -- with pushdown disabled
+
+explain select * from hbase_pushdown where key=90
+PREHOOK: type: QUERY
+POSTHOOK: query: -- with pushdown disabled
+
+explain select * from hbase_pushdown where key=90
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_pushdown))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (TOK_TABLE_OR_COL key) 90))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_pushdown 
+          TableScan
+            alias: hbase_pushdown
+            Filter Operator
+              predicate:
+                  expr: (key = 90)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
diff --git a/src/hbase-handler/src/test/results/positive/hbase_queries.q.out b/src/hbase-handler/src/test/results/positive/hbase_queries.q.out
new file mode 100644
index 0000000..6a4f68b
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_queries.q.out
@@ -0,0 +1,980 @@
+PREHOOK: query: DROP TABLE hbase_table_1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_table_1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE hbase_table_1(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_table_1(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_1
+PREHOOK: query: DESCRIBE EXTENDED hbase_table_1
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE EXTENDED hbase_table_1
+POSTHOOK: type: DESCTABLE
+key	int	from deserializer
+value	string	from deserializer
+	 	 
+#### A masked pattern was here ####
+PREHOOK: query: select * from hbase_table_1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_table_1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_1
+#### A masked pattern was here ####
+PREHOOK: query: EXPLAIN FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME hbase_table_1))) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (% (TOK_TABLE_OR_COL key) 2) 0))))
+
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Filter Operator
+              predicate:
+                  expr: ((key % 2) = 0)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                Select Operator
+                  expressions:
+                        expr: UDFToInteger(_col0)
+                        type: int
+                        expr: _col1
+                        type: string
+                  outputColumnNames: _col0, _col1
+                  File Output Operator
+                    compressed: false
+                    GlobalTableId: 1
+                    table:
+                        input format: org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat
+                        output format: org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat
+                        serde: org.apache.hadoop.hive.hbase.HBaseSerDe
+                        name: default.hbase_table_1
+
+
+PREHOOK: query: FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_table_1
+POSTHOOK: query: FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_table_1
+PREHOOK: query: DROP TABLE hbase_table_2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_table_2
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE hbase_table_2(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE hbase_table_2(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_0")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_2
+PREHOOK: query: EXPLAIN 
+SELECT Y.* 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.* FROM src) Y
+ON (x.key = Y.key)
+ORDER BY key, value LIMIT 20
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN 
+SELECT Y.* 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.* FROM src) Y
+ON (x.key = Y.key)
+ORDER BY key, value LIMIT 20
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_1)))))) x) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME src)))))) Y) (= (. (TOK_TABLE_OR_COL x) key) (. (TOK_TABLE_OR_COL Y) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME Y)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value))) (TOK_LIMIT 20)))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x:hbase_table_1 
+          TableScan
+            alias: hbase_table_1
+            Select Operator
+              expressions:
+                    expr: key
+                    type: int
+              outputColumnNames: _col0
+              Reduce Output Operator
+                key expressions:
+                      expr: UDFToDouble(_col0)
+                      type: double
+                sort order: +
+                Map-reduce partition columns:
+                      expr: UDFToDouble(_col0)
+                      type: double
+                tag: 0
+        y:src 
+          TableScan
+            alias: src
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+              outputColumnNames: _col0, _col1
+              Reduce Output Operator
+                key expressions:
+                      expr: UDFToDouble(_col0)
+                      type: double
+                sort order: +
+                Map-reduce partition columns:
+                      expr: UDFToDouble(_col0)
+                      type: double
+                tag: 1
+                value expressions:
+                      expr: _col0
+                      type: string
+                      expr: _col1
+                      type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 
+            1 {VALUE._col0} {VALUE._col1}
+          handleSkewJoin: false
+          outputColumnNames: _col2, _col3
+          Select Operator
+            expressions:
+                  expr: _col2
+                  type: string
+                  expr: _col3
+                  type: string
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+#### A masked pattern was here ####
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: string
+                    expr: _col1
+                    type: string
+              sort order: ++
+              tag: -1
+              value expressions:
+                    expr: _col0
+                    type: string
+                    expr: _col1
+                    type: string
+      Reduce Operator Tree:
+        Extract
+          Limit
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 20
+
+
+PREHOOK: query: SELECT Y.* 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.* FROM src) Y
+ON (x.key = Y.key)
+ORDER BY key, value LIMIT 20
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_1
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT Y.* 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.* FROM src) Y
+ON (x.key = Y.key)
+ORDER BY key, value LIMIT 20
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_1
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+0	val_0
+0	val_0
+0	val_0
+10	val_10
+100	val_100
+100	val_100
+104	val_104
+104	val_104
+114	val_114
+116	val_116
+118	val_118
+118	val_118
+12	val_12
+12	val_12
+120	val_120
+120	val_120
+126	val_126
+128	val_128
+128	val_128
+128	val_128
+PREHOOK: query: EXPLAIN 
+SELECT Y.*
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
+JOIN 
+(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
+ON (x.key = Y.key)
+ORDER BY key, value
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN 
+SELECT Y.*
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
+JOIN 
+(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
+ON (x.key = Y.key)
+ORDER BY key, value
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_1)))) (TOK_WHERE (> (. (TOK_TABLE_OR_COL hbase_table_1) key) 100)))) x) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_2))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_2)))) (TOK_WHERE (< (. (TOK_TABLE_OR_COL hbase_table_2) key) 120)))) Y) (= (. (TOK_TABLE_OR_COL x) key) (. (TOK_TABLE_OR_COL Y) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME Y)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL value)))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x:hbase_table_1 
+          TableScan
+            alias: hbase_table_1
+            Filter Operator
+              predicate:
+                  expr: (key > 100)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                outputColumnNames: _col0
+                Reduce Output Operator
+                  key expressions:
+                        expr: _col0
+                        type: int
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: _col0
+                        type: int
+                  tag: 0
+        y:hbase_table_2 
+          TableScan
+            alias: hbase_table_2
+            Filter Operator
+              predicate:
+                  expr: (key < 120)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                Reduce Output Operator
+                  key expressions:
+                        expr: _col0
+                        type: int
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: _col0
+                        type: int
+                  tag: 1
+                  value expressions:
+                        expr: _col0
+                        type: int
+                        expr: _col1
+                        type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 
+            1 {VALUE._col0} {VALUE._col1}
+          handleSkewJoin: false
+          outputColumnNames: _col2, _col3
+          Select Operator
+            expressions:
+                  expr: _col2
+                  type: int
+                  expr: _col3
+                  type: string
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+#### A masked pattern was here ####
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: int
+                    expr: _col1
+                    type: string
+              sort order: ++
+              tag: -1
+              value expressions:
+                    expr: _col0
+                    type: int
+                    expr: _col1
+                    type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            GlobalTableId: 0
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: SELECT Y.*
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
+JOIN 
+(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
+ON (x.key = Y.key)
+ORDER BY key,value
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_1
+PREHOOK: Input: default@hbase_table_2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT Y.*
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1 WHERE hbase_table_1.key > 100) x
+JOIN 
+(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
+ON (x.key = Y.key)
+ORDER BY key,value
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_1
+POSTHOOK: Input: default@hbase_table_2
+#### A masked pattern was here ####
+104	val_104
+114	val_114
+116	val_116
+118	val_118
+PREHOOK: query: DROP TABLE empty_hbase_table
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE empty_hbase_table
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE empty_hbase_table(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE empty_hbase_table(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@empty_hbase_table
+PREHOOK: query: DROP TABLE empty_normal_table
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE empty_normal_table
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE empty_normal_table(key int, value string)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE empty_normal_table(key int, value string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@empty_normal_table
+PREHOOK: query: select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_hbase_table) x order by c
+PREHOOK: type: QUERY
+PREHOOK: Input: default@empty_hbase_table
+PREHOOK: Input: default@empty_normal_table
+#### A masked pattern was here ####
+POSTHOOK: query: select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_hbase_table) x order by c
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@empty_hbase_table
+POSTHOOK: Input: default@empty_normal_table
+#### A masked pattern was here ####
+0
+0
+PREHOOK: query: select * from (select count(1) c from empty_normal_table union all select count(1) as c from hbase_table_1) x order by c
+PREHOOK: type: QUERY
+PREHOOK: Input: default@empty_normal_table
+PREHOOK: Input: default@hbase_table_1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from (select count(1) c from empty_normal_table union all select count(1) as c from hbase_table_1) x order by c
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@empty_normal_table
+POSTHOOK: Input: default@hbase_table_1
+#### A masked pattern was here ####
+0
+155
+PREHOOK: query: select * from (select count(1) c from src union all select count(1) as c from empty_hbase_table) x order by c
+PREHOOK: type: QUERY
+PREHOOK: Input: default@empty_hbase_table
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: select * from (select count(1) c from src union all select count(1) as c from empty_hbase_table) x order by c
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@empty_hbase_table
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+0
+500
+PREHOOK: query: select * from (select count(1) c from src union all select count(1) as c from hbase_table_1) x order by c
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_1
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: select * from (select count(1) c from src union all select count(1) as c from hbase_table_1) x order by c
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_1
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+155
+500
+PREHOOK: query: CREATE TABLE hbase_table_3(key int, value string, count int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "cf:val,cf2:count"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_table_3(key int, value string, count int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "cf:val,cf2:count"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_3
+PREHOOK: query: EXPLAIN 
+INSERT OVERWRITE TABLE hbase_table_3
+SELECT x.key, x.value, Y.count 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
+ON (x.key = Y.key)
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN 
+INSERT OVERWRITE TABLE hbase_table_3
+SELECT x.key, x.value, Y.count 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
+ON (x.key = Y.key)
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_table_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME hbase_table_1)))))) x) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) key)) (TOK_SELEXPR (TOK_FUNCTION count (. (TOK_TABLE_OR_COL src) key)) count)) (TOK_GROUPBY (. (TOK_TABLE_OR_COL src) key)))) Y) (= (. (TOK_TABLE_OR_COL x) key) (. (TOK_TABLE_OR_COL Y) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME hbase_table_3))) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL x) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL x) value)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL Y) count)))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        y:src 
+          TableScan
+            alias: src
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+              outputColumnNames: key
+              Group By Operator
+                aggregations:
+                      expr: count(key)
+                bucketGroup: false
+                keys:
+                      expr: key
+                      type: string
+                mode: hash
+                outputColumnNames: _col0, _col1
+                Reduce Output Operator
+                  key expressions:
+                        expr: _col0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: _col0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: _col1
+                        type: bigint
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: string
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: bigint
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+  Stage: Stage-0
+    Map Reduce
+      Alias -> Map Operator Tree:
+        $INTNAME 
+            Reduce Output Operator
+              key expressions:
+                    expr: UDFToDouble(_col0)
+                    type: double
+              sort order: +
+              Map-reduce partition columns:
+                    expr: UDFToDouble(_col0)
+                    type: double
+              tag: 1
+              value expressions:
+                    expr: _col1
+                    type: bigint
+        x:hbase_table_1 
+          TableScan
+            alias: hbase_table_1
+            Select Operator
+              expressions:
+                    expr: key
+                    type: int
+                    expr: value
+                    type: string
+              outputColumnNames: _col0, _col1
+              Reduce Output Operator
+                key expressions:
+                      expr: UDFToDouble(_col0)
+                      type: double
+                sort order: +
+                Map-reduce partition columns:
+                      expr: UDFToDouble(_col0)
+                      type: double
+                tag: 0
+                value expressions:
+                      expr: _col0
+                      type: int
+                      expr: _col1
+                      type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 {VALUE._col0} {VALUE._col1}
+            1 {VALUE._col1}
+          handleSkewJoin: false
+          outputColumnNames: _col0, _col1, _col3
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: int
+                  expr: _col1
+                  type: string
+                  expr: _col3
+                  type: bigint
+            outputColumnNames: _col0, _col1, _col2
+            Select Operator
+              expressions:
+                    expr: _col0
+                    type: int
+                    expr: _col1
+                    type: string
+                    expr: UDFToInteger(_col2)
+                    type: int
+              outputColumnNames: _col0, _col1, _col2
+              File Output Operator
+                compressed: false
+                GlobalTableId: 1
+                table:
+                    input format: org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat
+                    output format: org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat
+                    serde: org.apache.hadoop.hive.hbase.HBaseSerDe
+                    name: default.hbase_table_3
+
+
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_3
+SELECT x.key, x.value, Y.count 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
+ON (x.key = Y.key)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_1
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_table_3
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_3
+SELECT x.key, x.value, Y.count 
+FROM 
+(SELECT hbase_table_1.* FROM hbase_table_1) x
+JOIN 
+(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
+ON (x.key = Y.key)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_1
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_table_3
+PREHOOK: query: select count(1) from hbase_table_3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_3
+#### A masked pattern was here ####
+POSTHOOK: query: select count(1) from hbase_table_3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_3
+#### A masked pattern was here ####
+155
+PREHOOK: query: select * from hbase_table_3 order by key, value limit 5
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_3
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_table_3 order by key, value limit 5
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_3
+#### A masked pattern was here ####
+0	val_0	3
+2	val_2	1
+4	val_4	1
+8	val_8	1
+10	val_10	1
+PREHOOK: query: select key, count from hbase_table_3 order by key, count desc limit 5
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_3
+#### A masked pattern was here ####
+POSTHOOK: query: select key, count from hbase_table_3 order by key, count desc limit 5
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_3
+#### A masked pattern was here ####
+0	3
+2	1
+4	1
+8	1
+10	1
+PREHOOK: query: DROP TABLE hbase_table_4
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_table_4
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE hbase_table_4(key int, value1 string, value2 int, value3 int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "a:b,a:c,d:e"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_table_4(key int, value1 string, value2 int, value3 int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "a:b,a:c,d:e"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_4
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_4 SELECT key, value, key+1, key+2 
+FROM src WHERE key=98 OR key=100
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_table_4
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_4 SELECT key, value, key+1, key+2 
+FROM src WHERE key=98 OR key=100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_table_4
+PREHOOK: query: SELECT * FROM hbase_table_4 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_4
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM hbase_table_4 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_4
+#### A masked pattern was here ####
+98	val_98	99	100
+100	val_100	101	102
+PREHOOK: query: DROP TABLE hbase_table_5
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_table_5
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE EXTERNAL TABLE hbase_table_5(key int, value map<string,string>) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "a:")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_4")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE EXTERNAL TABLE hbase_table_5(key int, value map<string,string>) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = "a:")
+TBLPROPERTIES ("hbase.table.name" = "hbase_table_4")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_5
+PREHOOK: query: SELECT * FROM hbase_table_5 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_5
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM hbase_table_5 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_5
+#### A masked pattern was here ####
+98	{"b":"val_98","c":"99"}
+100	{"b":"val_100","c":"101"}
+PREHOOK: query: DROP TABLE hbase_table_6
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_table_6
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE hbase_table_6(key int, value map<string,string>) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = ":key,cf:"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_table_6(key int, value map<string,string>) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = ":key,cf:"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_6
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_6 SELECT key, map(value, key) FROM src
+WHERE key=98 OR key=100
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_table_6
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_6 SELECT key, map(value, key) FROM src
+WHERE key=98 OR key=100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_table_6
+PREHOOK: query: SELECT * FROM hbase_table_6 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_6
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM hbase_table_6 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_6
+#### A masked pattern was here ####
+98	{"val_98":"98"}
+100	{"val_100":"100"}
+PREHOOK: query: DROP TABLE hbase_table_7
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_table_7
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE hbase_table_7(value map<string,string>, key int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "cf:,:key"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_table_7(value map<string,string>, key int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "cf:,:key"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_7
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_7 
+SELECT map(value, key, upper(value), key+1), key FROM src
+WHERE key=98 OR key=100
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_table_7
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_7 
+SELECT map(value, key, upper(value), key+1), key FROM src
+WHERE key=98 OR key=100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_table_7
+PREHOOK: query: SELECT * FROM hbase_table_7 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_7
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM hbase_table_7 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_7
+#### A masked pattern was here ####
+{"VAL_98":"99.0","val_98":"98"}	98
+{"VAL_100":"101.0","val_100":"100"}	100
+PREHOOK: query: DROP TABLE hbase_table_8
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE hbase_table_8
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE hbase_table_8(key int, value1 string, value2 int, value3 int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "a:b,a:c,d:e"
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_table_8(key int, value1 string, value2 int, value3 int) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+"hbase.columns.mapping" = "a:b,a:c,d:e"
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_table_8
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_table_8 SELECT key, value, key+1, key+2 
+FROM src WHERE key=98 OR key=100
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_table_8
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_table_8 SELECT key, value, key+1, key+2 
+FROM src WHERE key=98 OR key=100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_table_8
+PREHOOK: query: SELECT * FROM hbase_table_8 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_table_8
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM hbase_table_8 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_table_8
+#### A masked pattern was here ####
+98	val_98	99	100
+100	val_100	101	102
+PREHOOK: query: DROP TABLE hbase_table_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_1
+PREHOOK: Output: default@hbase_table_1
+POSTHOOK: query: DROP TABLE hbase_table_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_1
+POSTHOOK: Output: default@hbase_table_1
+PREHOOK: query: DROP TABLE hbase_table_2
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_2
+PREHOOK: Output: default@hbase_table_2
+POSTHOOK: query: DROP TABLE hbase_table_2
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_2
+POSTHOOK: Output: default@hbase_table_2
+PREHOOK: query: DROP TABLE hbase_table_3
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_3
+PREHOOK: Output: default@hbase_table_3
+POSTHOOK: query: DROP TABLE hbase_table_3
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_3
+POSTHOOK: Output: default@hbase_table_3
+PREHOOK: query: DROP TABLE hbase_table_4
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_4
+PREHOOK: Output: default@hbase_table_4
+POSTHOOK: query: DROP TABLE hbase_table_4
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_4
+POSTHOOK: Output: default@hbase_table_4
+PREHOOK: query: DROP TABLE hbase_table_5
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_5
+PREHOOK: Output: default@hbase_table_5
+POSTHOOK: query: DROP TABLE hbase_table_5
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_5
+POSTHOOK: Output: default@hbase_table_5
+PREHOOK: query: DROP TABLE hbase_table_6
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_6
+PREHOOK: Output: default@hbase_table_6
+POSTHOOK: query: DROP TABLE hbase_table_6
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_6
+POSTHOOK: Output: default@hbase_table_6
+PREHOOK: query: DROP TABLE hbase_table_7
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_7
+PREHOOK: Output: default@hbase_table_7
+POSTHOOK: query: DROP TABLE hbase_table_7
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_7
+POSTHOOK: Output: default@hbase_table_7
+PREHOOK: query: DROP TABLE hbase_table_8
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_table_8
+PREHOOK: Output: default@hbase_table_8
+POSTHOOK: query: DROP TABLE hbase_table_8
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_table_8
+POSTHOOK: Output: default@hbase_table_8
+PREHOOK: query: DROP TABLE empty_hbase_table
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@empty_hbase_table
+PREHOOK: Output: default@empty_hbase_table
+POSTHOOK: query: DROP TABLE empty_hbase_table
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@empty_hbase_table
+POSTHOOK: Output: default@empty_hbase_table
+PREHOOK: query: DROP TABLE empty_normal_table
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@empty_normal_table
+PREHOOK: Output: default@empty_normal_table
+POSTHOOK: query: DROP TABLE empty_normal_table
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@empty_normal_table
+POSTHOOK: Output: default@empty_normal_table
diff --git a/src/hbase-handler/src/test/results/positive/hbase_stats.q.out b/src/hbase-handler/src/test/results/positive/hbase_stats.q.out
new file mode 100644
index 0000000..c8cac9e
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_stats.q.out
@@ -0,0 +1,390 @@
+PREHOOK: query: create table stats_src like src
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table stats_src like src
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@stats_src
+PREHOOK: query: insert overwrite table stats_src select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_src
+POSTHOOK: query: insert overwrite table stats_src select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_src
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table stats_src compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_src
+PREHOOK: Output: default@stats_src
+POSTHOOK: query: analyze table stats_src compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_src
+POSTHOOK: Output: default@stats_src
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: desc formatted stats_src
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_src
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	numFiles            	1                   
+	numPartitions       	0                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: create table stats_part like srcpart
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table stats_part like srcpart
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+PREHOOK: Output: default@stats_part
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+PREHOOK: Output: default@stats_part
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: desc formatted stats_part
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	numFiles            	3                   
+	numPartitions       	3                   
+	numRows             	1500                
+	rawDataSize         	15936               
+	totalSize           	17436               
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2010-04-08, 11]    	 
+Database:           	default             	 
+Table:              	stats_part          	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	numFiles            	1                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2010-04-08, 12]    	 
+Database:           	default             	 
+Table:              	stats_part          	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	numFiles            	1                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
+PREHOOK: Output: default@stats_part
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: desc formatted stats_part
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	numFiles            	3                   
+	numPartitions       	3                   
+	numRows             	1500                
+	rawDataSize         	15936               
+	totalSize           	17436               
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: drop table stats_src
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@stats_src
+PREHOOK: Output: default@stats_src
+POSTHOOK: query: drop table stats_src
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@stats_src
+POSTHOOK: Output: default@stats_src
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: drop table stats_part
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@stats_part
+PREHOOK: Output: default@stats_part
+POSTHOOK: query: drop table stats_part
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@stats_part
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/positive/hbase_stats2.q.out b/src/hbase-handler/src/test/results/positive/hbase_stats2.q.out
new file mode 100644
index 0000000..c8cac9e
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/hbase_stats2.q.out
@@ -0,0 +1,390 @@
+PREHOOK: query: create table stats_src like src
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table stats_src like src
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@stats_src
+PREHOOK: query: insert overwrite table stats_src select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_src
+POSTHOOK: query: insert overwrite table stats_src select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_src
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table stats_src compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_src
+PREHOOK: Output: default@stats_src
+POSTHOOK: query: analyze table stats_src compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_src
+POSTHOOK: Output: default@stats_src
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: desc formatted stats_src
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_src
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	numFiles            	1                   
+	numPartitions       	0                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: create table stats_part like srcpart
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table stats_part like srcpart
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+PREHOOK: Output: default@stats_part
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+PREHOOK: Output: default@stats_part
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: query: analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: query: insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: desc formatted stats_part
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	numFiles            	3                   
+	numPartitions       	3                   
+	numRows             	1500                
+	rawDataSize         	15936               
+	totalSize           	17436               
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '11')
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2010-04-08, 11]    	 
+Database:           	default             	 
+Table:              	stats_part          	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	numFiles            	1                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part partition (ds='2010-04-08', hr = '12')
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2010-04-08, 12]    	 
+Database:           	default             	 
+Table:              	stats_part          	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	numFiles            	1                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
+PREHOOK: type: QUERY
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+PREHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
+PREHOOK: Output: default@stats_part
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+PREHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: query: analyze table stats_part partition(ds, hr) compute statistics
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Input: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=11
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=12
+POSTHOOK: Output: default@stats_part@ds=2010-04-08/hr=13
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: desc formatted stats_part
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: desc formatted stats_part
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	default             
+value               	string              	default             
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Table Information	 	 
+Database:           	default             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+Retention:          	0                   	 
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	 
+Table Parameters:	 	 
+	numFiles            	3                   
+	numPartitions       	3                   
+	numRows             	1500                
+	rawDataSize         	15936               
+	totalSize           	17436               
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: drop table stats_src
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@stats_src
+PREHOOK: Output: default@stats_src
+POSTHOOK: query: drop table stats_src
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@stats_src
+POSTHOOK: Output: default@stats_src
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: drop table stats_part
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@stats_part
+PREHOOK: Output: default@stats_part
+POSTHOOK: query: drop table stats_part
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@stats_part
+POSTHOOK: Output: default@stats_part
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_part PARTITION(ds=2010-04-08,hr=13).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: stats_src.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/src/hbase-handler/src/test/results/positive/ppd_key_ranges.q.out b/src/hbase-handler/src/test/results/positive/ppd_key_ranges.q.out
new file mode 100644
index 0000000..460c3d0
--- /dev/null
+++ b/src/hbase-handler/src/test/results/positive/ppd_key_ranges.q.out
@@ -0,0 +1,251 @@
+PREHOOK: query: CREATE TABLE hbase_ppd_keyrange(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#binary,cf:string")
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE hbase_ppd_keyrange(key int, value string) 
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#binary,cf:string")
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@hbase_ppd_keyrange
+PREHOOK: query: INSERT OVERWRITE TABLE hbase_ppd_keyrange 
+SELECT *
+FROM src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hbase_ppd_keyrange
+POSTHOOK: query: INSERT OVERWRITE TABLE hbase_ppd_keyrange 
+SELECT *
+FROM src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hbase_ppd_keyrange
+PREHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key < 21
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key < 21
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) 8) (< (TOK_TABLE_OR_COL key) 21)))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_ppd_keyrange 
+          TableScan
+            alias: hbase_ppd_keyrange
+            filterExpr:
+                expr: ((key > 8) and (key < 21))
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: ((key > 8) and (key < 21))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key < 21
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key < 21
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+9	val_9
+10	val_10
+11	val_11
+12	val_12
+15	val_15
+17	val_17
+18	val_18
+19	val_19
+20	val_20
+PREHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) 8) (<= (TOK_TABLE_OR_COL key) 17)))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_ppd_keyrange 
+          TableScan
+            alias: hbase_ppd_keyrange
+            filterExpr:
+                expr: ((key > 8) and (key <= 17))
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: ((key > 8) and (key <= 17))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+9	val_9
+10	val_10
+11	val_11
+12	val_12
+15	val_15
+17	val_17
+PREHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (> (TOK_TABLE_OR_COL key) 8) (<= (TOK_TABLE_OR_COL key) 17)) (like (TOK_TABLE_OR_COL value) '%11%')))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_ppd_keyrange 
+          TableScan
+            alias: hbase_ppd_keyrange
+            filterExpr:
+                expr: ((key > 8) and (key <= 17))
+                type: boolean
+            Filter Operator
+              predicate:
+                  expr: (value like '%11%')
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+11	val_11
+PREHOOK: query: explain select * from hbase_ppd_keyrange where key >= 9 and key < 17 and key = 11
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select * from hbase_ppd_keyrange where key >= 9 and key < 17 and key = 11
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (>= (TOK_TABLE_OR_COL key) 9) (< (TOK_TABLE_OR_COL key) 17)) (= (TOK_TABLE_OR_COL key) 11)))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        hbase_ppd_keyrange 
+          TableScan
+            alias: hbase_ppd_keyrange
+            Filter Operator
+              predicate:
+                  expr: (((key >= 9) and (key < 17)) and (key = 11))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: int
+                      expr: value
+                      type: string
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+PREHOOK: query: select * from hbase_ppd_keyrange where key >=9  and key < 17 and key = 11
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hbase_ppd_keyrange where key >=9  and key < 17 and key = 11
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hbase_ppd_keyrange
+#### A masked pattern was here ####
+11	val_11
+PREHOOK: query: drop table  hbase_ppd_keyrange
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hbase_ppd_keyrange
+PREHOOK: Output: default@hbase_ppd_keyrange
+POSTHOOK: query: drop table  hbase_ppd_keyrange
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hbase_ppd_keyrange
+POSTHOOK: Output: default@hbase_ppd_keyrange
diff --git a/src/hbase-handler/src/test/results/ppd_key_ranges.q.out b/src/hbase-handler/src/test/results/ppd_key_ranges.q.out
deleted file mode 100644
index 460c3d0..0000000
--- a/src/hbase-handler/src/test/results/ppd_key_ranges.q.out
+++ /dev/null
@@ -1,251 +0,0 @@
-PREHOOK: query: CREATE TABLE hbase_ppd_keyrange(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#binary,cf:string")
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE hbase_ppd_keyrange(key int, value string) 
-STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
-WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#binary,cf:string")
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@hbase_ppd_keyrange
-PREHOOK: query: INSERT OVERWRITE TABLE hbase_ppd_keyrange 
-SELECT *
-FROM src
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@hbase_ppd_keyrange
-POSTHOOK: query: INSERT OVERWRITE TABLE hbase_ppd_keyrange 
-SELECT *
-FROM src
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@hbase_ppd_keyrange
-PREHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key < 21
-PREHOOK: type: QUERY
-POSTHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key < 21
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) 8) (< (TOK_TABLE_OR_COL key) 21)))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_ppd_keyrange 
-          TableScan
-            alias: hbase_ppd_keyrange
-            filterExpr:
-                expr: ((key > 8) and (key < 21))
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: ((key > 8) and (key < 21))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key < 21
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key < 21
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-9	val_9
-10	val_10
-11	val_11
-12	val_12
-15	val_15
-17	val_17
-18	val_18
-19	val_19
-20	val_20
-PREHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17
-PREHOOK: type: QUERY
-POSTHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (> (TOK_TABLE_OR_COL key) 8) (<= (TOK_TABLE_OR_COL key) 17)))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_ppd_keyrange 
-          TableScan
-            alias: hbase_ppd_keyrange
-            filterExpr:
-                expr: ((key > 8) and (key <= 17))
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: ((key > 8) and (key <= 17))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-9	val_9
-10	val_10
-11	val_11
-12	val_12
-15	val_15
-17	val_17
-PREHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
-PREHOOK: type: QUERY
-POSTHOOK: query: explain select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (> (TOK_TABLE_OR_COL key) 8) (<= (TOK_TABLE_OR_COL key) 17)) (like (TOK_TABLE_OR_COL value) '%11%')))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_ppd_keyrange 
-          TableScan
-            alias: hbase_ppd_keyrange
-            filterExpr:
-                expr: ((key > 8) and (key <= 17))
-                type: boolean
-            Filter Operator
-              predicate:
-                  expr: (value like '%11%')
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%'
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-11	val_11
-PREHOOK: query: explain select * from hbase_ppd_keyrange where key >= 9 and key < 17 and key = 11
-PREHOOK: type: QUERY
-POSTHOOK: query: explain select * from hbase_ppd_keyrange where key >= 9 and key < 17 and key = 11
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME hbase_ppd_keyrange))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (>= (TOK_TABLE_OR_COL key) 9) (< (TOK_TABLE_OR_COL key) 17)) (= (TOK_TABLE_OR_COL key) 11)))))
-
-STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 is a root stage
-
-STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Alias -> Map Operator Tree:
-        hbase_ppd_keyrange 
-          TableScan
-            alias: hbase_ppd_keyrange
-            Filter Operator
-              predicate:
-                  expr: (((key >= 9) and (key < 17)) and (key = 11))
-                  type: boolean
-              Select Operator
-                expressions:
-                      expr: key
-                      type: int
-                      expr: value
-                      type: string
-                outputColumnNames: _col0, _col1
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-
-  Stage: Stage-0
-    Fetch Operator
-      limit: -1
-
-
-PREHOOK: query: select * from hbase_ppd_keyrange where key >=9  and key < 17 and key = 11
-PREHOOK: type: QUERY
-PREHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-POSTHOOK: query: select * from hbase_ppd_keyrange where key >=9  and key < 17 and key = 11
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@hbase_ppd_keyrange
-#### A masked pattern was here ####
-11	val_11
-PREHOOK: query: drop table  hbase_ppd_keyrange
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@hbase_ppd_keyrange
-PREHOOK: Output: default@hbase_ppd_keyrange
-POSTHOOK: query: drop table  hbase_ppd_keyrange
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@hbase_ppd_keyrange
-POSTHOOK: Output: default@hbase_ppd_keyrange
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
index d239454..fd4b6a2 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java
@@ -458,6 +458,13 @@ public class HiveMetaStoreClient implements IMetaStoreClient {
       }
       return;
     }
+
+    if (cascade) {
+       List<String> tableList = getAllTables(name);
+       for (String table : tableList) {
+            dropTable(name, table, deleteData, false);
+        }
+    }
     client.drop_database(name, deleteData, cascade);
   }
 
-- 
1.7.0.4

