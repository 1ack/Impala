From f2889262b79df897ba406b6816b0adac748ffc4f Mon Sep 17 00:00:00 2001
From: Zhenxiao Luo <zhenxiao@cloudera.com>
Date: Mon, 9 Jul 2012 21:44:17 -0700
Subject: [PATCH 058/144] HIVE-3001. Returning Meaningful Error Codes & Messages

Reason: New Feature
Author: Bhushan Mandhani
Ref: CDH-6661
---
 .../clientnegative/invalid_row_sequence.q.out      |    2 +-
 .../results/clientnegative/udtf_explode2.q.out     |    2 +-
 .../apache/hadoop/hive/jdbc/TestJdbcDriver.java    |   25 +-
 ql/src/java/org/apache/hadoop/hive/ql/Driver.java  |   38 +-
 .../java/org/apache/hadoop/hive/ql/ErrorMsg.java   |  433 ++++++++++++++++++++
 .../org/apache/hadoop/hive/ql/exec/ExecDriver.java |    2 +-
 .../hadoop/hive/ql/exec/HadoopJobExecHelper.java   |    4 +
 .../apache/hadoop/hive/ql/exec/JobDebugger.java    |  106 ++++-
 .../apache/hadoop/hive/ql/exec/ScriptOperator.java |   13 +-
 .../org/apache/hadoop/hive/ql/exec/Utilities.java  |    2 +-
 .../zookeeper/ZooKeeperHiveLockManager.java        |    2 +-
 .../hadoop/hive/ql/optimizer/GenMRFileSink1.java   |    2 +-
 .../hadoop/hive/ql/optimizer/MapJoinFactory.java   |    2 +-
 .../hadoop/hive/ql/optimizer/MapJoinProcessor.java |    2 +-
 .../hive/ql/optimizer/ppr/PartitionPruner.java     |    2 +-
 .../hadoop/hive/ql/parse/BaseSemanticAnalyzer.java |    1 +
 .../hadoop/hive/ql/parse/DDLSemanticAnalyzer.java  |    1 +
 .../org/apache/hadoop/hive/ql/parse/ErrorMsg.java  |  375 -----------------
 .../org/apache/hadoop/hive/ql/parse/EximUtil.java  |    1 +
 .../hive/ql/parse/ExportSemanticAnalyzer.java      |    1 +
 .../hive/ql/parse/FunctionSemanticAnalyzer.java    |    1 +
 .../hive/ql/parse/ImportSemanticAnalyzer.java      |    1 +
 .../hadoop/hive/ql/parse/LoadSemanticAnalyzer.java |    1 +
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    1 +
 .../hadoop/hive/ql/parse/TypeCheckProcFactory.java |    1 +
 .../hive/ql/plan/ExprNodeGenericFuncDesc.java      |    2 +-
 .../metadata/TestSemanticAnalyzerHookLoading.java  |    3 +-
 .../alter_concatenate_indexed_table.q.out          |    2 +-
 .../clientnegative/alter_view_failure5.q.out       |    2 +-
 .../clientnegative/alter_view_failure6.q.out       |    4 +-
 .../clientnegative/alter_view_failure7.q.out       |    2 +-
 .../results/clientnegative/ambiguous_col.q.out     |    2 +-
 ql/src/test/results/clientnegative/analyze.q.out   |    2 +-
 ql/src/test/results/clientnegative/analyze1.q.out  |    2 +-
 .../test/results/clientnegative/analyze_view.q.out |    2 +-
 ql/src/test/results/clientnegative/archive3.q.out  |    2 +-
 ql/src/test/results/clientnegative/archive4.q.out  |    2 +-
 ql/src/test/results/clientnegative/archive5.q.out  |    2 +-
 .../results/clientnegative/archive_insert1.q.out   |    2 +-
 .../results/clientnegative/archive_insert2.q.out   |    2 +-
 .../results/clientnegative/archive_insert3.q.out   |    2 +-
 .../results/clientnegative/archive_insert4.q.out   |    2 +-
 .../results/clientnegative/archive_partspec1.q.out |    2 +-
 .../results/clientnegative/archive_partspec2.q.out |    2 +-
 .../results/clientnegative/archive_partspec3.q.out |    2 +-
 .../results/clientnegative/archive_partspec4.q.out |    2 +-
 .../results/clientnegative/archive_partspec5.q.out |    2 +-
 .../results/clientnegative/bad_indextype.q.out     |    2 +-
 .../results/clientnegative/bad_sample_clause.q.out |    2 +-
 .../clientnegative/clusterbydistributeby.q.out     |    2 +-
 .../results/clientnegative/clusterbyorderby.q.out  |    2 +-
 .../results/clientnegative/clusterbysortby.q.out   |    2 +-
 ql/src/test/results/clientnegative/clustern1.q.out |    2 +-
 ql/src/test/results/clientnegative/clustern2.q.out |    2 +-
 ql/src/test/results/clientnegative/clustern3.q.out |    2 +-
 ql/src/test/results/clientnegative/clustern4.q.out |    2 +-
 .../results/clientnegative/column_rename3.q.out    |    2 +-
 .../clientnegative/compare_double_bigint.q.out     |    2 +-
 .../clientnegative/compare_string_bigint.q.out     |    2 +-
 .../create_insert_outputformat.q.out               |    2 +-
 .../clientnegative/create_or_replace_view4.q.out   |    2 +-
 .../clientnegative/create_or_replace_view5.q.out   |    2 +-
 .../clientnegative/create_or_replace_view6.q.out   |    2 +-
 .../clientnegative/create_or_replace_view7.q.out   |    2 +-
 .../clientnegative/create_or_replace_view8.q.out   |    2 +-
 .../clientnegative/create_table_failure1.q.out     |    2 +-
 .../clientnegative/create_table_failure2.q.out     |    2 +-
 .../clientnegative/create_udaf_failure.q.out       |    2 +-
 .../clientnegative/create_view_failure3.q.out      |    2 +-
 .../clientnegative/create_view_failure5.q.out      |    2 +-
 .../clientnegative/create_view_failure6.q.out      |    2 +-
 .../clientnegative/create_view_failure7.q.out      |    2 +-
 .../clientnegative/create_view_failure8.q.out      |    2 +-
 .../clientnegative/create_view_failure9.q.out      |    2 +-
 ql/src/test/results/clientnegative/ctas.q.out      |    2 +-
 ql/src/test/results/clientnegative/ddltime.q.out   |    2 +-
 .../clientnegative/default_partition_name.q.out    |    2 +-
 .../clientnegative/drop_function_failure.q.out     |    2 +-
 .../clientnegative/drop_index_failure.q.out        |    2 +-
 .../clientnegative/drop_partition_failure.q.out    |    2 +-
 .../drop_partition_filter_failure.q.out            |    2 +-
 .../clientnegative/drop_table_failure1.q.out       |    2 +-
 .../clientnegative/drop_view_failure2.q.out        |    2 +-
 .../duplicate_alias_in_transform.q.out             |    2 +-
 .../duplicate_alias_in_transform_schema.q.out      |    2 +-
 .../results/clientnegative/duplicate_insert1.q.out |    2 +-
 .../results/clientnegative/duplicate_insert2.q.out |    2 +-
 ql/src/test/results/clientnegative/dyn_part1.q.out |    2 +-
 ql/src/test/results/clientnegative/dyn_part2.q.out |    2 +-
 ql/src/test/results/clientnegative/dyn_part4.q.out |    2 +-
 .../results/clientnegative/dyn_part_merge.q.out    |    2 +-
 .../exim_00_unsupported_schema.q.out               |    2 +-
 .../exim_01_nonpart_over_loaded.q.out              |    2 +-
 .../exim_02_all_part_over_overlap.q.out            |    2 +-
 .../exim_03_nonpart_noncompat_colschema.q.out      |    2 +-
 .../exim_04_nonpart_noncompat_colnumber.q.out      |    2 +-
 .../exim_05_nonpart_noncompat_coltype.q.out        |    2 +-
 .../exim_06_nonpart_noncompat_storage.q.out        |    2 +-
 .../exim_07_nonpart_noncompat_ifof.q.out           |    2 +-
 .../exim_08_nonpart_noncompat_serde.q.out          |    2 +-
 .../exim_09_nonpart_noncompat_serdeparam.q.out     |    2 +-
 .../exim_10_nonpart_noncompat_bucketing.q.out      |    2 +-
 .../exim_11_nonpart_noncompat_sorting.q.out        |    2 +-
 .../clientnegative/exim_12_nonnative_export.q.out  |    2 +-
 .../clientnegative/exim_13_nonnative_import.q.out  |    2 +-
 .../clientnegative/exim_14_nonpart_part.q.out      |    2 +-
 .../clientnegative/exim_15_part_nonpart.q.out      |    2 +-
 .../exim_16_part_noncompat_schema.q.out            |    2 +-
 .../exim_17_part_spec_underspec.q.out              |    2 +-
 .../clientnegative/exim_18_part_spec_missing.q.out |    2 +-
 .../exim_19_external_over_existing.q.out           |    2 +-
 .../exim_21_part_managed_external.q.out            |    2 +-
 .../clientnegative/fileformat_bad_class.q.out      |    2 +-
 .../clientnegative/fileformat_void_input.q.out     |    2 +-
 .../clientnegative/fileformat_void_output.q.out    |    2 +-
 .../results/clientnegative/fs_default_name1.q.out  |    5 +-
 .../results/clientnegative/fs_default_name2.q.out  |    5 +-
 .../results/clientnegative/genericFileFormat.q.out |    2 +-
 .../groupby2_map_skew_multi_distinct.q.out         |    2 +-
 .../clientnegative/groupby2_multi_distinct.q.out   |    2 +-
 .../groupby3_map_skew_multi_distinct.q.out         |    2 +-
 .../clientnegative/groupby3_multi_distinct.q.out   |    2 +-
 .../test/results/clientnegative/groupby_key.q.out  |    2 +-
 ql/src/test/results/clientnegative/having1.q.out   |    2 +-
 .../clientnegative/index_bitmap_no_map_aggr.q.out  |    2 +-
 .../clientnegative/index_compact_entry_limit.q.out |    4 +-
 .../clientnegative/index_compact_size_limit.q.out  |    4 +-
 ql/src/test/results/clientnegative/input1.q.out    |    2 +-
 ql/src/test/results/clientnegative/input2.q.out    |    2 +-
 ql/src/test/results/clientnegative/input4.q.out    |    2 +-
 ql/src/test/results/clientnegative/input41.q.out   |    2 +-
 .../results/clientnegative/input_part0_neg.q.out   |    2 +-
 .../clientnegative/insert_view_failure.q.out       |    2 +-
 .../results/clientnegative/insertexternal1.q.out   |    2 +-
 .../insertover_dynapart_ifnotexists.q.out          |    2 +-
 .../clientnegative/invalid_avg_syntax.q.out        |    2 +-
 .../invalid_cast_from_binary_1.q.out               |    2 +-
 .../invalid_cast_from_binary_2.q.out               |    2 +-
 .../invalid_cast_from_binary_3.q.out               |    2 +-
 .../invalid_cast_from_binary_4.q.out               |    2 +-
 .../invalid_cast_from_binary_5.q.out               |    2 +-
 .../invalid_cast_from_binary_6.q.out               |    2 +-
 .../clientnegative/invalid_cast_to_binary_1.q.out  |    2 +-
 .../clientnegative/invalid_cast_to_binary_2.q.out  |    2 +-
 .../clientnegative/invalid_cast_to_binary_3.q.out  |    2 +-
 .../clientnegative/invalid_cast_to_binary_4.q.out  |    2 +-
 .../clientnegative/invalid_cast_to_binary_5.q.out  |    2 +-
 .../clientnegative/invalid_cast_to_binary_6.q.out  |    2 +-
 .../clientnegative/invalid_create_tbl1.q.out       |    2 +-
 .../clientnegative/invalid_create_tbl2.q.out       |    2 +-
 .../clientnegative/invalid_max_syntax.q.out        |    2 +-
 .../clientnegative/invalid_min_syntax.q.out        |    2 +-
 .../clientnegative/invalid_select_expression.q.out |    2 +-
 .../clientnegative/invalid_std_syntax.q.out        |    2 +-
 .../invalid_stddev_samp_syntax.q.out               |    2 +-
 .../clientnegative/invalid_sum_syntax.q.out        |    2 +-
 .../results/clientnegative/invalid_t_alter1.q.out  |    2 +-
 .../results/clientnegative/invalid_t_alter2.q.out  |    2 +-
 .../results/clientnegative/invalid_t_create1.q.out |    2 +-
 .../results/clientnegative/invalid_t_create2.q.out |    2 +-
 .../clientnegative/invalid_t_transform.q.out       |    2 +-
 .../results/clientnegative/invalid_tbl_name.q.out  |    2 +-
 .../clientnegative/invalid_var_samp_syntax.q.out   |    2 +-
 .../clientnegative/invalid_variance_syntax.q.out   |    2 +-
 .../results/clientnegative/invalidate_view1.q.out  |    2 +-
 ql/src/test/results/clientnegative/join2.q.out     |    2 +-
 ql/src/test/results/clientnegative/joinneg.q.out   |    2 +-
 .../clientnegative/lateral_view_alias.q.out        |    2 +-
 .../results/clientnegative/lateral_view_join.q.out |    2 +-
 .../results/clientnegative/line_terminator.q.out   |    2 +-
 .../results/clientnegative/load_non_native.q.out   |    2 +-
 .../results/clientnegative/load_part_nospec.q.out  |    2 +-
 .../results/clientnegative/load_view_failure.q.out |    2 +-
 .../clientnegative/load_wrong_noof_part.q.out      |    2 +-
 .../clientnegative/local_mapred_error_cache.q.out  |    6 +-
 .../clientnegative/mapreduce_stack_trace.q.out     |    2 +-
 .../mapreduce_stack_trace_turnoff.q.out            |    2 +-
 .../results/clientnegative/merge_negative_1.q.out  |    2 +-
 .../results/clientnegative/merge_negative_2.q.out  |    2 +-
 .../clientnegative/minimr_broken_pipe.q.out        |    2 +-
 .../results/clientnegative/no_matching_udf.q.out   |    2 +-
 .../results/clientnegative/nonkey_groupby.q.out    |    2 +-
 .../results/clientnegative/nopart_insert.q.out     |    2 +-
 .../test/results/clientnegative/nopart_load.q.out  |    2 +-
 .../results/clientnegative/notable_alias3.q.out    |    2 +-
 .../results/clientnegative/notable_alias4.q.out    |    2 +-
 .../results/clientnegative/orderbysortby.q.out     |    2 +-
 .../clientnegative/part_col_complex_type.q.out     |    2 +-
 .../results/clientnegative/protectmode_part.q.out  |    2 +-
 .../results/clientnegative/protectmode_part1.q.out |    2 +-
 .../results/clientnegative/protectmode_part2.q.out |    2 +-
 .../results/clientnegative/protectmode_tbl1.q.out  |    2 +-
 .../results/clientnegative/protectmode_tbl2.q.out  |    2 +-
 .../results/clientnegative/protectmode_tbl3.q.out  |    2 +-
 .../results/clientnegative/protectmode_tbl4.q.out  |    2 +-
 .../results/clientnegative/protectmode_tbl5.q.out  |    2 +-
 .../results/clientnegative/recursive_view.q.out    |    2 +-
 .../test/results/clientnegative/regex_col_1.q.out  |    2 +-
 .../test/results/clientnegative/regex_col_2.q.out  |    2 +-
 .../results/clientnegative/regex_col_groupby.q.out |    2 +-
 ql/src/test/results/clientnegative/sample.q.out    |    2 +-
 .../clientnegative/select_charliteral.q.out        |    2 +-
 .../results/clientnegative/select_udtf_alias.q.out |    2 +-
 ql/src/test/results/clientnegative/semijoin1.q.out |    2 +-
 ql/src/test/results/clientnegative/semijoin2.q.out |    2 +-
 ql/src/test/results/clientnegative/semijoin3.q.out |    2 +-
 ql/src/test/results/clientnegative/semijoin4.q.out |    2 +-
 .../results/clientnegative/show_tables_bad1.q.out  |    2 +-
 .../results/clientnegative/show_tables_bad2.q.out  |    2 +-
 .../results/clientnegative/smb_bucketmapjoin.q.out |    2 +-
 .../clientnegative/split_sample_out_of_range.q.out |    2 +-
 .../clientnegative/split_sample_wrong_format.q.out |    2 +-
 .../test/results/clientnegative/strict_join.q.out  |    2 +-
 .../results/clientnegative/strict_orderby.q.out    |    2 +-
 .../results/clientnegative/strict_pruning.q.out    |    2 +-
 .../test/results/clientnegative/subq_insert.q.out  |    2 +-
 .../clientnegative/udaf_invalid_place.q.out        |    2 +-
 .../clientnegative/udf_array_contains_wrong1.q.out |    2 +-
 .../clientnegative/udf_array_contains_wrong2.q.out |    2 +-
 .../clientnegative/udf_case_type_wrong.q.out       |    2 +-
 .../clientnegative/udf_case_type_wrong2.q.out      |    2 +-
 .../clientnegative/udf_case_type_wrong3.q.out      |    2 +-
 .../test/results/clientnegative/udf_coalesce.q.out |    2 +-
 .../clientnegative/udf_concat_ws_wrong1.q.out      |    2 +-
 .../clientnegative/udf_concat_ws_wrong2.q.out      |    2 +-
 .../clientnegative/udf_concat_ws_wrong3.q.out      |    2 +-
 .../clientnegative/udf_elt_wrong_args_len.q.out    |    2 +-
 .../clientnegative/udf_elt_wrong_type.q.out        |    2 +-
 .../clientnegative/udf_field_wrong_args_len.q.out  |    2 +-
 .../clientnegative/udf_field_wrong_type.q.out      |    2 +-
 .../results/clientnegative/udf_if_not_bool.q.out   |    2 +-
 .../clientnegative/udf_if_wrong_args_len.q.out     |    2 +-
 ql/src/test/results/clientnegative/udf_in.q.out    |    2 +-
 .../clientnegative/udf_instr_wrong_args_len.q.out  |    2 +-
 .../clientnegative/udf_instr_wrong_type.q.out      |    2 +-
 .../clientnegative/udf_locate_wrong_args_len.q.out |    2 +-
 .../clientnegative/udf_locate_wrong_type.q.out     |    2 +-
 .../clientnegative/udf_map_keys_arg_num.q.out      |    2 +-
 .../clientnegative/udf_map_keys_arg_type.q.out     |    2 +-
 .../clientnegative/udf_map_values_arg_num.q.out    |    2 +-
 .../clientnegative/udf_map_values_arg_type.q.out   |    2 +-
 ql/src/test/results/clientnegative/udf_max.q.out   |    2 +-
 ql/src/test/results/clientnegative/udf_min.q.out   |    2 +-
 .../results/clientnegative/udf_printf_wrong1.q.out |    2 +-
 .../results/clientnegative/udf_printf_wrong2.q.out |    2 +-
 .../results/clientnegative/udf_printf_wrong3.q.out |    2 +-
 .../results/clientnegative/udf_printf_wrong4.q.out |    2 +-
 .../clientnegative/udf_size_wrong_args_len.q.out   |    2 +-
 .../clientnegative/udf_size_wrong_type.q.out       |    2 +-
 .../clientnegative/udf_sort_array_wrong1.q.out     |    2 +-
 .../clientnegative/udf_sort_array_wrong2.q.out     |    2 +-
 .../clientnegative/udf_sort_array_wrong3.q.out     |    2 +-
 .../clientnegative/udf_when_type_wrong.q.out       |    2 +-
 .../clientnegative/udf_when_type_wrong2.q.out      |    2 +-
 .../clientnegative/udf_when_type_wrong3.q.out      |    2 +-
 .../udtf_explode_not_supported1.q.out              |    2 +-
 .../udtf_explode_not_supported2.q.out              |    2 +-
 .../udtf_explode_not_supported3.q.out              |    2 +-
 .../udtf_explode_not_supported4.q.out              |    2 +-
 .../clientnegative/udtf_invalid_place.q.out        |    2 +-
 .../clientnegative/udtf_not_supported1.q.out       |    2 +-
 .../clientnegative/udtf_not_supported2.q.out       |    2 +-
 .../clientnegative/udtf_not_supported3.q.out       |    2 +-
 ql/src/test/results/clientnegative/union.q.out     |    2 +-
 ql/src/test/results/clientnegative/union2.q.out    |    2 +-
 ql/src/test/results/clientnegative/union3.q.out    |    2 +-
 .../test/results/clientnegative/uniquejoin.q.out   |    2 +-
 .../test/results/clientnegative/uniquejoin2.q.out  |    2 +-
 .../test/results/clientnegative/uniquejoin3.q.out  |    2 +-
 .../results/clientnegative/wrong_column_type.q.out |    2 +-
 .../test/results/clientpositive/auto_join25.q.out  |   16 +-
 .../test/results/clientpositive/mapjoin_hook.q.out |   12 +-
 272 files changed, 840 insertions(+), 716 deletions(-)
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
 delete mode 100644 ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java

diff --git a/src/contrib/src/test/results/clientnegative/invalid_row_sequence.q.out b/src/contrib/src/test/results/clientnegative/invalid_row_sequence.q.out
index 2c577a0..22ceca1 100644
--- a/src/contrib/src/test/results/clientnegative/invalid_row_sequence.q.out
+++ b/src/contrib/src/test/results/clientnegative/invalid_row_sequence.q.out
@@ -12,4 +12,4 @@ PREHOOK: type: CREATEFUNCTION
 POSTHOOK: query: create temporary function row_sequence as 
 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence'
 POSTHOOK: type: CREATEFUNCTION
-FAILED: Error in semantic analysis: Stateful UDF's can only be invoked in the SELECT list
+FAILED: SemanticException [Error 10084]: Stateful UDF's can only be invoked in the SELECT list
diff --git a/src/contrib/src/test/results/clientnegative/udtf_explode2.q.out b/src/contrib/src/test/results/clientnegative/udtf_explode2.q.out
index f17e897..1ba909c 100644
--- a/src/contrib/src/test/results/clientnegative/udtf_explode2.q.out
+++ b/src/contrib/src/test/results/clientnegative/udtf_explode2.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: CREATE TEMPORARY FUNCTION explode2 AS 'org.apache.hadoop.hive.co
 PREHOOK: type: CREATEFUNCTION
 POSTHOOK: query: CREATE TEMPORARY FUNCTION explode2 AS 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2'
 POSTHOOK: type: CREATEFUNCTION
-FAILED: Error in semantic analysis: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 2 aliases but got 1
+FAILED: SemanticException [Error 10083]: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 2 aliases but got 1
diff --git a/src/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java b/src/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
index de03c15..f8103bf 100644
--- a/src/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
+++ b/src/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java
@@ -496,7 +496,6 @@ public class TestJdbcDriver extends TestCase {
 
   public void testErrorMessages() throws SQLException {
     String invalidSyntaxSQLState = "42000";
-    int parseErrorCode = 10;
 
     // These tests inherently cause exceptions to be written to the test output
     // logs. This is undesirable, since you it might appear to someone looking
@@ -504,27 +503,23 @@ public class TestJdbcDriver extends TestCase {
     // sure
     // how to get around that.
     doTestErrorCase("SELECTT * FROM " + tableName,
-        "cannot recognize input near 'SELECTT' '*' 'FROM'", invalidSyntaxSQLState, 11);
+        "cannot recognize input near 'SELECTT' '*' 'FROM'",
+        invalidSyntaxSQLState, 40000);
     doTestErrorCase("SELECT * FROM some_table_that_does_not_exist",
-        "Table not found", "42000", parseErrorCode);
+        "Table not found", "42S02", 10001);
     doTestErrorCase("drop table some_table_that_does_not_exist",
-        "Table not found", "42000", parseErrorCode);
+        "Table not found", "42S02", 10001);
     doTestErrorCase("SELECT invalid_column FROM " + tableName,
-        "Invalid table alias or column reference", invalidSyntaxSQLState,
-        parseErrorCode);
+        "Invalid table alias or column reference", invalidSyntaxSQLState, 10004);
     doTestErrorCase("SELECT invalid_function(under_col) FROM " + tableName,
-        "Invalid function", invalidSyntaxSQLState, parseErrorCode);
+    "Invalid function", invalidSyntaxSQLState, 10011);
 
-    // TODO: execute errors like this currently don't return good messages (i.e.
-    // 'Table already exists'). This is because the Driver class calls
-    // Task.executeTask() which swallows meaningful exceptions and returns a
-    // status
-    // code. This should be refactored.
+    // TODO: execute errors like this currently don't return good error
+    // codes and messages. This should be fixed.
     doTestErrorCase(
         "create table " + tableName + " (key int, value string)",
-        "Query returned non-zero code: 9, cause: FAILED: Execution Error, "
-        + "return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",
-        "08S01", 9);
+        "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask",
+        "08S01", 1);
   }
 
   private void doTestErrorCase(String sql, String expectedMessage,
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index c286e90..a6ce320 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -82,7 +82,6 @@ import org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner;
 import org.apache.hadoop.hive.ql.parse.ASTNode;
 import org.apache.hadoop.hive.ql.parse.AbstractSemanticAnalyzerHook;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContext;
 import org.apache.hadoop.hive.ql.parse.HiveSemanticAnalyzerHookContextImpl;
 import org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer;
@@ -496,24 +495,17 @@ public class Driver implements CommandProcessor {
       //restore state after we're done executing a specific query
 
       return 0;
-    } catch (SemanticException e) {
-      errorMessage = "FAILED: Error in semantic analysis: " + e.getMessage();
-      SQLState = ErrorMsg.findSQLState(e.getMessage());
-      console.printError(errorMessage, "\n"
-          + org.apache.hadoop.util.StringUtils.stringifyException(e));
-      return (10);
-    } catch (ParseException e) {
-      errorMessage = "FAILED: Parse Error: " + e.getMessage();
-      SQLState = ErrorMsg.findSQLState(e.getMessage());
-      console.printError(errorMessage, "\n"
-          + org.apache.hadoop.util.StringUtils.stringifyException(e));
-      return (11);
     } catch (Exception e) {
-      errorMessage = "FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
-      SQLState = ErrorMsg.findSQLState(e.getMessage());
-      console.printError(errorMessage + "\n"
+      ErrorMsg error = ErrorMsg.getErrorMsg(e.getMessage());
+      errorMessage = "FAILED: " + e.getClass().getSimpleName();
+      if (error != ErrorMsg.GENERIC_ERROR) {
+        errorMessage += " [Error "  + error.getErrorCode()  + "]:";
+      }
+      errorMessage += " " + e.getMessage();
+      SQLState = error.getSQLState();
+      console.printError(errorMessage, "\n"
           + org.apache.hadoop.util.StringUtils.stringifyException(e));
-      return (12);
+      return error.getErrorCode();
     } finally {
       perfLogger.PerfLogEnd(LOG, PerfLogger.COMPILE);
       restoreSession(queryState);
@@ -1137,8 +1129,11 @@ public class Driver implements CommandProcessor {
           if (backupTask != null) {
             errorMessage = "FAILED: Execution Error, return code " + exitVal + " from "
                 + tsk.getClass().getName();
+            ErrorMsg em = ErrorMsg.getErrorMsg(exitVal);
+            if (em != null) {
+              errorMessage += ". " +  em.getMsg();
+            }
             console.printError(errorMessage);
-
             errorMessage = "ATTEMPT: Execute BackupTask: " + backupTask.getClass().getName();
             console.printError(errorMessage);
 
@@ -1159,9 +1154,12 @@ public class Driver implements CommandProcessor {
               perfLogger.PerfLogEnd(LOG, PerfLogger.FAILURE_HOOK + ofh.getClass().getName());
             }
 
-            // TODO: This error messaging is not very informative. Fix that.
             errorMessage = "FAILED: Execution Error, return code " + exitVal + " from "
                 + tsk.getClass().getName();
+            ErrorMsg em = ErrorMsg.getErrorMsg(exitVal);
+            if (em != null) {
+              errorMessage += ". " +  em.getMsg();
+            }
             SQLState = "08S01";
             console.printError(errorMessage);
             if (running.size() != 0) {
@@ -1170,7 +1168,7 @@ public class Driver implements CommandProcessor {
             // in case we decided to run everything in local mode, restore the
             // the jobtracker setting to its initial value
             ctx.restoreOriginalTracker();
-            return 9;
+            return exitVal;
           }
         }
 
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/src/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
new file mode 100644
index 0000000..4d770dd
--- /dev/null
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -0,0 +1,433 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.antlr.runtime.tree.Tree;
+import org.apache.hadoop.hive.ql.metadata.HiveUtils;
+import org.apache.hadoop.hive.ql.parse.ASTNode;
+import org.apache.hadoop.hive.ql.parse.ASTNodeOrigin;
+
+/**
+ * List of error messages thrown by the parser.
+ **/
+
+public enum ErrorMsg {
+  // The error codes are Hive-specific and partitioned into the following ranges:
+  // 10000 to 19999: Errors occuring during semantic analysis and compilation of the query.
+  // 20000 to 29999: Runtime errors where Hive believes that retries are unlikely to succeed.
+  // 30000 to 39999: Runtime errors which Hive thinks may be transient and retrying may succeed.
+  // 40000 to 49999: Errors where Hive is unable to advise about retries.
+  // In addition to the error code, ErrorMsg also has a SQLState field.
+  // SQLStates are taken from Section 12.5 of ISO-9075.
+  // See http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt
+  // Most will just rollup to the generic syntax error state of 42000, but
+  // specific errors can override the that state.
+  // See this page for how MySQL uses SQLState codes:
+  // http://dev.mysql.com/doc/refman/5.0/en/connector-j-reference-error-sqlstates.html
+  GENERIC_ERROR(40000, "Exception while processing"),
+
+  INVALID_TABLE(10001, "Table not found", "42S02"),
+  INVALID_COLUMN(10002, "Invalid column reference"),
+  INVALID_INDEX(10003, "Invalid index"),
+  INVALID_TABLE_OR_COLUMN(10004, "Invalid table alias or column reference"),
+  AMBIGUOUS_TABLE_OR_COLUMN(10005, "Ambiguous table alias or column reference"),
+  INVALID_PARTITION(10006, "Partition not found"),
+  AMBIGUOUS_COLUMN(10007, "Ambiguous column reference"),
+  AMBIGUOUS_TABLE_ALIAS(10008, "Ambiguous table alias"),
+  INVALID_TABLE_ALIAS(10009, "Invalid table alias"),
+  NO_TABLE_ALIAS(10010, "No table alias"),
+  INVALID_FUNCTION(10011, "Invalid function"),
+  INVALID_FUNCTION_SIGNATURE(10012, "Function argument type mismatch"),
+  INVALID_OPERATOR_SIGNATURE(10013, "Operator argument type mismatch"),
+  INVALID_ARGUMENT(10014, "Wrong arguments"),
+  INVALID_ARGUMENT_LENGTH(10015, "Arguments length mismatch", "21000"),
+  INVALID_ARGUMENT_TYPE(10016, "Argument type mismatch"),
+  INVALID_JOIN_CONDITION_1(10017, "Both left and right aliases encountered in JOIN"),
+  INVALID_JOIN_CONDITION_2(10018, "Neither left nor right aliases encountered in JOIN"),
+  INVALID_JOIN_CONDITION_3(10019, "OR not supported in JOIN currently"),
+  INVALID_TRANSFORM(10020, "TRANSFORM with other SELECT columns not supported"),
+  DUPLICATE_GROUPBY_KEY(10021, "Repeated key in GROUP BY"),
+  UNSUPPORTED_MULTIPLE_DISTINCTS(10022, "DISTINCT on different columns not supported" +
+      " with skew in data"),
+  NO_SUBQUERY_ALIAS(10023, "No alias for subquery"),
+  NO_INSERT_INSUBQUERY(10024, "Cannot insert in a subquery. Inserting to table "),
+  NON_KEY_EXPR_IN_GROUPBY(10025, "Expression not in GROUP BY key"),
+  INVALID_XPATH(10026, "General . and [] operators are not supported"),
+  INVALID_PATH(10027, "Invalid path"),
+  ILLEGAL_PATH(10028, "Path is not legal"),
+  INVALID_NUMERICAL_CONSTANT(10029, "Invalid numerical constant"),
+  INVALID_ARRAYINDEX_CONSTANT(10030, "Non-constant expressions for array indexes not supported"),
+  INVALID_MAPINDEX_CONSTANT(10031, "Non-constant expression for map indexes not supported"),
+  INVALID_MAPINDEX_TYPE(10032, "MAP key type does not match index expression type"),
+  NON_COLLECTION_TYPE(10033, "[] not valid on non-collection types"),
+  SELECT_DISTINCT_WITH_GROUPBY(10034, "SELECT DISTINCT and GROUP BY can not be in the same query"),
+  COLUMN_REPEATED_IN_PARTITIONING_COLS(10035, "Column repeated in partitioning columns"),
+  DUPLICATE_COLUMN_NAMES(10036, "Duplicate column name:"),
+  INVALID_BUCKET_NUMBER(10037, "Bucket number should be bigger than zero"),
+  COLUMN_REPEATED_IN_CLUSTER_SORT(10038, "Same column cannot appear in CLUSTER BY and SORT BY"),
+  SAMPLE_RESTRICTION(10039, "Cannot SAMPLE on more than two columns"),
+  SAMPLE_COLUMN_NOT_FOUND(10040, "SAMPLE column not found"),
+  NO_PARTITION_PREDICATE(10041, "No partition predicate found"),
+  INVALID_DOT(10042, ". Operator is only supported on struct or list of struct types"),
+  INVALID_TBL_DDL_SERDE(10043, "Either list of columns or a custom serializer should be specified"),
+  TARGET_TABLE_COLUMN_MISMATCH(10044,
+      "Cannot insert into target table because column number/types are different"),
+  TABLE_ALIAS_NOT_ALLOWED(10045, "Table alias not allowed in sampling clause"),
+  CLUSTERBY_DISTRIBUTEBY_CONFLICT(10046, "Cannot have both CLUSTER BY and DISTRIBUTE BY clauses"),
+  ORDERBY_DISTRIBUTEBY_CONFLICT(10047, "Cannot have both ORDER BY and DISTRIBUTE BY clauses"),
+  CLUSTERBY_SORTBY_CONFLICT(10048, "Cannot have both CLUSTER BY and SORT BY clauses"),
+  ORDERBY_SORTBY_CONFLICT(10049, "Cannot have both ORDER BY and SORT BY clauses"),
+  CLUSTERBY_ORDERBY_CONFLICT(10050, "Cannot have both CLUSTER BY and ORDER BY clauses"),
+  NO_LIMIT_WITH_ORDERBY(10051, "In strict mode, if ORDER BY is specified, "
+      + "LIMIT must also be specified"),
+  NO_CARTESIAN_PRODUCT(10052, "In strict mode, cartesian product is not allowed. "
+      + "If you really want to perform the operation, set hive.mapred.mode=nonstrict"),
+  UNION_NOTIN_SUBQ(10053, "Top level UNION is not supported currently; "
+      + "use a subquery for the UNION"),
+  INVALID_INPUT_FORMAT_TYPE(10054, "Input format must implement InputFormat"),
+  INVALID_OUTPUT_FORMAT_TYPE(10055, "Output Format must implement HiveOutputFormat, "
+      + "otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat"),
+  NO_VALID_PARTN(10056, "The query does not reference any valid partition. "
+      + "To run this query, set hive.mapred.mode=nonstrict"),
+  NO_OUTER_MAPJOIN(10057, "MAPJOIN cannot be performed with OUTER JOIN"),
+  INVALID_MAPJOIN_HINT(10058, "Neither table specified as map-table"),
+  INVALID_MAPJOIN_TABLE(10059, "Result of a union cannot be a map table"),
+  NON_BUCKETED_TABLE(10060, "Sampling expression needed for non-bucketed table"),
+  BUCKETED_NUMBERATOR_BIGGER_DENOMINATOR(10061, "Numberator should not be bigger than "
+      + "denaminator in sample clause for table"),
+  NEED_PARTITION_ERROR(10062, "Need to specify partition columns because the destination "
+      + "table is partitioned"),
+  CTAS_CTLT_COEXISTENCE(10063, "Create table command does not allow LIKE and AS-SELECT in "
+      + "the same command"),
+  LINES_TERMINATED_BY_NON_NEWLINE(10064, "LINES TERMINATED BY only supports "
+      + "newline '\\n' right now"),
+  CTAS_COLLST_COEXISTENCE(10065, "CREATE TABLE AS SELECT command cannot specify "
+      + "the list of columns "
+      + "for the target table"),
+  CTLT_COLLST_COEXISTENCE(10066, "CREATE TABLE LIKE command cannot specify the list of columns for "
+      + "the target table"),
+  INVALID_SELECT_SCHEMA(10067, "Cannot derive schema from the select-clause"),
+  CTAS_PARCOL_COEXISTENCE(10068, "CREATE-TABLE-AS-SELECT does not support "
+      + "partitioning in the target table "),
+  CTAS_MULTI_LOADFILE(10069, "CREATE-TABLE-AS-SELECT results in multiple file load"),
+  CTAS_EXTTBL_COEXISTENCE(10070, "CREATE-TABLE-AS-SELECT cannot create external table"),
+  INSERT_EXTERNAL_TABLE(10071, "Inserting into a external table is not allowed"),
+  DATABASE_NOT_EXISTS(10072, "Database does not exist:"),
+  TABLE_ALREADY_EXISTS(10073, "Table already exists:", "42S02"),
+  COLUMN_ALIAS_ALREADY_EXISTS(10074, "Column alias already exists:", "42S02"),
+  UDTF_MULTIPLE_EXPR(10075, "Only a single expression in the SELECT clause is "
+      + "supported with UDTF's"),
+  UDTF_REQUIRE_AS(10076, "UDTF's require an AS clause"),
+  UDTF_NO_GROUP_BY(10077, "GROUP BY is not supported with a UDTF in the SELECT clause"),
+  UDTF_NO_SORT_BY(10078, "SORT BY is not supported with a UDTF in the SELECT clause"),
+  UDTF_NO_CLUSTER_BY(10079, "CLUSTER BY is not supported with a UDTF in the SELECT clause"),
+  UDTF_NO_DISTRIBUTE_BY(10080, "DISTRUBTE BY is not supported with a UDTF in the SELECT clause"),
+  UDTF_INVALID_LOCATION(10081, "UDTF's are not supported outside the SELECT clause, nor nested "
+      + "in expressions"),
+  UDTF_LATERAL_VIEW(10082, "UDTF's cannot be in a select expression when there is a lateral view"),
+  UDTF_ALIAS_MISMATCH(10083, "The number of aliases supplied in the AS clause does not match the "
+      + "number of columns output by the UDTF"),
+  UDF_STATEFUL_INVALID_LOCATION(10084, "Stateful UDF's can only be invoked in the SELECT list"),
+  LATERAL_VIEW_WITH_JOIN(10085, "JOIN with a LATERAL VIEW is not supported"),
+  LATERAL_VIEW_INVALID_CHILD(10086, "LATERAL VIEW AST with invalid child"),
+  OUTPUT_SPECIFIED_MULTIPLE_TIMES(10087, "The same output cannot be present multiple times: "),
+  INVALID_AS(10088, "AS clause has an invalid number of aliases"),
+  VIEW_COL_MISMATCH(10089, "The number of columns produced by the SELECT clause does not match the "
+      + "number of column names specified by CREATE VIEW"),
+  DML_AGAINST_VIEW(10090, "A view cannot be used as target table for LOAD or INSERT"),
+  ANALYZE_VIEW(10091, "ANALYZE is not supported for views"),
+  VIEW_PARTITION_TOTAL(10092, "At least one non-partitioning column must be present in view"),
+  VIEW_PARTITION_MISMATCH(10093, "Rightmost columns in view output do not match "
+      + "PARTITIONED ON clause"),
+  PARTITION_DYN_STA_ORDER(10094, "Dynamic partition cannot be the parent of a static partition"),
+  DYNAMIC_PARTITION_DISABLED(10095, "Dynamic partition is disabled. Either enable it by setting "
+      + "hive.exec.dynamic.partition=true or specify partition column values"),
+  DYNAMIC_PARTITION_STRICT_MODE(10096, "Dynamic partition strict mode requires at least one "
+      + "static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict"),
+  DYNAMIC_PARTITION_MERGE(10097, "Dynamic partition does not support merging using "
+      + "non-CombineHiveInputFormat. Please check your hive.input.format setting and "
+      + "make sure your Hadoop version support CombineFileInputFormat"),
+  NONEXISTPARTCOL(10098, "Non-Partition column appears in the partition specification: "),
+  UNSUPPORTED_TYPE(10099, "DATE and DATETIME types aren't supported yet. Please use "
+      + "TIMESTAMP instead"),
+  CREATE_NON_NATIVE_AS(10100, "CREATE TABLE AS SELECT cannot be used for a non-native table"),
+  LOAD_INTO_NON_NATIVE(10101, "A non-native table cannot be used as target for LOAD"),
+  LOCKMGR_NOT_SPECIFIED(10102, "Lock manager not specified correctly, set hive.lock.manager"),
+  LOCKMGR_NOT_INITIALIZED(10103, "Lock manager could not be initialized, check hive.lock.manager "),
+  LOCK_CANNOT_BE_ACQUIRED(10104, "Locks on the underlying objects cannot be acquired. "
+      + "retry after some time"),
+  ZOOKEEPER_CLIENT_COULD_NOT_BE_INITIALIZED(10105, "Check hive.zookeeper.quorum "
+      + "and hive.zookeeper.client.port"),
+  OVERWRITE_ARCHIVED_PART(10106, "Cannot overwrite an archived partition. " +
+      "Unarchive before running this command"),
+  ARCHIVE_METHODS_DISABLED(10107, "Archiving methods are currently disabled. " +
+      "Please see the Hive wiki for more information about enabling archiving"),
+  ARCHIVE_ON_MULI_PARTS(10108, "ARCHIVE can only be run on a single partition"),
+  UNARCHIVE_ON_MULI_PARTS(10109, "ARCHIVE can only be run on a single partition"),
+  ARCHIVE_ON_TABLE(10110, "ARCHIVE can only be run on partitions"),
+  RESERVED_PART_VAL(10111, "Partition value contains a reserved substring"),
+  HOLD_DDLTIME_ON_NONEXIST_PARTITIONS(10112, "HOLD_DDLTIME hint cannot be applied to dynamic " +
+                                      "partitions or non-existent partitions"),
+  OFFLINE_TABLE_OR_PARTITION(10113, "Query against an offline table or partition"),
+  OUTERJOIN_USES_FILTERS(10114, "The query results could be wrong. " +
+                         "Turn on hive.outerjoin.supports.filters"),
+  NEED_PARTITION_SPECIFICATION(10115, "Table is partitioned and partition specification is needed"),
+  INVALID_METADATA(10116, "The metadata file could not be parsed "),
+  NEED_TABLE_SPECIFICATION(10117, "Table name could be determined; It should be specified "),
+  PARTITION_EXISTS(10118, "Partition already exists"),
+  TABLE_DATA_EXISTS(10119, "Table exists and contains data files"),
+  INCOMPATIBLE_SCHEMA(10120, "The existing table is not compatible with the import spec. "),
+  EXIM_FOR_NON_NATIVE(10121, "Export/Import cannot be done for a non-native table. "),
+  INSERT_INTO_BUCKETIZED_TABLE(10122, "Bucketized tables do not support INSERT INTO:"),
+  NO_COMPARE_BIGINT_STRING(10123, "In strict mode, comparing bigints and strings is not allowed, "
+      + "it may result in a loss of precision. "
+      + "If you really want to perform the operation, set hive.mapred.mode=nonstrict"),
+  NO_COMPARE_BIGINT_DOUBLE(10124, "In strict mode, comparing bigints and doubles is not allowed, "
+      + "it may result in a loss of precision. "
+      + "If you really want to perform the operation, set hive.mapred.mode=nonstrict"),
+  PARTSPEC_DIFFER_FROM_SCHEMA(10125, "Partition columns in partition specification are "
+      + "not the same as that defined in the table schema. "
+      + "The names and orders have to be exactly the same."),
+  PARTITION_COLUMN_NON_PRIMITIVE(10126, "Partition column must be of primitive type."),
+  INSERT_INTO_DYNAMICPARTITION_IFNOTEXISTS(10127,
+      "Dynamic partitions do not support IF NOT EXISTS. Specified partitions with value :"),
+  UDAF_INVALID_LOCATION(10128, "Not yet supported place for UDAF"),
+
+  SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
+  SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "
+      + "It may have crashed with an error."),
+  SCRIPT_GENERIC_ERROR(20002, "Hive encountered some unknown error while "
+      + "running your custom script."),
+  SCRIPT_CLOSING_ERROR(20003, "An error occurred when trying to close the Operator " +
+      "running your custom script.")
+    ;
+
+  private int errorCode;
+  private String mesg;
+  private String sqlState;
+
+  private static final char SPACE = ' ';
+  private static final Pattern ERROR_MESSAGE_PATTERN = Pattern.compile(".*Line [0-9]+:[0-9]+ (.*)");
+  private static final Pattern ERROR_CODE_PATTERN =
+    Pattern.compile("HiveException:\\s+\\[Error ([0-9]+)\\]: (.*)");
+  private static Map<String, ErrorMsg> mesgToErrorMsgMap = new HashMap<String, ErrorMsg>();
+  private static int minMesgLength = -1;
+
+  static {
+    for (ErrorMsg errorMsg : values()) {
+      mesgToErrorMsgMap.put(errorMsg.getMsg().trim(), errorMsg);
+
+      int length = errorMsg.getMsg().trim().length();
+      if (minMesgLength == -1 || length < minMesgLength) {
+        minMesgLength = length;
+      }
+    }
+  }
+
+  /**
+   * Given an error message string, returns the ErrorMsg object associated with it.
+   * @param mesg An error message string
+   * @return ErrorMsg
+   */
+  public static ErrorMsg getErrorMsg(String mesg) {
+    if (mesg == null) {
+      return GENERIC_ERROR;
+    }
+
+    // first see if there is a direct match
+    ErrorMsg errorMsg = mesgToErrorMsgMap.get(mesg);
+    if (errorMsg != null) {
+      return errorMsg;
+    }
+
+    // if not see if the mesg follows type of format, which is typically the
+    // case:
+    // line 1:14 Table not found table_name
+    String truncatedMesg = mesg.trim();
+    Matcher match = ERROR_MESSAGE_PATTERN.matcher(mesg);
+    if (match.matches()) {
+      truncatedMesg = match.group(1);
+    }
+
+    // appends might exist after the root message, so strip tokens off until we
+    // match
+    while (truncatedMesg.length() > minMesgLength) {
+      errorMsg = mesgToErrorMsgMap.get(truncatedMesg.trim());
+      if (errorMsg != null) {
+        return errorMsg;
+      }
+
+      int lastSpace = truncatedMesg.lastIndexOf(SPACE);
+      if (lastSpace == -1) {
+        break;
+      }
+
+      // hack off the last word and try again
+      truncatedMesg = truncatedMesg.substring(0, lastSpace).trim();
+    }
+
+    return GENERIC_ERROR;
+  }
+
+  /**
+   * Given an error code, returns the ErrorMsg object associated with it.
+   * @param errorCode An error code
+   * @return ErrorMsg
+   */
+  public static ErrorMsg getErrorMsg(int errorCode) {
+    for (ErrorMsg errorMsg : values()) {
+      if (errorMsg.getErrorCode() == errorCode) {
+        return errorMsg;
+      }
+    }
+    return null;
+  }
+
+  /**
+   * For a given error message string, searches for a <code>ErrorMsg</code> enum
+   * that appears to be a match. If an match is found, returns the
+   * <code>SQLState</code> associated with the <code>ErrorMsg</code>. If a match
+   * is not found or <code>ErrorMsg</code> has no <code>SQLState</code>, returns
+   * the <code>SQLState</code> bound to the <code>GENERIC_ERROR</code>
+   * <code>ErrorMsg</code>.
+   *
+   * @param mesg
+   *          An error message string
+   * @return SQLState
+   */
+  public static String findSQLState(String mesg) {
+    ErrorMsg error = getErrorMsg(mesg);
+    return error.getSQLState();
+  }
+
+  private ErrorMsg(int errorCode, String mesg) {
+    // 42000 is the generic SQLState for syntax error.
+    this(errorCode, mesg, "42000");
+  }
+
+  private ErrorMsg(int errorCode, String mesg, String sqlState) {
+    this.errorCode = errorCode;
+    this.mesg = mesg;
+    this.sqlState = sqlState;
+  }
+
+  private static int getLine(ASTNode tree) {
+    if (tree.getChildCount() == 0) {
+      return tree.getToken().getLine();
+    }
+
+    return getLine((ASTNode) tree.getChild(0));
+  }
+
+  private static int getCharPositionInLine(ASTNode tree) {
+    if (tree.getChildCount() == 0) {
+      return tree.getToken().getCharPositionInLine();
+    }
+
+    return getCharPositionInLine((ASTNode) tree.getChild(0));
+  }
+
+  // Dirty hack as this will throw away spaces and other things - find a better
+  // way!
+  public static String getText(ASTNode tree) {
+    if (tree.getChildCount() == 0) {
+      return tree.getText();
+    }
+    return getText((ASTNode) tree.getChild(tree.getChildCount() - 1));
+  }
+
+  public String getMsg(ASTNode tree) {
+    StringBuilder sb = new StringBuilder();
+    renderPosition(sb, tree);
+    sb.append(" ");
+    sb.append(mesg);
+    sb.append(" '");
+    sb.append(getText(tree));
+    sb.append("'");
+    renderOrigin(sb, tree.getOrigin());
+    return sb.toString();
+  }
+
+  public static void renderOrigin(StringBuilder sb, ASTNodeOrigin origin) {
+    while (origin != null) {
+      sb.append(" in definition of ");
+      sb.append(origin.getObjectType());
+      sb.append(" ");
+      sb.append(origin.getObjectName());
+      sb.append(" [");
+      sb.append(HiveUtils.LINE_SEP);
+      sb.append(origin.getObjectDefinition());
+      sb.append(HiveUtils.LINE_SEP);
+      sb.append("] used as ");
+      sb.append(origin.getUsageAlias());
+      sb.append(" at ");
+      ASTNode usageNode = origin.getUsageNode();
+      renderPosition(sb, usageNode);
+      origin = usageNode.getOrigin();
+    }
+  }
+
+  private static void renderPosition(StringBuilder sb, ASTNode tree) {
+    sb.append("Line ");
+    sb.append(getLine(tree));
+    sb.append(":");
+    sb.append(getCharPositionInLine(tree));
+  }
+
+  public String getMsg(Tree tree) {
+    return getMsg((ASTNode) tree);
+  }
+
+  public String getMsg(ASTNode tree, String reason) {
+    return getMsg(tree) + ": " + reason;
+  }
+
+  public String getMsg(Tree tree, String reason) {
+    return getMsg((ASTNode) tree, reason);
+  }
+
+  public String getMsg(String reason) {
+    return mesg + " " + reason;
+  }
+
+  public String getErrorCodedMsg() {
+    return "[Error " + errorCode + "]: " + mesg;
+  }
+
+  public static Pattern getErrorCodePattern() {
+    return ERROR_CODE_PATTERN;
+  }
+
+  public String getMsg() {
+    return mesg;
+  }
+
+  public String getSQLState() {
+    return sqlState;
+  }
+
+  public int getErrorCode() {
+    return errorCode;
+  }
+}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
index ff0e8ac..3ba72b9 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
@@ -694,7 +694,7 @@ public class ExecDriver extends Task<MapredWork> implements Serializable, Hadoop
     }
 
     if (ret != 0) {
-      System.exit(2);
+      System.exit(ret);
     }
   }
 
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
index 082d23c..a4af44c 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/HadoopJobExecHelper.java
@@ -715,6 +715,10 @@ public class HadoopJobExecHelper {
           Thread t = new Thread(jd);
           t.start();
           t.join(HiveConf.getIntVar(job, HiveConf.ConfVars.JOB_DEBUG_TIMEOUT));
+          int ec = jd.getErrorCode();
+          if (ec > 0) {
+            returnVal = ec;
+          }
         } catch (InterruptedException e) {
           console.printError("Timed out trying to grab more detailed job failure"
               + " information, please check jobtracker for more info");
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/JobDebugger.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/JobDebugger.java
index 8fe76fa..b95ff0e 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/JobDebugger.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/JobDebugger.java
@@ -27,8 +27,11 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.errors.ErrorAndSolution;
 import org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
@@ -50,21 +53,34 @@ public class JobDebugger implements Runnable {
   private final Map<String, Integer> failures = new HashMap<String, Integer>();
   private final Set<String> successes = new HashSet<String>(); // Successful task ID's
   private final Map<String, TaskInfo> taskIdToInfo = new HashMap<String, TaskInfo>();
+  private int maxFailures = 0;
 
   // Used for showJobFailDebugInfo
   private static class TaskInfo {
     String jobId;
     Set<String> logUrls;
+    int errorCode;  // Obtained from the HiveException thrown
+    String[] diagnosticMesgs;
 
     public TaskInfo(String jobId) {
       this.jobId = jobId;
       logUrls = new HashSet<String>();
+      errorCode = 0;
+      diagnosticMesgs = null;
     }
 
     public void addLogUrl(String logUrl) {
       logUrls.add(logUrl);
     }
 
+    public void setErrorCode(int errorCode) {
+      this.errorCode = errorCode;
+    }
+
+    public void setDiagnosticMesgs(String[] diagnosticMesgs) {
+      this.diagnosticMesgs = diagnosticMesgs;
+    }
+
     public Set<String> getLogUrls() {
       return logUrls;
     }
@@ -72,6 +88,14 @@ public class JobDebugger implements Runnable {
     public String getJobId() {
       return jobId;
     }
+
+    public int getErrorCode() {
+      return errorCode;
+    }
+
+    public String[] getDiagnosticMesgs() {
+      return diagnosticMesgs;
+    }
   }
 
   public JobDebugger(JobConf conf, RunningJob rj, LogHelper console) {
@@ -97,17 +121,31 @@ public class JobDebugger implements Runnable {
     }
   }
 
-  class TaskLogGrabber implements Runnable {
+  public static int extractErrorCode(String[] diagnostics) {
+    int result = 0;
+    Pattern errorCodeRegex = ErrorMsg.getErrorCodePattern();
+    for (String mesg : diagnostics) {
+      Matcher matcher = errorCodeRegex.matcher(mesg);
+      if (matcher.find()) {
+        result = Integer.parseInt(matcher.group(1));
+        // We don't exit the loop early because we want to extract the error code
+        // corresponding to the bottommost error coded exception.
+      }
+    }
+    return result;
+  }
+
+  class TaskInfoGrabber implements Runnable {
 
     public void run() {
       try {
-        getTaskLogs();
-      } catch (Exception e) {
+        getTaskInfos();
+      } catch (IOException e) {
         console.printError(e.getMessage());
       }
     }
 
-    private void getTaskLogs() throws IOException, MalformedURLException {
+    private void getTaskInfos() throws IOException, MalformedURLException {
       int startIndex = 0;
       while (true) {
         TaskCompletionEvent[] taskCompletions = rj.getTaskCompletionEvents(startIndex);
@@ -151,11 +189,16 @@ public class JobDebugger implements Runnable {
             ti.getLogUrls().add(taskAttemptLogUrl);
           }
 
-          // If a task failed, then keep track of the total number of failures
-          // for that task (typically, a task gets re-run up to 4 times if it
-          // fails
-
+          // If a task failed, fetch its error code (if available).
+          // Also keep track of the total number of failures for that
+          // task (typically, a task gets re-run up to 4 times if it fails.
           if (t.getTaskStatus() != TaskCompletionEvent.Status.SUCCEEDED) {
+            if (ti.getErrorCode() == 0) {
+              String[] diags = rj.getTaskDiagnostics(t.getTaskAttemptId());
+              ti.setErrorCode(extractErrorCode(diags));
+              ti.setDiagnosticMesgs(diags);
+            }
+
             Integer failAttempts = failures.get(taskId);
             if (failAttempts == null) {
               failAttempts = Integer.valueOf(0);
@@ -174,14 +217,21 @@ public class JobDebugger implements Runnable {
     }
   }
 
+  private void computeMaxFailures() {
+    maxFailures = 0;
+    for (Integer failCount : failures.values()) {
+      if (maxFailures < failCount.intValue()) {
+        maxFailures = failCount.intValue();
+      }
+    }
+  }
+
   @SuppressWarnings("deprecation")
   private void showJobFailDebugInfo() throws IOException {
-
-
     console.printError("Error during job, obtaining debugging information...");
     // Loop to get all task completion events because getTaskCompletionEvents
     // only returns a subset per call
-    TaskLogGrabber tlg = new TaskLogGrabber();
+    TaskInfoGrabber tlg = new TaskInfoGrabber();
     Thread t = new Thread(tlg);
     try {
       t.start();
@@ -199,23 +249,24 @@ public class JobDebugger implements Runnable {
     if (failures.keySet().size() == 0) {
       return;
     }
-
     // Find the highest failure count
-    int maxFailures = 0;
-    for (Integer failCount : failures.values()) {
-      if (maxFailures < failCount.intValue()) {
-        maxFailures = failCount.intValue();
-      }
-    }
+    computeMaxFailures() ;
 
     // Display Error Message for tasks with the highest failure count
-    String jtUrl = JobTrackerURLResolver.getURL(conf);
+    String jtUrl = null;
+    try {
+      jtUrl = JobTrackerURLResolver.getURL(conf);
+    } catch (Exception e) {
+      console.printError("Unable to retrieve URL for Hadoop Task logs. "
+          + e.getMessage());
+    }
 
     for (String task : failures.keySet()) {
       if (failures.get(task).intValue() == maxFailures) {
         TaskInfo ti = taskIdToInfo.get(task);
         String jobId = ti.getJobId();
-        String taskUrl = jtUrl + "/taskdetails.jsp?jobid=" + jobId + "&tipid=" + task.toString();
+        String taskUrl = (jtUrl == null) ? "Unavailable" :
+            jtUrl + "/taskdetails.jsp?jobid=" + jobId + "&tipid=" + task.toString();
 
         TaskLogProcessor tlp = new TaskLogProcessor(conf);
         for (String logUrl : ti.getLogUrls()) {
@@ -251,6 +302,11 @@ public class JobDebugger implements Runnable {
           }
           sb.append("-----\n");
 
+          sb.append("Diagnostic Messages for this Task:\n");
+          String[] diagMesgs = ti.getDiagnosticMesgs();
+          for (String mesg : diagMesgs) {
+            sb.append(mesg + "\n");
+          }
           console.printError(sb.toString());
         }
 
@@ -259,6 +315,16 @@ public class JobDebugger implements Runnable {
       }
     }
     return;
+  }
 
+  public int getErrorCode() {
+    for (String task : failures.keySet()) {
+      if (failures.get(task).intValue() == maxFailures) {
+        TaskInfo ti = taskIdToInfo.get(task);
+        return ti.getErrorCode();
+      }
+    }
+    // Should never reach here unless there were no failed tasks.
+    return 0;
   }
 }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
index aa5d0bf..8f56082 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java
@@ -34,6 +34,7 @@ import java.util.TimerTask;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ScriptDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
@@ -214,7 +215,7 @@ public class ScriptOperator extends Operator<ScriptDesc> implements
       // initialize all children before starting the script
       initializeChildren(hconf);
     } catch (Exception e) {
-      throw new HiveException("Cannot initialize ScriptOperator", e);
+      throw new HiveException(ErrorMsg.SCRIPT_INIT_ERROR.getErrorCodedMsg(), e);
     }
   }
 
@@ -317,12 +318,12 @@ public class ScriptOperator extends Operator<ScriptDesc> implements
         outThread.start();
         errThread.start();
       } catch (Exception e) {
-        throw new HiveException("Cannot initialize ScriptOperator", e);
+        throw new HiveException(ErrorMsg.SCRIPT_INIT_ERROR.getErrorCodedMsg(), e);
       }
     }
 
     if (scriptError != null) {
-      throw new HiveException(scriptError);
+      throw new HiveException(ErrorMsg.SCRIPT_GENERIC_ERROR.getErrorCodedMsg(), scriptError);
     }
 
     try {
@@ -345,7 +346,7 @@ public class ScriptOperator extends Operator<ScriptDesc> implements
           displayBrokenPipeInfo();
         }
         scriptError = e;
-        throw new HiveException(e);
+        throw new HiveException(ErrorMsg.SCRIPT_IO_ERROR.getErrorCodedMsg(), e);
       }
     }
   }
@@ -356,7 +357,7 @@ public class ScriptOperator extends Operator<ScriptDesc> implements
     boolean new_abort = abort;
     if (!abort) {
       if (scriptError != null) {
-        throw new HiveException(scriptError);
+        throw new HiveException(ErrorMsg.SCRIPT_GENERIC_ERROR.getErrorCodedMsg(), scriptError);
       }
       // everything ok. try normal shutdown
       try {
@@ -449,7 +450,7 @@ public class ScriptOperator extends Operator<ScriptDesc> implements
     super.close(new_abort);
 
     if (new_abort && !abort) {
-      throw new HiveException("Hit error while closing ..");
+      throw new HiveException(ErrorMsg.SCRIPT_CLOSING_ERROR.getErrorCodedMsg());
     }
   }
 
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 823d995..e15e265 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -92,6 +92,7 @@ import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryPlan;
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;
 import org.apache.hadoop.hive.ql.io.ContentSummaryInputFormat;
@@ -105,7 +106,6 @@ import org.apache.hadoop.hive.ql.io.ReworkMapredInputFormat;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx;
 import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java b/src/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
index 1812e25..867505d 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
@@ -39,7 +39,7 @@ import java.util.regex.Pattern;
 import java.util.regex.Matcher;
 import org.apache.zookeeper.KeeperException;
 
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManager;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLockManagerCtx;
 import org.apache.hadoop.hive.ql.lockmgr.HiveLock;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
index 0b05b3f..8d8d8de 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java
@@ -30,6 +30,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.ConditionalTask;
@@ -50,7 +51,6 @@ import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRMapJoinCtx;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.RowResolver;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java
index eca42f7..4376d1b 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java
@@ -26,6 +26,7 @@ import java.util.Stack;
 
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.OperatorFactory;
@@ -39,7 +40,6 @@ import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRMapJoinCtx;
 import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;
 import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.FileSinkDesc;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
index b51c6fd..47a29f9 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
@@ -32,6 +32,7 @@ import java.util.Stack;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.GroupByOperator;
@@ -53,7 +54,6 @@ import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
 import org.apache.hadoop.hive.ql.lib.Rule;
 import org.apache.hadoop.hive.ql.lib.RuleRegExp;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.GenMapRedWalker;
 import org.apache.hadoop.hive.ql.parse.OpParseContext;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
index 6503015..98b8600 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -50,7 +51,6 @@ import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.optimizer.Transform;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 22e13e9..6a60d38 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -37,6 +37,7 @@ import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryProperties;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
 import org.apache.hadoop.hive.ql.exec.Task;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index afa754b..42e83d1 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -50,6 +50,7 @@ import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
 import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ArchiveUtils;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
 import org.apache.hadoop.hive.ql.exec.Task;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
deleted file mode 100644
index 9815b57..0000000
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
+++ /dev/null
@@ -1,375 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.parse;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.antlr.runtime.tree.Tree;
-import org.apache.hadoop.hive.ql.metadata.HiveUtils;
-
-/**
- * List of error messages thrown by the parser.
- **/
-
-public enum ErrorMsg {
-  // SQLStates are taken from Section 12.5 of ISO-9075.
-  // See http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt
-  // Most will just rollup to the generic syntax error state of 42000, but
-  // specific errors can override the that state.
-  // See this page for how MySQL uses SQLState codes:
-  // http://dev.mysql.com/doc/refman/5.0/en/connector-j-reference-error-sqlstates.html
-
-  GENERIC_ERROR("Exception while processing"),
-  INVALID_TABLE("Table not found", "42S02"),
-  INVALID_COLUMN("Invalid column reference"),
-  INVALID_INDEX("Invalid index"),
-  INVALID_TABLE_OR_COLUMN("Invalid table alias or column reference"),
-  AMBIGUOUS_TABLE_OR_COLUMN("Ambiguous table alias or column reference"),
-  INVALID_PARTITION("Partition not found"),
-  AMBIGUOUS_COLUMN("Ambiguous column reference"),
-  AMBIGUOUS_TABLE_ALIAS("Ambiguous table alias"),
-  INVALID_TABLE_ALIAS("Invalid table alias"),
-  NO_TABLE_ALIAS("No table alias"),
-  INVALID_FUNCTION("Invalid function"),
-  INVALID_FUNCTION_SIGNATURE("Function argument type mismatch"),
-  INVALID_OPERATOR_SIGNATURE("Operator argument type mismatch"),
-  INVALID_ARGUMENT("Wrong arguments"),
-  INVALID_ARGUMENT_LENGTH("Arguments length mismatch", "21000"),
-  INVALID_ARGUMENT_TYPE("Argument type mismatch"),
-  INVALID_JOIN_CONDITION_1("Both left and right aliases encountered in JOIN"),
-  INVALID_JOIN_CONDITION_2("Neither left nor right aliases encountered in JOIN"),
-  INVALID_JOIN_CONDITION_3("OR not supported in JOIN currently"),
-  INVALID_TRANSFORM("TRANSFORM with other SELECT columns not supported"),
-  DUPLICATE_GROUPBY_KEY("Repeated key in GROUP BY"),
-  UNSUPPORTED_MULTIPLE_DISTINCTS("DISTINCT on different columns not supported with skew in data"),
-  NO_SUBQUERY_ALIAS("No alias for subquery"),
-  NO_INSERT_INSUBQUERY("Cannot insert in a subquery. Inserting to table "),
-  NON_KEY_EXPR_IN_GROUPBY("Expression not in GROUP BY key"),
-  INVALID_XPATH("General . and [] operators are not supported"),
-  INVALID_PATH("Invalid path"), ILLEGAL_PATH("Path is not legal"),
-  INVALID_NUMERICAL_CONSTANT("Invalid numerical constant"),
-  INVALID_ARRAYINDEX_CONSTANT("Non-constant expressions for array indexes not supported"),
-  INVALID_MAPINDEX_CONSTANT("Non-constant expression for map indexes not supported"),
-  INVALID_MAPINDEX_TYPE("MAP key type does not match index expression type"),
-  NON_COLLECTION_TYPE("[] not valid on non-collection types"),
-  SELECT_DISTINCT_WITH_GROUPBY("SELECT DISTINCT and GROUP BY can not be in the same query"),
-  COLUMN_REPEATED_IN_PARTITIONING_COLS("Column repeated in partitioning columns"),
-  DUPLICATE_COLUMN_NAMES("Duplicate column name:"),
-  INVALID_BUCKET_NUMBER("Bucket number should be bigger than zero"),
-  COLUMN_REPEATED_IN_CLUSTER_SORT("Same column cannot appear in CLUSTER BY and SORT BY"),
-  SAMPLE_RESTRICTION("Cannot SAMPLE on more than two columns"),
-  SAMPLE_COLUMN_NOT_FOUND("SAMPLE column not found"),
-  NO_PARTITION_PREDICATE("No partition predicate found"),
-  INVALID_DOT(". Operator is only supported on struct or list of struct types"),
-  INVALID_TBL_DDL_SERDE("Either list of columns or a custom serializer should be specified"),
-  TARGET_TABLE_COLUMN_MISMATCH(
-      "Cannot insert into target table because column number/types are different"),
-  TABLE_ALIAS_NOT_ALLOWED("Table alias not allowed in sampling clause"),
-  CLUSTERBY_DISTRIBUTEBY_CONFLICT("Cannot have both CLUSTER BY and DISTRIBUTE BY clauses"),
-  ORDERBY_DISTRIBUTEBY_CONFLICT("Cannot have both ORDER BY and DISTRIBUTE BY clauses"),
-  CLUSTERBY_SORTBY_CONFLICT("Cannot have both CLUSTER BY and SORT BY clauses"),
-  ORDERBY_SORTBY_CONFLICT("Cannot have both ORDER BY and SORT BY clauses"),
-  CLUSTERBY_ORDERBY_CONFLICT("Cannot have both CLUSTER BY and ORDER BY clauses"),
-  NO_LIMIT_WITH_ORDERBY("In strict mode, if ORDER BY is specified, LIMIT must also be specified"),
-  NO_CARTESIAN_PRODUCT("In strict mode, cartesian product is not allowed. "
-      + "If you really want to perform the operation, set hive.mapred.mode=nonstrict"),
-  UNION_NOTIN_SUBQ("Top level UNION is not supported currently; use a subquery for the UNION"),
-  INVALID_INPUT_FORMAT_TYPE("Input format must implement InputFormat"),
-  INVALID_OUTPUT_FORMAT_TYPE("Output Format must implement HiveOutputFormat, "
-      + "otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat"),
-  NO_VALID_PARTN("The query does not reference any valid partition. "
-      + "To run this query, set hive.mapred.mode=nonstrict"),
-  NO_OUTER_MAPJOIN("MAPJOIN cannot be performed with OUTER JOIN"),
-  INVALID_MAPJOIN_HINT("Neither table specified as map-table"),
-  INVALID_MAPJOIN_TABLE("Result of a union cannot be a map table"),
-  NON_BUCKETED_TABLE("Sampling expression needed for non-bucketed table"),
-  BUCKETED_NUMBERATOR_BIGGER_DENOMINATOR("Numberator should not be bigger than "
-      + "denaminator in sample clause for table"),
-  NEED_PARTITION_ERROR("Need to specify partition columns because the destination "
-      + "table is partitioned"),
-  CTAS_CTLT_COEXISTENCE("Create table command does not allow LIKE and AS-SELECT in "
-      + "the same command"),
-  LINES_TERMINATED_BY_NON_NEWLINE("LINES TERMINATED BY only supports newline '\\n' right now"),
-  CTAS_COLLST_COEXISTENCE("CREATE TABLE AS SELECT command cannot specify the list of columns "
-      + "for the target table"),
-  CTLT_COLLST_COEXISTENCE("CREATE TABLE LIKE command cannot specify the list of columns for "
-      + "the target table"),
-  INVALID_SELECT_SCHEMA("Cannot derive schema from the select-clause"),
-  CTAS_PARCOL_COEXISTENCE("CREATE-TABLE-AS-SELECT does not support partitioning in the target "
-      + "table"),
-  CTAS_MULTI_LOADFILE("CREATE-TABLE-AS-SELECT results in multiple file load"),
-  CTAS_EXTTBL_COEXISTENCE("CREATE-TABLE-AS-SELECT cannot create external table"),
-  INSERT_EXTERNAL_TABLE("Inserting into a external table is not allowed"),
-  DATABASE_NOT_EXISTS("Database does not exist:"),
-  TABLE_ALREADY_EXISTS("Table already exists:", "42S02"),
-  COLUMN_ALIAS_ALREADY_EXISTS("Column alias already exists:", "42S02"),
-  UDTF_MULTIPLE_EXPR("Only a single expression in the SELECT clause is supported with UDTF's"),
-  UDTF_REQUIRE_AS("UDTF's require an AS clause"),
-  UDTF_NO_GROUP_BY("GROUP BY is not supported with a UDTF in the SELECT clause"),
-  UDTF_NO_SORT_BY("SORT BY is not supported with a UDTF in the SELECT clause"),
-  UDTF_NO_CLUSTER_BY("CLUSTER BY is not supported with a UDTF in the SELECT clause"),
-  UDTF_NO_DISTRIBUTE_BY("DISTRUBTE BY is not supported with a UDTF in the SELECT clause"),
-  UDTF_INVALID_LOCATION("UDTF's are not supported outside the SELECT clause, nor nested "
-      + "in expressions"),
-  UDAF_INVALID_LOCATION("Not yet supported place for UDAF"),
-  UDTF_LATERAL_VIEW("UDTF's cannot be in a select expression when there is a lateral view"),
-  UDTF_ALIAS_MISMATCH("The number of aliases supplied in the AS clause does not match the "
-      + "number of columns output by the UDTF"),
-  UDF_STATEFUL_INVALID_LOCATION("Stateful UDF's can only be invoked in the SELECT list"),
-  LATERAL_VIEW_WITH_JOIN("JOIN with a LATERAL VIEW is not supported"),
-  LATERAL_VIEW_INVALID_CHILD("LATERAL VIEW AST with invalid child"),
-  OUTPUT_SPECIFIED_MULTIPLE_TIMES("The same output cannot be present multiple times: "),
-  INVALID_AS("AS clause has an invalid number of aliases"),
-  VIEW_COL_MISMATCH("The number of columns produced by the SELECT clause does not match the "
-      + "number of column names specified by CREATE VIEW"),
-  DML_AGAINST_VIEW("A view cannot be used as target table for LOAD or INSERT"),
-  ANALYZE_VIEW("ANALYZE is not supported for views"),
-  VIEW_PARTITION_TOTAL("At least one non-partitioning column must be present in view"),
-  VIEW_PARTITION_MISMATCH("Rightmost columns in view output do not match PARTITIONED ON clause"),
-  PARTITION_DYN_STA_ORDER("Dynamic partition cannot be the parent of a static partition"),
-  DYNAMIC_PARTITION_DISABLED("Dynamic partition is disabled. Either enable it by setting "
-      + "hive.exec.dynamic.partition=true or specify partition column values"),
-  DYNAMIC_PARTITION_STRICT_MODE("Dynamic partition strict mode requires at least one "
-      + "static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict"),
-  DYNAMIC_PARTITION_MERGE("Dynamic partition does not support merging using non-CombineHiveInputFormat"
-      + "Please check your hive.input.format setting and make sure your Hadoop version support "
-      + "CombineFileInputFormat"),
-  NONEXISTPARTCOL("Non-Partition column appears in the partition specification: "),
-  UNSUPPORTED_TYPE("DATE and DATETIME types aren't supported yet. Please use "
-      + "TIMESTAMP instead"),
-  CREATE_NON_NATIVE_AS("CREATE TABLE AS SELECT cannot be used for a non-native table"),
-  LOAD_INTO_NON_NATIVE("A non-native table cannot be used as target for LOAD"),
-  LOCKMGR_NOT_SPECIFIED("Lock manager not specified correctly, set hive.lock.manager"),
-  LOCKMGR_NOT_INITIALIZED("Lock manager could not be initialized, check hive.lock.manager "),
-  LOCK_CANNOT_BE_ACQUIRED("Locks on the underlying objects cannot be acquired. retry after some time"),
-  ZOOKEEPER_CLIENT_COULD_NOT_BE_INITIALIZED("Check hive.zookeeper.quorum and hive.zookeeper.client.port"),
-  OVERWRITE_ARCHIVED_PART("Cannot overwrite an archived partition. " +
-      "Unarchive before running this command"),
-  ARCHIVE_METHODS_DISABLED("Archiving methods are currently disabled. " +
-      "Please see the Hive wiki for more information about enabling archiving"),
-  ARCHIVE_ON_MULI_PARTS("ARCHIVE can only be run on a single partition"),
-  UNARCHIVE_ON_MULI_PARTS("ARCHIVE can only be run on a single partition"),
-  ARCHIVE_ON_TABLE("ARCHIVE can only be run on partitions"),
-  RESERVED_PART_VAL("Partition value contains a reserved substring"),
-  HOLD_DDLTIME_ON_NONEXIST_PARTITIONS("HOLD_DDLTIME hint cannot be applied to dynamic " +
-                                      "partitions or non-existent partitions"),
-  OFFLINE_TABLE_OR_PARTITION("Query against an offline table or partition"),
-  OUTERJOIN_USES_FILTERS("The query results could be wrong. " +
-                         "Turn on hive.outerjoin.supports.filters"),
-  NEED_PARTITION_SPECIFICATION("Table is partitioned and partition specification is needed"),
-  INVALID_METADATA("The metadata file could not be parsed "),
-  NEED_TABLE_SPECIFICATION("Table name could be determined; It should be specified "),
-  PARTITION_EXISTS("Partition already exists"),
-  TABLE_DATA_EXISTS("Table exists and contains data files"),
-  INCOMPATIBLE_SCHEMA("The existing table is not compatible with the import spec. "),
-  EXIM_FOR_NON_NATIVE("Export/Import cannot be done for a non-native table. "),
-  INSERT_INTO_BUCKETIZED_TABLE("Bucketized tables do not support INSERT INTO:"),
-  NO_COMPARE_BIGINT_STRING("In strict mode, comparing bigints and strings is not allowed, "
-      + "it may result in a loss of precision. "
-      + "If you really want to perform the operation, set hive.mapred.mode=nonstrict"),
-  NO_COMPARE_BIGINT_DOUBLE("In strict mode, comparing bigints and doubles is not allowed, "
-      + "it may result in a loss of precision. "
-      + "If you really want to perform the operation, set hive.mapred.mode=nonstrict"),
-  PARTSPEC_DIFFER_FROM_SCHEMA("Partition columns in partition specification are not the same as "
-      + "that defined in the table schema. The names and orders have to be exactly the same."),
-  PARTITION_COLUMN_NON_PRIMITIVE("Partition column must be of primitive type."),
-  INSERT_INTO_DYNAMICPARTITION_IFNOTEXISTS(
-      "Dynamic partitions do not support IF NOT EXISTS. Specified partitions with value :"),
-      ;
-
-  private String mesg;
-  private String sqlState;
-
-  private static final char SPACE = ' ';
-  private static final Pattern ERROR_MESSAGE_PATTERN = Pattern.compile(".*line [0-9]+:[0-9]+ (.*)");
-  private static Map<String, ErrorMsg> mesgToErrorMsgMap = new HashMap<String, ErrorMsg>();
-  private static int minMesgLength = -1;
-
-  static {
-    for (ErrorMsg errorMsg : values()) {
-      mesgToErrorMsgMap.put(errorMsg.getMsg().trim(), errorMsg);
-
-      int length = errorMsg.getMsg().trim().length();
-      if (minMesgLength == -1 || length < minMesgLength) {
-        minMesgLength = length;
-      }
-    }
-  }
-
-  /**
-   * For a given error message string, searches for a <code>ErrorMsg</code> enum
-   * that appears to be a match. If an match is found, returns the
-   * <code>SQLState</code> associated with the <code>ErrorMsg</code>. If a match
-   * is not found or <code>ErrorMsg</code> has no <code>SQLState</code>, returns
-   * the <code>SQLState</code> bound to the <code>GENERIC_ERROR</code>
-   * <code>ErrorMsg</code>.
-   *
-   * @param mesg
-   *          An error message string
-   * @return SQLState
-   */
-  public static String findSQLState(String mesg) {
-
-    if (mesg == null) {
-      return GENERIC_ERROR.getSQLState();
-    }
-
-    // first see if there is a direct match
-    ErrorMsg errorMsg = mesgToErrorMsgMap.get(mesg);
-    if (errorMsg != null) {
-      if (errorMsg.getSQLState() != null) {
-        return errorMsg.getSQLState();
-      } else {
-        return GENERIC_ERROR.getSQLState();
-      }
-    }
-
-    // if not see if the mesg follows type of format, which is typically the
-    // case:
-    // line 1:14 Table not found table_name
-    String truncatedMesg = mesg.trim();
-    Matcher match = ERROR_MESSAGE_PATTERN.matcher(mesg);
-    if (match.matches()) {
-      truncatedMesg = match.group(1);
-    }
-
-    // appends might exist after the root message, so strip tokens off until we
-    // match
-    while (truncatedMesg.length() > minMesgLength) {
-      errorMsg = mesgToErrorMsgMap.get(truncatedMesg.trim());
-      if (errorMsg != null) {
-        if (errorMsg.getSQLState() != null) {
-          return errorMsg.getSQLState();
-        } else {
-          return GENERIC_ERROR.getSQLState();
-        }
-      }
-
-      int lastSpace = truncatedMesg.lastIndexOf(SPACE);
-      if (lastSpace == -1) {
-        break;
-      }
-
-      // hack off the last word and try again
-      truncatedMesg = truncatedMesg.substring(0, lastSpace).trim();
-    }
-
-    return GENERIC_ERROR.getSQLState();
-  }
-
-  ErrorMsg(String mesg) {
-    // 42000 is the generic SQLState for syntax error.
-    this(mesg, "42000");
-  }
-
-  ErrorMsg(String mesg, String sqlState) {
-    this.mesg = mesg;
-    this.sqlState = sqlState;
-  }
-
-  private static int getLine(ASTNode tree) {
-    if (tree.getChildCount() == 0) {
-      return tree.getToken().getLine();
-    }
-
-    return getLine((ASTNode) tree.getChild(0));
-  }
-
-  private static int getCharPositionInLine(ASTNode tree) {
-    if (tree.getChildCount() == 0) {
-      return tree.getToken().getCharPositionInLine();
-    }
-
-    return getCharPositionInLine((ASTNode) tree.getChild(0));
-  }
-
-  // Dirty hack as this will throw away spaces and other things - find a better
-  // way!
-  public static String getText(ASTNode tree) {
-    if (tree.getChildCount() == 0) {
-      return tree.getText();
-    }
-    return getText((ASTNode) tree.getChild(tree.getChildCount() - 1));
-  }
-
-  public String getMsg(ASTNode tree) {
-    StringBuilder sb = new StringBuilder();
-    renderPosition(sb, tree);
-    sb.append(" ");
-    sb.append(mesg);
-    sb.append(" '");
-    sb.append(getText(tree));
-    sb.append("'");
-    renderOrigin(sb, tree.getOrigin());
-    return sb.toString();
-  }
-
-  public static void renderOrigin(StringBuilder sb, ASTNodeOrigin origin) {
-    while (origin != null) {
-      sb.append(" in definition of ");
-      sb.append(origin.getObjectType());
-      sb.append(" ");
-      sb.append(origin.getObjectName());
-      sb.append(" [");
-      sb.append(HiveUtils.LINE_SEP);
-      sb.append(origin.getObjectDefinition());
-      sb.append(HiveUtils.LINE_SEP);
-      sb.append("] used as ");
-      sb.append(origin.getUsageAlias());
-      sb.append(" at ");
-      ASTNode usageNode = origin.getUsageNode();
-      renderPosition(sb, usageNode);
-      origin = usageNode.getOrigin();
-    }
-  }
-
-  private static void renderPosition(StringBuilder sb, ASTNode tree) {
-    sb.append("Line ");
-    sb.append(getLine(tree));
-    sb.append(":");
-    sb.append(getCharPositionInLine(tree));
-  }
-
-  public String getMsg(Tree tree) {
-    return getMsg((ASTNode) tree);
-  }
-
-  public String getMsg(ASTNode tree, String reason) {
-    return getMsg(tree) + ": " + reason;
-  }
-
-  public String getMsg(Tree tree, String reason) {
-    return getMsg((ASTNode) tree, reason);
-  }
-
-  public String getMsg(String reason) {
-    return mesg + " " + reason;
-  }
-
-  public String getMsg() {
-    return mesg;
-  }
-
-  public String getSQLState() {
-    return sqlState;
-  }
-}
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
index 265f14b..455910a 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java
@@ -42,6 +42,7 @@ import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.thrift.TDeserializer;
 import org.apache.thrift.TException;
 import org.apache.thrift.TSerializer;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
index 918cf23..61bc7fd 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
index f2b7018..6024dd4 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java
@@ -24,6 +24,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.plan.CreateFunctionDesc;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
index 5a8118c..6e231df 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
@@ -42,6 +42,7 @@ import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
index 227327e..3abc3da 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 05cb9a7..ff00df2 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -48,6 +48,7 @@ import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Order;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryProperties;
 import org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator;
 import org.apache.hadoop.hive.ql.exec.ArchiveUtils;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
index 81dd807..fe5441d 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
@@ -31,6 +31,7 @@ import java.util.Stack;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionInfo;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
index 8ca4a06..e8f95f4 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
@@ -26,10 +26,10 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.parse.ErrorMsg;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java b/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
index de816a9..3027ef4 100644
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestSemanticAnalyzerHookLoading.java
@@ -41,8 +41,7 @@ public class TestSemanticAnalyzerHookLoading extends TestCase {
 
     driver.run("drop table testDL");
     CommandProcessorResponse resp = driver.run("create table testDL (a int) as select * from tbl2");
-    assertEquals(10, resp.getResponseCode());
-    assertTrue(resp.getErrorMessage().contains("CTAS not supported."));
+    assertEquals(40000, resp.getResponseCode());
 
     resp = driver.run("create table testDL (a int)");
     assertEquals(0, resp.getResponseCode());
diff --git a/src/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out b/src/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out
index cb8030d..4efa8fb 100644
--- a/src/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out
+++ b/src/ql/src/test/results/clientnegative/alter_concatenate_indexed_table.q.out
@@ -66,4 +66,4 @@ PREHOOK: type: SHOWINDEXES
 POSTHOOK: query: show indexes on src_rc_concatenate_test
 POSTHOOK: type: SHOWINDEXES
 src_rc_concatenate_test_index	src_rc_concatenate_test	key                 	default__src_rc_concatenate_test_src_rc_concatenate_test_index__	compact             	
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: can not do merge because source table src_rc_concatenate_test is indexed.
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: can not do merge because source table src_rc_concatenate_test is indexed.
diff --git a/src/ql/src/test/results/clientnegative/alter_view_failure5.q.out b/src/ql/src/test/results/clientnegative/alter_view_failure5.q.out
index 232baac..0f1185d 100644
--- a/src/ql/src/test/results/clientnegative/alter_view_failure5.q.out
+++ b/src/ql/src/test/results/clientnegative/alter_view_failure5.q.out
@@ -15,4 +15,4 @@ SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx6
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: value not found in table's partition spec: {v=val_86}
+FAILED: SemanticException value not found in table's partition spec: {v=val_86}
diff --git a/src/ql/src/test/results/clientnegative/alter_view_failure6.q.out b/src/ql/src/test/results/clientnegative/alter_view_failure6.q.out
index 545ff09..b6c87c9 100644
--- a/src/ql/src/test/results/clientnegative/alter_view_failure6.q.out
+++ b/src/ql/src/test/results/clientnegative/alter_view_failure6.q.out
@@ -15,5 +15,5 @@ SELECT hr,key FROM srcpart
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx7
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: No partition predicate found for Alias "xxx7:srcpart" Table "srcpart"
-FAILED: Error in semantic analysis: The query does not reference any valid partition. To run this query, set hive.mapred.mode=nonstrict
+FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "xxx7:srcpart" Table "srcpart"
+FAILED: SemanticException [Error 10056]: The query does not reference any valid partition. To run this query, set hive.mapred.mode=nonstrict
diff --git a/src/ql/src/test/results/clientnegative/alter_view_failure7.q.out b/src/ql/src/test/results/clientnegative/alter_view_failure7.q.out
index a75dd6a..1a5c9cd 100644
--- a/src/ql/src/test/results/clientnegative/alter_view_failure7.q.out
+++ b/src/ql/src/test/results/clientnegative/alter_view_failure7.q.out
@@ -15,4 +15,4 @@ SELECT key,ds,hr FROM srcpart
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx8
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: table is partitioned but partition spec is not specified or does not fully match table partitioning: {ds=2011-01-01}
+FAILED: SemanticException table is partitioned but partition spec is not specified or does not fully match table partitioning: {ds=2011-01-01}
diff --git a/src/ql/src/test/results/clientnegative/ambiguous_col.q.out b/src/ql/src/test/results/clientnegative/ambiguous_col.q.out
index 05e525f..237c21f 100644
--- a/src/ql/src/test/results/clientnegative/ambiguous_col.q.out
+++ b/src/ql/src/test/results/clientnegative/ambiguous_col.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Ambiguous column reference key
+FAILED: SemanticException [Error 10007]: Ambiguous column reference key
diff --git a/src/ql/src/test/results/clientnegative/analyze.q.out b/src/ql/src/test/results/clientnegative/analyze.q.out
index 05a5c8f..94079c9 100644
--- a/src/ql/src/test/results/clientnegative/analyze.q.out
+++ b/src/ql/src/test/results/clientnegative/analyze.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Table is partitioned and partition specification is needed
+FAILED: SemanticException [Error 10115]: Table is partitioned and partition specification is needed
diff --git a/src/ql/src/test/results/clientnegative/analyze1.q.out b/src/ql/src/test/results/clientnegative/analyze1.q.out
index ede6e15..d60baa3 100644
--- a/src/ql/src/test/results/clientnegative/analyze1.q.out
+++ b/src/ql/src/test/results/clientnegative/analyze1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Non-Partition column appears in the partition specification:  key
+FAILED: SemanticException [Error 10098]: Non-Partition column appears in the partition specification:  key
diff --git a/src/ql/src/test/results/clientnegative/analyze_view.q.out b/src/ql/src/test/results/clientnegative/analyze_view.q.out
index 70e9d58..8e632f1 100644
--- a/src/ql/src/test/results/clientnegative/analyze_view.q.out
+++ b/src/ql/src/test/results/clientnegative/analyze_view.q.out
@@ -9,4 +9,4 @@ POSTHOOK: query: CREATE VIEW av AS SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@av
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: ANALYZE is not supported for views
+FAILED: SemanticException [Error 10091]: ANALYZE is not supported for views
diff --git a/src/ql/src/test/results/clientnegative/archive3.q.out b/src/ql/src/test/results/clientnegative/archive3.q.out
index 693dbc3..7adefb5 100644
--- a/src/ql/src/test/results/clientnegative/archive3.q.out
+++ b/src/ql/src/test/results/clientnegative/archive3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: ARCHIVE can only be run on partitions
+FAILED: SemanticException [Error 10110]: ARCHIVE can only be run on partitions
diff --git a/src/ql/src/test/results/clientnegative/archive4.q.out b/src/ql/src/test/results/clientnegative/archive4.q.out
index 1497fc3..c66123f 100644
--- a/src/ql/src/test/results/clientnegative/archive4.q.out
+++ b/src/ql/src/test/results/clientnegative/archive4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: ARCHIVE can only be run on a single partition
+FAILED: SemanticException [Error 10109]: ARCHIVE can only be run on a single partition
diff --git a/src/ql/src/test/results/clientnegative/archive5.q.out b/src/ql/src/test/results/clientnegative/archive5.q.out
index bc32ff4..1b0cafd 100644
--- a/src/ql/src/test/results/clientnegative/archive5.q.out
+++ b/src/ql/src/test/results/clientnegative/archive5.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Partition value contains a reserved substring (User value: 14_INTERMEDIATE_ORIGINAL Reserved substring: _INTERMEDIATE_ORIGINAL)
+FAILED: SemanticException [Error 10111]: Partition value contains a reserved substring (User value: 14_INTERMEDIATE_ORIGINAL Reserved substring: _INTERMEDIATE_ORIGINAL)
diff --git a/src/ql/src/test/results/clientnegative/archive_insert1.q.out b/src/ql/src/test/results/clientnegative/archive_insert1.q.out
index fe505df..0da06e4 100644
--- a/src/ql/src/test/results/clientnegative/archive_insert1.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_insert1.q.out
@@ -31,4 +31,4 @@ POSTHOOK: Input: default@tstsrcpart
 POSTHOOK: Output: default@tstsrcpart@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Insert conflict with existing archive: ds=2008-04-08/hr=12
+FAILED: SemanticException Insert conflict with existing archive: ds=2008-04-08/hr=12
diff --git a/src/ql/src/test/results/clientnegative/archive_insert2.q.out b/src/ql/src/test/results/clientnegative/archive_insert2.q.out
index 3e6d78b..ba9544e 100644
--- a/src/ql/src/test/results/clientnegative/archive_insert2.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_insert2.q.out
@@ -31,4 +31,4 @@ POSTHOOK: Input: default@tstsrcpart
 POSTHOOK: Output: default@tstsrcpart@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Insert conflict with existing archive: ds=2008-04-08
+FAILED: SemanticException Insert conflict with existing archive: ds=2008-04-08
diff --git a/src/ql/src/test/results/clientnegative/archive_insert3.q.out b/src/ql/src/test/results/clientnegative/archive_insert3.q.out
index 6242304..0f6ec6a 100644
--- a/src/ql/src/test/results/clientnegative/archive_insert3.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_insert3.q.out
@@ -31,4 +31,4 @@ POSTHOOK: Input: default@tstsrcpart
 POSTHOOK: Output: default@tstsrcpart@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Insert conflict with existing archive: ds=2008-04-08
+FAILED: SemanticException Insert conflict with existing archive: ds=2008-04-08
diff --git a/src/ql/src/test/results/clientnegative/archive_insert4.q.out b/src/ql/src/test/results/clientnegative/archive_insert4.q.out
index db1255e..15b85cc 100644
--- a/src/ql/src/test/results/clientnegative/archive_insert4.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_insert4.q.out
@@ -31,4 +31,4 @@ POSTHOOK: Input: default@tstsrcpart
 POSTHOOK: Output: default@tstsrcpart@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Insert conflict with existing archive: ds=2008-04-08/hr=12
+FAILED: SemanticException Insert conflict with existing archive: ds=2008-04-08/hr=12
diff --git a/src/ql/src/test/results/clientnegative/archive_partspec1.q.out b/src/ql/src/test/results/clientnegative/archive_partspec1.q.out
index ad503bd..63352a6 100644
--- a/src/ql/src/test/results/clientnegative/archive_partspec1.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_partspec1.q.out
@@ -21,4 +21,4 @@ POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 POSTHOOK: Output: default@srcpart_archived@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr), while the partitions specified in the query are: (ds, nonexistingpart).
+FAILED: SemanticException [Error 10125]: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr), while the partitions specified in the query are: (ds, nonexistingpart).
diff --git a/src/ql/src/test/results/clientnegative/archive_partspec2.q.out b/src/ql/src/test/results/clientnegative/archive_partspec2.q.out
index 05c554e..0cbc29f 100644
--- a/src/ql/src/test/results/clientnegative/archive_partspec2.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_partspec2.q.out
@@ -21,4 +21,4 @@ POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 POSTHOOK: Output: default@srcpart_archived@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr), while the partitions specified in the query are: (hr).
+FAILED: SemanticException [Error 10125]: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr), while the partitions specified in the query are: (hr).
diff --git a/src/ql/src/test/results/clientnegative/archive_partspec3.q.out b/src/ql/src/test/results/clientnegative/archive_partspec3.q.out
index 6690ffc..69aa8e9 100644
--- a/src/ql/src/test/results/clientnegative/archive_partspec3.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_partspec3.q.out
@@ -21,5 +21,5 @@ POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 POSTHOOK: Output: default@srcpart_archived@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Parse Error: line 3:48 mismatched input ')' expecting Identifier near '(' in archive statement
+FAILED: ParseException line 3:48 mismatched input ')' expecting Identifier near '(' in archive statement
 
diff --git a/src/ql/src/test/results/clientnegative/archive_partspec4.q.out b/src/ql/src/test/results/clientnegative/archive_partspec4.q.out
index eacfbb4..0ad0434 100644
--- a/src/ql/src/test/results/clientnegative/archive_partspec4.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_partspec4.q.out
@@ -21,4 +21,4 @@ POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 POSTHOOK: Output: default@srcpart_archived@ds=2008-04-08/hr=12
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr), while the partitions specified in the query are: (hr, ds).
+FAILED: SemanticException [Error 10125]: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr), while the partitions specified in the query are: (hr, ds).
diff --git a/src/ql/src/test/results/clientnegative/archive_partspec5.q.out b/src/ql/src/test/results/clientnegative/archive_partspec5.q.out
index 117001d..1599472 100644
--- a/src/ql/src/test/results/clientnegative/archive_partspec5.q.out
+++ b/src/ql/src/test/results/clientnegative/archive_partspec5.q.out
@@ -21,4 +21,4 @@ POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 POSTHOOK: Output: default@srcpart_archived@ds=2008-04-08/hr=12/min=00
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12,min=00).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: srcpart_archived PARTITION(ds=2008-04-08,hr=12,min=00).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr, min), while the partitions specified in the query are: (ds, min).
+FAILED: SemanticException [Error 10125]: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr, min), while the partitions specified in the query are: (ds, min).
diff --git a/src/ql/src/test/results/clientnegative/bad_indextype.q.out b/src/ql/src/test/results/clientnegative/bad_indextype.q.out
index b2bdd2e..1ec59a7 100644
--- a/src/ql/src/test/results/clientnegative/bad_indextype.q.out
+++ b/src/ql/src/test/results/clientnegative/bad_indextype.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: class name provided for index handler not found.
+FAILED: SemanticException class name provided for index handler not found.
diff --git a/src/ql/src/test/results/clientnegative/bad_sample_clause.q.out b/src/ql/src/test/results/clientnegative/bad_sample_clause.q.out
index 15b8fef..bbbb5f4 100644
--- a/src/ql/src/test/results/clientnegative/bad_sample_clause.q.out
+++ b/src/ql/src/test/results/clientnegative/bad_sample_clause.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key INT, value STRING, dt STRING, hr STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: Sampling expression needed for non-bucketed table srcpart
+FAILED: SemanticException [Error 10060]: Sampling expression needed for non-bucketed table srcpart
diff --git a/src/ql/src/test/results/clientnegative/clusterbydistributeby.q.out b/src/ql/src/test/results/clientnegative/clusterbydistributeby.q.out
index 07939a9..e029f5d 100644
--- a/src/ql/src/test/results/clientnegative/clusterbydistributeby.q.out
+++ b/src/ql/src/test/results/clientnegative/clusterbydistributeby.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: 8:14 Cannot have both CLUSTER BY and DISTRIBUTE BY clauses. Error encountered near token 'tkey'
+FAILED: SemanticException 8:14 Cannot have both CLUSTER BY and DISTRIBUTE BY clauses. Error encountered near token 'tkey'
diff --git a/src/ql/src/test/results/clientnegative/clusterbyorderby.q.out b/src/ql/src/test/results/clientnegative/clusterbyorderby.q.out
index 0b30640..3d4d901 100644
--- a/src/ql/src/test/results/clientnegative/clusterbyorderby.q.out
+++ b/src/ql/src/test/results/clientnegative/clusterbyorderby.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 5:0 mismatched input 'ORDER' expecting EOF near 'tkey'
+FAILED: ParseException line 5:0 mismatched input 'ORDER' expecting EOF near 'tkey'
 
diff --git a/src/ql/src/test/results/clientnegative/clusterbysortby.q.out b/src/ql/src/test/results/clientnegative/clusterbysortby.q.out
index 4c1f315..3d30460 100644
--- a/src/ql/src/test/results/clientnegative/clusterbysortby.q.out
+++ b/src/ql/src/test/results/clientnegative/clusterbysortby.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: 8:8 Cannot have both CLUSTER BY and SORT BY clauses. Error encountered near token 'one'
+FAILED: SemanticException 8:8 Cannot have both CLUSTER BY and SORT BY clauses. Error encountered near token 'one'
diff --git a/src/ql/src/test/results/clientnegative/clustern1.q.out b/src/ql/src/test/results/clientnegative/clustern1.q.out
index 7ad3f86..7c33af4 100644
--- a/src/ql/src/test/results/clientnegative/clustern1.q.out
+++ b/src/ql/src/test/results/clientnegative/clustern1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Column key Found in more than One Tables/Subqueries
+FAILED: SemanticException Column key Found in more than One Tables/Subqueries
diff --git a/src/ql/src/test/results/clientnegative/clustern2.q.out b/src/ql/src/test/results/clientnegative/clustern2.q.out
index 7ad3f86..7c33af4 100644
--- a/src/ql/src/test/results/clientnegative/clustern2.q.out
+++ b/src/ql/src/test/results/clientnegative/clustern2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Column key Found in more than One Tables/Subqueries
+FAILED: SemanticException Column key Found in more than One Tables/Subqueries
diff --git a/src/ql/src/test/results/clientnegative/clustern3.q.out b/src/ql/src/test/results/clientnegative/clustern3.q.out
index 63adb30..dd1c9e7 100644
--- a/src/ql/src/test/results/clientnegative/clustern3.q.out
+++ b/src/ql/src/test/results/clientnegative/clustern3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:52 Invalid column reference 'key'
+FAILED: SemanticException [Error 10002]: Line 2:52 Invalid column reference 'key'
diff --git a/src/ql/src/test/results/clientnegative/clustern4.q.out b/src/ql/src/test/results/clientnegative/clustern4.q.out
index f56a873..ef624aa 100644
--- a/src/ql/src/test/results/clientnegative/clustern4.q.out
+++ b/src/ql/src/test/results/clientnegative/clustern4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:50 Invalid table alias or column reference 'key': (possible column names are: _col0, _col1)
+FAILED: SemanticException [Error 10004]: Line 2:50 Invalid table alias or column reference 'key': (possible column names are: _col0, _col1)
diff --git a/src/ql/src/test/results/clientnegative/column_rename3.q.out b/src/ql/src/test/results/clientnegative/column_rename3.q.out
index e9f7515..f1ed689 100644
--- a/src/ql/src/test/results/clientnegative/column_rename3.q.out
+++ b/src/ql/src/test/results/clientnegative/column_rename3.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:27 cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in column type
+FAILED: ParseException line 1:27 cannot recognize input near '<EOF>' '<EOF>' '<EOF>' in column type
 
diff --git a/src/ql/src/test/results/clientnegative/compare_double_bigint.q.out b/src/ql/src/test/results/clientnegative/compare_double_bigint.q.out
index 926ddb4..68a295a 100644
--- a/src/ql/src/test/results/clientnegative/compare_double_bigint.q.out
+++ b/src/ql/src/test/results/clientnegative/compare_double_bigint.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments '1.0': In strict mode, comparing bigints and doubles is not allowed, it may result in a loss of precision. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException Line 0:-1 Wrong arguments '1.0': In strict mode, comparing bigints and doubles is not allowed, it may result in a loss of precision. If you really want to perform the operation, set hive.mapred.mode=nonstrict
diff --git a/src/ql/src/test/results/clientnegative/compare_string_bigint.q.out b/src/ql/src/test/results/clientnegative/compare_string_bigint.q.out
index 31c25cd..9a604fa 100644
--- a/src/ql/src/test/results/clientnegative/compare_string_bigint.q.out
+++ b/src/ql/src/test/results/clientnegative/compare_string_bigint.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments ''1'': In strict mode, comparing bigints and strings is not allowed, it may result in a loss of precision. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException Line 0:-1 Wrong arguments ''1'': In strict mode, comparing bigints and strings is not allowed, it may result in a loss of precision. If you really want to perform the operation, set hive.mapred.mode=nonstrict
diff --git a/src/ql/src/test/results/clientnegative/create_insert_outputformat.q.out b/src/ql/src/test/results/clientnegative/create_insert_outputformat.q.out
index 037b64c..ab1e935 100644
--- a/src/ql/src/test/results/clientnegative/create_insert_outputformat.q.out
+++ b/src/ql/src/test/results/clientnegative/create_insert_outputformat.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
+FAILED: SemanticException [Error 10055]: Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
diff --git a/src/ql/src/test/results/clientnegative/create_or_replace_view4.q.out b/src/ql/src/test/results/clientnegative/create_or_replace_view4.q.out
index a5f3c25..6ef8b80 100644
--- a/src/ql/src/test/results/clientnegative/create_or_replace_view4.q.out
+++ b/src/ql/src/test/results/clientnegative/create_or_replace_view4.q.out
@@ -13,4 +13,4 @@ POSTHOOK: query: create view v partitioned on (ds, hr) as select * from srcpart
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@v
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: At least one non-partitioning column must be present in view
+FAILED: SemanticException [Error 10092]: At least one non-partitioning column must be present in view
diff --git a/src/ql/src/test/results/clientnegative/create_or_replace_view5.q.out b/src/ql/src/test/results/clientnegative/create_or_replace_view5.q.out
index ca405da..ff5ec3b 100644
--- a/src/ql/src/test/results/clientnegative/create_or_replace_view5.q.out
+++ b/src/ql/src/test/results/clientnegative/create_or_replace_view5.q.out
@@ -13,4 +13,4 @@ POSTHOOK: query: create view v partitioned on (ds, hr) as select * from srcpart
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@v
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Can't combine IF NOT EXISTS and OR REPLACE.
+FAILED: SemanticException Can't combine IF NOT EXISTS and OR REPLACE.
diff --git a/src/ql/src/test/results/clientnegative/create_or_replace_view6.q.out b/src/ql/src/test/results/clientnegative/create_or_replace_view6.q.out
index 3d9f287..b53e42a 100644
--- a/src/ql/src/test/results/clientnegative/create_or_replace_view6.q.out
+++ b/src/ql/src/test/results/clientnegative/create_or_replace_view6.q.out
@@ -13,5 +13,5 @@ POSTHOOK: query: create view v partitioned on (ds, hr) as select * from srcpart
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@v
 #### A masked pattern was here ####
-FAILED: Parse Error: line 2:52 cannot recognize input near 'blah' '<EOF>' '<EOF>' in select clause
+FAILED: ParseException line 2:52 cannot recognize input near 'blah' '<EOF>' '<EOF>' in select clause
 
diff --git a/src/ql/src/test/results/clientnegative/create_or_replace_view7.q.out b/src/ql/src/test/results/clientnegative/create_or_replace_view7.q.out
index fe229ce..5c225a5 100644
--- a/src/ql/src/test/results/clientnegative/create_or_replace_view7.q.out
+++ b/src/ql/src/test/results/clientnegative/create_or_replace_view7.q.out
@@ -33,4 +33,4 @@ POSTHOOK: Input: default@v1
 POSTHOOK: Input: default@v2
 POSTHOOK: Output: default@v3
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Recursive view default.v1 detected (cycle: default.v1 -> default.v3 -> default.v2 -> default.v1).
+FAILED: SemanticException Recursive view default.v1 detected (cycle: default.v1 -> default.v3 -> default.v2 -> default.v1).
diff --git a/src/ql/src/test/results/clientnegative/create_or_replace_view8.q.out b/src/ql/src/test/results/clientnegative/create_or_replace_view8.q.out
index dc51328..44f0a74 100644
--- a/src/ql/src/test/results/clientnegative/create_or_replace_view8.q.out
+++ b/src/ql/src/test/results/clientnegative/create_or_replace_view8.q.out
@@ -13,4 +13,4 @@ POSTHOOK: query: create view v1 partitioned on (ds, hr) as select * from srcpart
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@v1
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Recursive view default.v1 detected (cycle: default.v1 -> default.v1).
+FAILED: SemanticException Recursive view default.v1 detected (cycle: default.v1 -> default.v1).
diff --git a/src/ql/src/test/results/clientnegative/create_table_failure1.q.out b/src/ql/src/test/results/clientnegative/create_table_failure1.q.out
index d6ba062..7a9876e 100644
--- a/src/ql/src/test/results/clientnegative/create_table_failure1.q.out
+++ b/src/ql/src/test/results/clientnegative/create_table_failure1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist: table_in_database_creation_not_exist
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist: table_in_database_creation_not_exist
diff --git a/src/ql/src/test/results/clientnegative/create_table_failure2.q.out b/src/ql/src/test/results/clientnegative/create_table_failure2.q.out
index d6ba062..7a9876e 100644
--- a/src/ql/src/test/results/clientnegative/create_table_failure2.q.out
+++ b/src/ql/src/test/results/clientnegative/create_table_failure2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist: table_in_database_creation_not_exist
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Database does not exist: table_in_database_creation_not_exist
diff --git a/src/ql/src/test/results/clientnegative/create_udaf_failure.q.out b/src/ql/src/test/results/clientnegative/create_udaf_failure.q.out
index d3d91fc..3fc3d36 100644
--- a/src/ql/src/test/results/clientnegative/create_udaf_failure.q.out
+++ b/src/ql/src/test/results/clientnegative/create_udaf_failure.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: CREATE TEMPORARY FUNCTION test_udaf AS 'org.apache.hadoop.hive.q
 PREHOOK: type: CREATEFUNCTION
 POSTHOOK: query: CREATE TEMPORARY FUNCTION test_udaf AS 'org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase'
 POSTHOOK: type: CREATEFUNCTION
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException: public boolean org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase$UDAFWrongArgLengthForTestCaseEvaluator.merge() requires 0 arguments but 1 are passed in.
+FAILED: SemanticException org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException: public boolean org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase$UDAFWrongArgLengthForTestCaseEvaluator.merge() requires 0 arguments but 1 are passed in.
diff --git a/src/ql/src/test/results/clientnegative/create_view_failure3.q.out b/src/ql/src/test/results/clientnegative/create_view_failure3.q.out
index 8b40a8f..5ddbdb6 100644
--- a/src/ql/src/test/results/clientnegative/create_view_failure3.q.out
+++ b/src/ql/src/test/results/clientnegative/create_view_failure3.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: DROP VIEW xxx13
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx13
 POSTHOOK: type: DROPVIEW
-FAILED: Error in semantic analysis: 5:16 The number of columns produced by the SELECT clause does not match the number of column names specified by CREATE VIEW. Error encountered near token 'key'
+FAILED: SemanticException 5:16 The number of columns produced by the SELECT clause does not match the number of column names specified by CREATE VIEW. Error encountered near token 'key'
diff --git a/src/ql/src/test/results/clientnegative/create_view_failure5.q.out b/src/ql/src/test/results/clientnegative/create_view_failure5.q.out
index 1a5327d..d79dc64 100644
--- a/src/ql/src/test/results/clientnegative/create_view_failure5.q.out
+++ b/src/ql/src/test/results/clientnegative/create_view_failure5.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: DROP VIEW xxx14
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx14
 POSTHOOK: type: DROPVIEW
-FAILED: Error in semantic analysis: Duplicate column name: key
+FAILED: SemanticException [Error 10036]: Duplicate column name: key
diff --git a/src/ql/src/test/results/clientnegative/create_view_failure6.q.out b/src/ql/src/test/results/clientnegative/create_view_failure6.q.out
index fd7af21..25c1c7f 100644
--- a/src/ql/src/test/results/clientnegative/create_view_failure6.q.out
+++ b/src/ql/src/test/results/clientnegative/create_view_failure6.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: DROP VIEW xxx15
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx15
 POSTHOOK: type: DROPVIEW
-FAILED: Error in semantic analysis: Rightmost columns in view output do not match PARTITIONED ON clause
+FAILED: SemanticException [Error 10093]: Rightmost columns in view output do not match PARTITIONED ON clause
diff --git a/src/ql/src/test/results/clientnegative/create_view_failure7.q.out b/src/ql/src/test/results/clientnegative/create_view_failure7.q.out
index 140a351..f13ab63 100644
--- a/src/ql/src/test/results/clientnegative/create_view_failure7.q.out
+++ b/src/ql/src/test/results/clientnegative/create_view_failure7.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: DROP VIEW xxx16
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx16
 POSTHOOK: type: DROPVIEW
-FAILED: Error in semantic analysis: At least one non-partitioning column must be present in view
+FAILED: SemanticException [Error 10092]: At least one non-partitioning column must be present in view
diff --git a/src/ql/src/test/results/clientnegative/create_view_failure8.q.out b/src/ql/src/test/results/clientnegative/create_view_failure8.q.out
index 8027979..158fed1 100644
--- a/src/ql/src/test/results/clientnegative/create_view_failure8.q.out
+++ b/src/ql/src/test/results/clientnegative/create_view_failure8.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: DROP VIEW xxx17
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx17
 POSTHOOK: type: DROPVIEW
-FAILED: Error in semantic analysis: Rightmost columns in view output do not match PARTITIONED ON clause
+FAILED: SemanticException [Error 10093]: Rightmost columns in view output do not match PARTITIONED ON clause
diff --git a/src/ql/src/test/results/clientnegative/create_view_failure9.q.out b/src/ql/src/test/results/clientnegative/create_view_failure9.q.out
index b20e3d8..e6ad388 100644
--- a/src/ql/src/test/results/clientnegative/create_view_failure9.q.out
+++ b/src/ql/src/test/results/clientnegative/create_view_failure9.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: DROP VIEW xxx18
 PREHOOK: type: DROPVIEW
 POSTHOOK: query: DROP VIEW xxx18
 POSTHOOK: type: DROPVIEW
-FAILED: Error in semantic analysis: Rightmost columns in view output do not match PARTITIONED ON clause
+FAILED: SemanticException [Error 10093]: Rightmost columns in view output do not match PARTITIONED ON clause
diff --git a/src/ql/src/test/results/clientnegative/ctas.q.out b/src/ql/src/test/results/clientnegative/ctas.q.out
index 64612df..ef27427 100644
--- a/src/ql/src/test/results/clientnegative/ctas.q.out
+++ b/src/ql/src/test/results/clientnegative/ctas.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: CREATE-TABLE-AS-SELECT cannot create external table
+FAILED: SemanticException [Error 10070]: CREATE-TABLE-AS-SELECT cannot create external table
diff --git a/src/ql/src/test/results/clientnegative/ddltime.q.out b/src/ql/src/test/results/clientnegative/ddltime.q.out
index ed747e3..c86d5cf 100644
--- a/src/ql/src/test/results/clientnegative/ddltime.q.out
+++ b/src/ql/src/test/results/clientnegative/ddltime.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table T2 like srcpart
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@T2
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: 3:23 HOLD_DDLTIME hint cannot be applied to dynamic partitions or non-existent partitions. Error encountered near token ''1''
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: 3:23 HOLD_DDLTIME hint cannot be applied to dynamic partitions or non-existent partitions. Error encountered near token ''1''
diff --git a/src/ql/src/test/results/clientnegative/default_partition_name.q.out b/src/ql/src/test/results/clientnegative/default_partition_name.q.out
index 05cc0d4..6a81e2c 100644
--- a/src/ql/src/test/results/clientnegative/default_partition_name.q.out
+++ b/src/ql/src/test/results/clientnegative/default_partition_name.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table default_partition_name (key int, value string) partitioned by (ds string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@default_partition_name
-FAILED: Error in semantic analysis: Partition value contains a reserved substring (User value: __HIVE_DEFAULT_PARTITION__ Reserved substring: __HIVE_DEFAULT_PARTITION__)
+FAILED: SemanticException [Error 10111]: Partition value contains a reserved substring (User value: __HIVE_DEFAULT_PARTITION__ Reserved substring: __HIVE_DEFAULT_PARTITION__)
diff --git a/src/ql/src/test/results/clientnegative/drop_function_failure.q.out b/src/ql/src/test/results/clientnegative/drop_function_failure.q.out
index d7ebf64..5821c1f 100644
--- a/src/ql/src/test/results/clientnegative/drop_function_failure.q.out
+++ b/src/ql/src/test/results/clientnegative/drop_function_failure.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Invalid function UnknownFunction
+FAILED: SemanticException [Error 10011]: Invalid function UnknownFunction
diff --git a/src/ql/src/test/results/clientnegative/drop_index_failure.q.out b/src/ql/src/test/results/clientnegative/drop_index_failure.q.out
index cdbfcbe..f64ff5b 100644
--- a/src/ql/src/test/results/clientnegative/drop_index_failure.q.out
+++ b/src/ql/src/test/results/clientnegative/drop_index_failure.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Invalid index UnknownIndex
+FAILED: SemanticException [Error 10003]: Invalid index UnknownIndex
diff --git a/src/ql/src/test/results/clientnegative/drop_partition_failure.q.out b/src/ql/src/test/results/clientnegative/drop_partition_failure.q.out
index e881e9d..5db9d92 100644
--- a/src/ql/src/test/results/clientnegative/drop_partition_failure.q.out
+++ b/src/ql/src/test/results/clientnegative/drop_partition_failure.q.out
@@ -31,4 +31,4 @@ POSTHOOK: type: SHOWPARTITIONS
 b=1/c=1
 b=1/c=2
 b=2/c=2
-FAILED: Error in semantic analysis: Partition not found b = '3'
+FAILED: SemanticException [Error 10006]: Partition not found b = '3'
diff --git a/src/ql/src/test/results/clientnegative/drop_partition_filter_failure.q.out b/src/ql/src/test/results/clientnegative/drop_partition_filter_failure.q.out
index 120f514..863d821 100644
--- a/src/ql/src/test/results/clientnegative/drop_partition_filter_failure.q.out
+++ b/src/ql/src/test/results/clientnegative/drop_partition_filter_failure.q.out
@@ -15,4 +15,4 @@ PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: show partitions ptestfilter1
 POSTHOOK: type: SHOWPARTITIONS
 c=US/d=1
-FAILED: Error in semantic analysis: Partition not found c = 'US' AND d < 1
+FAILED: SemanticException [Error 10006]: Partition not found c = 'US' AND d < 1
diff --git a/src/ql/src/test/results/clientnegative/drop_table_failure1.q.out b/src/ql/src/test/results/clientnegative/drop_table_failure1.q.out
index c9a141b..b744332 100644
--- a/src/ql/src/test/results/clientnegative/drop_table_failure1.q.out
+++ b/src/ql/src/test/results/clientnegative/drop_table_failure1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Table not found UnknownTable
+FAILED: SemanticException [Error 10001]: Table not found UnknownTable
diff --git a/src/ql/src/test/results/clientnegative/drop_view_failure2.q.out b/src/ql/src/test/results/clientnegative/drop_view_failure2.q.out
index 644e3f0..428a2b7 100644
--- a/src/ql/src/test/results/clientnegative/drop_view_failure2.q.out
+++ b/src/ql/src/test/results/clientnegative/drop_view_failure2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Table not found UnknownView
+FAILED: SemanticException [Error 10001]: Table not found UnknownView
diff --git a/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform.q.out b/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform.q.out
index fd672db..4ab2f3a 100644
--- a/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform.q.out
+++ b/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Column alias already exists: foo
+FAILED: SemanticException [Error 10074]: Column alias already exists: foo
diff --git a/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform_schema.q.out b/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform_schema.q.out
index fd672db..4ab2f3a 100644
--- a/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform_schema.q.out
+++ b/src/ql/src/test/results/clientnegative/duplicate_alias_in_transform_schema.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Column alias already exists: foo
+FAILED: SemanticException [Error 10074]: Column alias already exists: foo
diff --git a/src/ql/src/test/results/clientnegative/duplicate_insert1.q.out b/src/ql/src/test/results/clientnegative/duplicate_insert1.q.out
index 2432f64..2a9c02e 100644
--- a/src/ql/src/test/results/clientnegative/duplicate_insert1.q.out
+++ b/src/ql/src/test/results/clientnegative/duplicate_insert1.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dest1_din1(key int, value string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1_din1
-FAILED: Error in semantic analysis: The same output cannot be present multiple times:  dest1_din1
+FAILED: SemanticException [Error 10087]: The same output cannot be present multiple times:  dest1_din1
diff --git a/src/ql/src/test/results/clientnegative/duplicate_insert2.q.out b/src/ql/src/test/results/clientnegative/duplicate_insert2.q.out
index 17477fb..5255d85 100644
--- a/src/ql/src/test/results/clientnegative/duplicate_insert2.q.out
+++ b/src/ql/src/test/results/clientnegative/duplicate_insert2.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dest1_din2(key int, value string) partitioned by (ds string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1_din2
-FAILED: Error in semantic analysis: The same output cannot be present multiple times:  dest1_din2@ds=1
+FAILED: SemanticException [Error 10087]: The same output cannot be present multiple times:  dest1_din2@ds=1
diff --git a/src/ql/src/test/results/clientnegative/dyn_part1.q.out b/src/ql/src/test/results/clientnegative/dyn_part1.q.out
index 6011a59..197923a 100644
--- a/src/ql/src/test/results/clientnegative/dyn_part1.q.out
+++ b/src/ql/src/test/results/clientnegative/dyn_part1.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dynamic_partition (key string) partitioned by (value string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dynamic_partition
-FAILED: Error in semantic analysis: Non-Partition column appears in the partition specification:  hr
+FAILED: SemanticException [Error 10098]: Non-Partition column appears in the partition specification:  hr
diff --git a/src/ql/src/test/results/clientnegative/dyn_part2.q.out b/src/ql/src/test/results/clientnegative/dyn_part2.q.out
index 455adba..9800013 100644
--- a/src/ql/src/test/results/clientnegative/dyn_part2.q.out
+++ b/src/ql/src/test/results/clientnegative/dyn_part2.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table nzhang_part1 (key string, value string) partitioned by (ds string, hr string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@nzhang_part1
-FAILED: Error in semantic analysis: Line 3:23 Cannot insert into target table because column number/types are different 'hr': Table insclause-0 has 3 columns, but query has 2 columns.
+FAILED: SemanticException [Error 10044]: Line 3:23 Cannot insert into target table because column number/types are different 'hr': Table insclause-0 has 3 columns, but query has 2 columns.
diff --git a/src/ql/src/test/results/clientnegative/dyn_part4.q.out b/src/ql/src/test/results/clientnegative/dyn_part4.q.out
index e13aac6..43f1e4d 100644
--- a/src/ql/src/test/results/clientnegative/dyn_part4.q.out
+++ b/src/ql/src/test/results/clientnegative/dyn_part4.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table nzhang_part4 (key string) partitioned by (ds string, hr string, value string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@nzhang_part4
-FAILED: Error in semantic analysis: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr, value), while the partitions specified in the query are: (value, ds, hr).
+FAILED: SemanticException [Error 10125]: Partition columns in partition specification are not the same as that defined in the table schema. The names and orders have to be exactly the same. Partition columns in the table schema are: (ds, hr, value), while the partitions specified in the query are: (value, ds, hr).
diff --git a/src/ql/src/test/results/clientnegative/dyn_part_merge.q.out b/src/ql/src/test/results/clientnegative/dyn_part_merge.q.out
index de01fd7..ba63b6c 100644
--- a/src/ql/src/test/results/clientnegative/dyn_part_merge.q.out
+++ b/src/ql/src/test/results/clientnegative/dyn_part_merge.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table dyn_merge(key string, value string) partitioned by (ds string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dyn_merge
-FAILED: Error in semantic analysis: Dynamic partition does not support merging using non-CombineHiveInputFormatPlease check your hive.input.format setting and make sure your Hadoop version support CombineFileInputFormat
+FAILED: SemanticException [Error 10097]: Dynamic partition does not support merging using non-CombineHiveInputFormat. Please check your hive.input.format setting and make sure your Hadoop version support CombineFileInputFormat
diff --git a/src/ql/src/test/results/clientnegative/exim_00_unsupported_schema.q.out b/src/ql/src/test/results/clientnegative/exim_00_unsupported_schema.q.out
index 814b742..821da6f 100644
--- a/src/ql/src/test/results/clientnegative/exim_00_unsupported_schema.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_00_unsupported_schema.q.out
@@ -13,4 +13,4 @@ PREHOOK: Output: default@exim_department
 POSTHOOK: query: load data local inpath "../data/files/test.dat" into table exim_department
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@exim_department
-FAILED: Error in semantic analysis: Invalid path only the following file systems accepted for export/import : hdfs,pfile
+FAILED: SemanticException Invalid path only the following file systems accepted for export/import : hdfs,pfile
diff --git a/src/ql/src/test/results/clientnegative/exim_01_nonpart_over_loaded.q.out b/src/ql/src/test/results/clientnegative/exim_01_nonpart_over_loaded.q.out
index bd23450..963bf8b 100644
--- a/src/ql/src/test/results/clientnegative/exim_01_nonpart_over_loaded.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_01_nonpart_over_loaded.q.out
@@ -52,4 +52,4 @@ PREHOOK: Output: importer@exim_department
 POSTHOOK: query: load data local inpath "../data/files/test.dat" into table exim_department
 POSTHOOK: type: LOAD
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: Table exists and contains data files
+FAILED: SemanticException [Error 10119]: Table exists and contains data files
diff --git a/src/ql/src/test/results/clientnegative/exim_02_all_part_over_overlap.q.out b/src/ql/src/test/results/clientnegative/exim_02_all_part_over_overlap.q.out
index 3ba42b8..fc551aa 100644
--- a/src/ql/src/test/results/clientnegative/exim_02_all_part_over_overlap.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_02_all_part_over_overlap.q.out
@@ -99,4 +99,4 @@ POSTHOOK: query: load data local inpath "../data/files/test.dat"
 POSTHOOK: type: LOAD
 POSTHOOK: Output: importer@exim_employee
 POSTHOOK: Output: importer@exim_employee@emp_country=us/emp_state=ka
-FAILED: Error in semantic analysis: Partition already exists emp_country=us,emp_state=ka
+FAILED: SemanticException [Error 10118]: Partition already exists emp_country=us,emp_state=ka
diff --git a/src/ql/src/test/results/clientnegative/exim_03_nonpart_noncompat_colschema.q.out b/src/ql/src/test/results/clientnegative/exim_03_nonpart_noncompat_colschema.q.out
index ca58c80..1a47509 100644
--- a/src/ql/src/test/results/clientnegative/exim_03_nonpart_noncompat_colschema.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_03_nonpart_noncompat_colschema.q.out
@@ -46,4 +46,4 @@ POSTHOOK: query: create table exim_department ( dep_key int comment "department
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Column Schema does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Column Schema does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_04_nonpart_noncompat_colnumber.q.out b/src/ql/src/test/results/clientnegative/exim_04_nonpart_noncompat_colnumber.q.out
index 0fe5ec0..0326757 100644
--- a/src/ql/src/test/results/clientnegative/exim_04_nonpart_noncompat_colnumber.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_04_nonpart_noncompat_colnumber.q.out
@@ -46,4 +46,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Column Schema does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Column Schema does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_05_nonpart_noncompat_coltype.q.out b/src/ql/src/test/results/clientnegative/exim_05_nonpart_noncompat_coltype.q.out
index 165039f..5e87a58 100644
--- a/src/ql/src/test/results/clientnegative/exim_05_nonpart_noncompat_coltype.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_05_nonpart_noncompat_coltype.q.out
@@ -46,4 +46,4 @@ POSTHOOK: query: create table exim_department ( dep_id bigint comment "departmen
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Column Schema does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Column Schema does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_06_nonpart_noncompat_storage.q.out b/src/ql/src/test/results/clientnegative/exim_06_nonpart_noncompat_storage.q.out
index d95cf31..773c2e5 100644
--- a/src/ql/src/test/results/clientnegative/exim_06_nonpart_noncompat_storage.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_06_nonpart_noncompat_storage.q.out
@@ -46,4 +46,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Table inputformat/outputformats do not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Table inputformat/outputformats do not match
diff --git a/src/ql/src/test/results/clientnegative/exim_07_nonpart_noncompat_ifof.q.out b/src/ql/src/test/results/clientnegative/exim_07_nonpart_noncompat_ifof.q.out
index 90cdb1a..54f591d 100644
--- a/src/ql/src/test/results/clientnegative/exim_07_nonpart_noncompat_ifof.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_07_nonpart_noncompat_ifof.q.out
@@ -52,4 +52,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Table inputformat/outputformats do not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Table inputformat/outputformats do not match
diff --git a/src/ql/src/test/results/clientnegative/exim_08_nonpart_noncompat_serde.q.out b/src/ql/src/test/results/clientnegative/exim_08_nonpart_noncompat_serde.q.out
index 2b770fc..55146a7 100644
--- a/src/ql/src/test/results/clientnegative/exim_08_nonpart_noncompat_serde.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_08_nonpart_noncompat_serde.q.out
@@ -48,4 +48,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Table Serde class does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Table Serde class does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_09_nonpart_noncompat_serdeparam.q.out b/src/ql/src/test/results/clientnegative/exim_09_nonpart_noncompat_serdeparam.q.out
index 868e92f..ffdeea1 100644
--- a/src/ql/src/test/results/clientnegative/exim_09_nonpart_noncompat_serdeparam.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_09_nonpart_noncompat_serdeparam.q.out
@@ -56,4 +56,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Table Serde format does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Table Serde format does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_10_nonpart_noncompat_bucketing.q.out b/src/ql/src/test/results/clientnegative/exim_10_nonpart_noncompat_bucketing.q.out
index 7994b56..ffa164d 100644
--- a/src/ql/src/test/results/clientnegative/exim_10_nonpart_noncompat_bucketing.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_10_nonpart_noncompat_bucketing.q.out
@@ -48,4 +48,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Table bucketing spec does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Table bucketing spec does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_11_nonpart_noncompat_sorting.q.out b/src/ql/src/test/results/clientnegative/exim_11_nonpart_noncompat_sorting.q.out
index fac0e0b..c4605e6 100644
--- a/src/ql/src/test/results/clientnegative/exim_11_nonpart_noncompat_sorting.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_11_nonpart_noncompat_sorting.q.out
@@ -50,4 +50,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Table sorting spec does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Table sorting spec does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_12_nonnative_export.q.out b/src/ql/src/test/results/clientnegative/exim_12_nonnative_export.q.out
index d2333a9..89456b6 100644
--- a/src/ql/src/test/results/clientnegative/exim_12_nonnative_export.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_12_nonnative_export.q.out
@@ -9,4 +9,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@exim_department
-FAILED: Error in semantic analysis: Export/Import cannot be done for a non-native table. 
+FAILED: SemanticException [Error 10121]: Export/Import cannot be done for a non-native table. 
diff --git a/src/ql/src/test/results/clientnegative/exim_13_nonnative_import.q.out b/src/ql/src/test/results/clientnegative/exim_13_nonnative_import.q.out
index ab44cfb..67aeabc 100644
--- a/src/ql/src/test/results/clientnegative/exim_13_nonnative_import.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_13_nonnative_import.q.out
@@ -46,4 +46,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: Export/Import cannot be done for a non-native table. 
+FAILED: SemanticException [Error 10121]: Export/Import cannot be done for a non-native table. 
diff --git a/src/ql/src/test/results/clientnegative/exim_14_nonpart_part.q.out b/src/ql/src/test/results/clientnegative/exim_14_nonpart_part.q.out
index cfaa1e6..6ddb3dc 100644
--- a/src/ql/src/test/results/clientnegative/exim_14_nonpart_part.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_14_nonpart_part.q.out
@@ -48,4 +48,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Partition Schema does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Partition Schema does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_15_part_nonpart.q.out b/src/ql/src/test/results/clientnegative/exim_15_part_nonpart.q.out
index ca6ba0c..4435650 100644
--- a/src/ql/src/test/results/clientnegative/exim_15_part_nonpart.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_15_part_nonpart.q.out
@@ -49,4 +49,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Partition Schema does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Partition Schema does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_16_part_noncompat_schema.q.out b/src/ql/src/test/results/clientnegative/exim_16_part_noncompat_schema.q.out
index 95b1ed2..8ce2691 100644
--- a/src/ql/src/test/results/clientnegative/exim_16_part_noncompat_schema.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_16_part_noncompat_schema.q.out
@@ -51,4 +51,4 @@ POSTHOOK: query: create table exim_department ( dep_id int comment "department i
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   Partition Schema does not match
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   Partition Schema does not match
diff --git a/src/ql/src/test/results/clientnegative/exim_17_part_spec_underspec.q.out b/src/ql/src/test/results/clientnegative/exim_17_part_spec_underspec.q.out
index 221bfb2..63d2395 100644
--- a/src/ql/src/test/results/clientnegative/exim_17_part_spec_underspec.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_17_part_spec_underspec.q.out
@@ -77,4 +77,4 @@ PREHOOK: query: use importer
 PREHOOK: type: SWITCHDATABASE
 POSTHOOK: query: use importer
 POSTHOOK: type: SWITCHDATABASE
-FAILED: Error in semantic analysis: Partition not found  - Specified partition not found in import directory
+FAILED: SemanticException [Error 10006]: Partition not found  - Specified partition not found in import directory
diff --git a/src/ql/src/test/results/clientnegative/exim_18_part_spec_missing.q.out b/src/ql/src/test/results/clientnegative/exim_18_part_spec_missing.q.out
index 221bfb2..63d2395 100644
--- a/src/ql/src/test/results/clientnegative/exim_18_part_spec_missing.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_18_part_spec_missing.q.out
@@ -77,4 +77,4 @@ PREHOOK: query: use importer
 PREHOOK: type: SWITCHDATABASE
 POSTHOOK: query: use importer
 POSTHOOK: type: SWITCHDATABASE
-FAILED: Error in semantic analysis: Partition not found  - Specified partition not found in import directory
+FAILED: SemanticException [Error 10006]: Partition not found  - Specified partition not found in import directory
diff --git a/src/ql/src/test/results/clientnegative/exim_19_external_over_existing.q.out b/src/ql/src/test/results/clientnegative/exim_19_external_over_existing.q.out
index aaa68a0..3189f1a 100644
--- a/src/ql/src/test/results/clientnegative/exim_19_external_over_existing.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_19_external_over_existing.q.out
@@ -46,4 +46,4 @@ POSTHOOK: query: create  table exim_department ( dep_id int comment "department
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_department
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   External table cannot overwrite existing table. Drop existing table first.
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   External table cannot overwrite existing table. Drop existing table first.
diff --git a/src/ql/src/test/results/clientnegative/exim_21_part_managed_external.q.out b/src/ql/src/test/results/clientnegative/exim_21_part_managed_external.q.out
index 22d921f..95c4256 100644
--- a/src/ql/src/test/results/clientnegative/exim_21_part_managed_external.q.out
+++ b/src/ql/src/test/results/clientnegative/exim_21_part_managed_external.q.out
@@ -90,4 +90,4 @@ POSTHOOK: query: create table exim_employee ( emp_id int comment "employee id")
 	tblproperties("creator"="krishna")
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: importer@exim_employee
-FAILED: Error in semantic analysis: The existing table is not compatible with the import spec.   External table cannot overwrite existing table. Drop existing table first.
+FAILED: SemanticException [Error 10120]: The existing table is not compatible with the import spec.   External table cannot overwrite existing table. Drop existing table first.
diff --git a/src/ql/src/test/results/clientnegative/fileformat_bad_class.q.out b/src/ql/src/test/results/clientnegative/fileformat_bad_class.q.out
index 037b64c..ab1e935 100644
--- a/src/ql/src/test/results/clientnegative/fileformat_bad_class.q.out
+++ b/src/ql/src/test/results/clientnegative/fileformat_bad_class.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
+FAILED: SemanticException [Error 10055]: Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
diff --git a/src/ql/src/test/results/clientnegative/fileformat_void_input.q.out b/src/ql/src/test/results/clientnegative/fileformat_void_input.q.out
index bf09113..78187ed 100644
--- a/src/ql/src/test/results/clientnegative/fileformat_void_input.q.out
+++ b/src/ql/src/test/results/clientnegative/fileformat_void_input.q.out
@@ -19,4 +19,4 @@ POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
 POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: 3:20 Input format must implement InputFormat. Error encountered near token 'dest1'
+FAILED: SemanticException 3:20 Input format must implement InputFormat. Error encountered near token 'dest1'
diff --git a/src/ql/src/test/results/clientnegative/fileformat_void_output.q.out b/src/ql/src/test/results/clientnegative/fileformat_void_output.q.out
index 037b64c..ab1e935 100644
--- a/src/ql/src/test/results/clientnegative/fileformat_void_output.q.out
+++ b/src/ql/src/test/results/clientnegative/fileformat_void_output.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
+FAILED: SemanticException [Error 10055]: Output Format must implement HiveOutputFormat, otherwise it should be either IgnoreKeyTextOutputFormat or SequenceFileOutputFormat
diff --git a/src/ql/src/test/results/clientnegative/fs_default_name1.q.out b/src/ql/src/test/results/clientnegative/fs_default_name1.q.out
index ad46d51..571a70d 100644
--- a/src/ql/src/test/results/clientnegative/fs_default_name1.q.out
+++ b/src/ql/src/test/results/clientnegative/fs_default_name1.q.out
@@ -1,4 +1 @@
-FAILED: Hive Internal Error: java.lang.IllegalArgumentException(null)
-java.lang.IllegalArgumentException
-#### A masked pattern was here ####
-
+FAILED: IllegalArgumentException null
diff --git a/src/ql/src/test/results/clientnegative/fs_default_name2.q.out b/src/ql/src/test/results/clientnegative/fs_default_name2.q.out
index ad46d51..571a70d 100644
--- a/src/ql/src/test/results/clientnegative/fs_default_name2.q.out
+++ b/src/ql/src/test/results/clientnegative/fs_default_name2.q.out
@@ -1,4 +1 @@
-FAILED: Hive Internal Error: java.lang.IllegalArgumentException(null)
-java.lang.IllegalArgumentException
-#### A masked pattern was here ####
-
+FAILED: IllegalArgumentException null
diff --git a/src/ql/src/test/results/clientnegative/genericFileFormat.q.out b/src/ql/src/test/results/clientnegative/genericFileFormat.q.out
index 32be000..9613df9 100644
--- a/src/ql/src/test/results/clientnegative/genericFileFormat.q.out
+++ b/src/ql/src/test/results/clientnegative/genericFileFormat.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Unrecognized file format in STORED AS clause: foo
+FAILED: SemanticException Unrecognized file format in STORED AS clause: foo
diff --git a/src/ql/src/test/results/clientnegative/groupby2_map_skew_multi_distinct.q.out b/src/ql/src/test/results/clientnegative/groupby2_map_skew_multi_distinct.q.out
index 216aa47..ce40d2a 100644
--- a/src/ql/src/test/results/clientnegative/groupby2_map_skew_multi_distinct.q.out
+++ b/src/ql/src/test/results/clientnegative/groupby2_map_skew_multi_distinct.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: DISTINCT on different columns not supported with skew in data
+FAILED: SemanticException [Error 10022]: DISTINCT on different columns not supported with skew in data
diff --git a/src/ql/src/test/results/clientnegative/groupby2_multi_distinct.q.out b/src/ql/src/test/results/clientnegative/groupby2_multi_distinct.q.out
index 46914cf..ca3d4fc 100644
--- a/src/ql/src/test/results/clientnegative/groupby2_multi_distinct.q.out
+++ b/src/ql/src/test/results/clientnegative/groupby2_multi_distinct.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest_g2
-FAILED: Error in semantic analysis: DISTINCT on different columns not supported with skew in data
+FAILED: SemanticException [Error 10022]: DISTINCT on different columns not supported with skew in data
diff --git a/src/ql/src/test/results/clientnegative/groupby3_map_skew_multi_distinct.q.out b/src/ql/src/test/results/clientnegative/groupby3_map_skew_multi_distinct.q.out
index 81a4633..856213b 100644
--- a/src/ql/src/test/results/clientnegative/groupby3_map_skew_multi_distinct.q.out
+++ b/src/ql/src/test/results/clientnegative/groupby3_map_skew_multi_distinct.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE, c10 DOUBLE, c11 DOUBLE) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: DISTINCT on different columns not supported with skew in data
+FAILED: SemanticException [Error 10022]: DISTINCT on different columns not supported with skew in data
diff --git a/src/ql/src/test/results/clientnegative/groupby3_multi_distinct.q.out b/src/ql/src/test/results/clientnegative/groupby3_multi_distinct.q.out
index 81a4633..856213b 100644
--- a/src/ql/src/test/results/clientnegative/groupby3_multi_distinct.q.out
+++ b/src/ql/src/test/results/clientnegative/groupby3_multi_distinct.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE, c10 DOUBLE, c11 DOUBLE) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: DISTINCT on different columns not supported with skew in data
+FAILED: SemanticException [Error 10022]: DISTINCT on different columns not supported with skew in data
diff --git a/src/ql/src/test/results/clientnegative/groupby_key.q.out b/src/ql/src/test/results/clientnegative/groupby_key.q.out
index 906197c..f5f0958 100644
--- a/src/ql/src/test/results/clientnegative/groupby_key.q.out
+++ b/src/ql/src/test/results/clientnegative/groupby_key.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Expression not in GROUP BY key 'value'
+FAILED: SemanticException [Error 10025]: Line 1:7 Expression not in GROUP BY key 'value'
diff --git a/src/ql/src/test/results/clientnegative/having1.q.out b/src/ql/src/test/results/clientnegative/having1.q.out
index 0951bdb..31e12c5 100644
--- a/src/ql/src/test/results/clientnegative/having1.q.out
+++ b/src/ql/src/test/results/clientnegative/having1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: HAVING specified without GROUP BY
+FAILED: SemanticException HAVING specified without GROUP BY
diff --git a/src/ql/src/test/results/clientnegative/index_bitmap_no_map_aggr.q.out b/src/ql/src/test/results/clientnegative/index_bitmap_no_map_aggr.q.out
index 7bb4f62..9eb5c12 100644
--- a/src/ql/src/test/results/clientnegative/index_bitmap_no_map_aggr.q.out
+++ b/src/ql/src/test/results/clientnegative/index_bitmap_no_map_aggr.q.out
@@ -19,4 +19,4 @@ PREHOOK: type: CREATEINDEX
 POSTHOOK: query: CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD
 POSTHOOK: type: CREATEINDEX
 POSTHOOK: Output: default@default__src_src1_index__
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot construct index without map-side aggregation
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot construct index without map-side aggregation
diff --git a/src/ql/src/test/results/clientnegative/index_compact_entry_limit.q.out b/src/ql/src/test/results/clientnegative/index_compact_entry_limit.q.out
index cea7de1..ff408b1 100644
--- a/src/ql/src/test/results/clientnegative/index_compact_entry_limit.q.out
+++ b/src/ql/src/test/results/clientnegative/index_compact_entry_limit.q.out
@@ -32,7 +32,7 @@ PREHOOK: query: SELECT key, value FROM src WHERE key=100 ORDER BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-Execution failed with exit status: 2
+Execution failed with exit status: 1
 Obtaining error information
 
 Task failed!
@@ -42,4 +42,4 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask
diff --git a/src/ql/src/test/results/clientnegative/index_compact_size_limit.q.out b/src/ql/src/test/results/clientnegative/index_compact_size_limit.q.out
index cea7de1..ff408b1 100644
--- a/src/ql/src/test/results/clientnegative/index_compact_size_limit.q.out
+++ b/src/ql/src/test/results/clientnegative/index_compact_size_limit.q.out
@@ -32,7 +32,7 @@ PREHOOK: query: SELECT key, value FROM src WHERE key=100 ORDER BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-Execution failed with exit status: 2
+Execution failed with exit status: 1
 Obtaining error information
 
 Task failed!
@@ -42,4 +42,4 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask
diff --git a/src/ql/src/test/results/clientnegative/input1.q.out b/src/ql/src/test/results/clientnegative/input1.q.out
index 7ce4204..1c1be74 100644
--- a/src/ql/src/test/results/clientnegative/input1.q.out
+++ b/src/ql/src/test/results/clientnegative/input1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Invalid table alias 'a'
+FAILED: SemanticException [Error 10009]: Line 1:7 Invalid table alias 'a'
diff --git a/src/ql/src/test/results/clientnegative/input2.q.out b/src/ql/src/test/results/clientnegative/input2.q.out
index f81cd1e..f7cdf6e 100644
--- a/src/ql/src/test/results/clientnegative/input2.q.out
+++ b/src/ql/src/test/results/clientnegative/input2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Invalid table alias or column reference 'a': (possible column names are: key, value)
+FAILED: SemanticException [Error 10004]: Line 1:7 Invalid table alias or column reference 'a': (possible column names are: key, value)
diff --git a/src/ql/src/test/results/clientnegative/input4.q.out b/src/ql/src/test/results/clientnegative/input4.q.out
index 0657740..eb17075 100644
--- a/src/ql/src/test/results/clientnegative/input4.q.out
+++ b/src/ql/src/test/results/clientnegative/input4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: In strict mode, cartesian product is not allowed. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException [Error 10052]: In strict mode, cartesian product is not allowed. If you really want to perform the operation, set hive.mapred.mode=nonstrict
diff --git a/src/ql/src/test/results/clientnegative/input41.q.out b/src/ql/src/test/results/clientnegative/input41.q.out
index 7c754b9..6995e6a 100644
--- a/src/ql/src/test/results/clientnegative/input41.q.out
+++ b/src/ql/src/test/results/clientnegative/input41.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Schema of both sides of union should match.
+FAILED: SemanticException Schema of both sides of union should match.
diff --git a/src/ql/src/test/results/clientnegative/input_part0_neg.q.out b/src/ql/src/test/results/clientnegative/input_part0_neg.q.out
index 2c9c0c5..4c717b7 100644
--- a/src/ql/src/test/results/clientnegative/input_part0_neg.q.out
+++ b/src/ql/src/test/results/clientnegative/input_part0_neg.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: No partition predicate found for Alias "x" Table "srcpart"
+FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "x" Table "srcpart"
diff --git a/src/ql/src/test/results/clientnegative/insert_view_failure.q.out b/src/ql/src/test/results/clientnegative/insert_view_failure.q.out
index e031f8b..067eb6f 100644
--- a/src/ql/src/test/results/clientnegative/insert_view_failure.q.out
+++ b/src/ql/src/test/results/clientnegative/insert_view_failure.q.out
@@ -9,4 +9,4 @@ POSTHOOK: query: CREATE VIEW xxx2 AS SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx2
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: A view cannot be used as target table for LOAD or INSERT
+FAILED: SemanticException [Error 10090]: A view cannot be used as target table for LOAD or INSERT
diff --git a/src/ql/src/test/results/clientnegative/insertexternal1.q.out b/src/ql/src/test/results/clientnegative/insertexternal1.q.out
index 4f499c0..c23d014 100644
--- a/src/ql/src/test/results/clientnegative/insertexternal1.q.out
+++ b/src/ql/src/test/results/clientnegative/insertexternal1.q.out
@@ -10,4 +10,4 @@ PREHOOK: Input: default@texternal
 POSTHOOK: type: ALTERTABLE_ADDPARTS
 POSTHOOK: Input: default@texternal
 POSTHOOK: Output: default@texternal@insertdate=2008-01-01
-FAILED: Error in semantic analysis: Inserting into a external table is not allowed texternal
+FAILED: SemanticException [Error 10071]: Inserting into a external table is not allowed texternal
diff --git a/src/ql/src/test/results/clientnegative/insertover_dynapart_ifnotexists.q.out b/src/ql/src/test/results/clientnegative/insertover_dynapart_ifnotexists.q.out
index 7516c2f..2b61605 100644
--- a/src/ql/src/test/results/clientnegative/insertover_dynapart_ifnotexists.q.out
+++ b/src/ql/src/test/results/clientnegative/insertover_dynapart_ifnotexists.q.out
@@ -15,4 +15,4 @@ POSTHOOK: query: load data local inpath '../data/files/srcbucket20.txt' INTO TAB
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@srcpart_dp
 POSTHOOK: Output: default@srcpart_dp@ds=2008-04-08/hr=11
-FAILED: Error in semantic analysis: Dynamic partitions do not support IF NOT EXISTS. Specified partitions with value : {ds=2008-04-08}
+FAILED: SemanticException [Error 10127]: Dynamic partitions do not support IF NOT EXISTS. Specified partitions with value : {ds=2008-04-08}
diff --git a/src/ql/src/test/results/clientnegative/invalid_avg_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_avg_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_avg_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_avg_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_1.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_1.q.out
index 6b72826..a3686d9 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_1.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_1.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tbl (a binary)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tbl
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
+FAILED: SemanticException Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToInteger with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_2.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_2.q.out
index cd10561..494b22c 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_2.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_2.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tbl (a binary)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tbl
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToByte with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
+FAILED: SemanticException Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToByte with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_3.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_3.q.out
index 6ca388c..2428f3c 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_3.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_3.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tbl (a binary)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tbl
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToShort with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
+FAILED: SemanticException Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToShort with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_4.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_4.q.out
index 606301f..25ec117 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_4.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_4.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tbl (a binary)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tbl
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToLong with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
+FAILED: SemanticException Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToLong with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_5.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_5.q.out
index c066ec9..6152e47 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_5.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_5.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tbl (a binary)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tbl
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToFloat with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
+FAILED: SemanticException Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToFloat with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_6.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_6.q.out
index ef3f10d..6eff980 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_6.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_from_binary_6.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tbl (a binary)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tbl
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToDouble with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(string)  _FUNC_(timestamp)  
+FAILED: SemanticException Line 0:-1 Wrong arguments 'a': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToDouble with (binary). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(float)  _FUNC_(string)  _FUNC_(timestamp)  
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_1.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_1.q.out
index ce29521..cc3b10d 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_1.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
+FAILED: SemanticException Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_2.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_2.q.out
index ce29521..cc3b10d 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_2.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
+FAILED: SemanticException Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_3.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_3.q.out
index ce29521..cc3b10d 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_3.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
+FAILED: SemanticException Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_4.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_4.q.out
index ce29521..cc3b10d 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_4.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
+FAILED: SemanticException Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_5.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_5.q.out
index ce29521..cc3b10d 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_5.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_5.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
+FAILED: SemanticException Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
diff --git a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_6.q.out b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_6.q.out
index ce29521..cc3b10d 100644
--- a/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_6.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_cast_to_binary_6.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
+FAILED: SemanticException Line 0:-1 Wrong arguments '2': Only string or binary data can be cast into binary data types.
diff --git a/src/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out b/src/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
index 18dd891..a8839fe 100644
--- a/src/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
+FAILED: SemanticException [Error 10099]: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
diff --git a/src/ql/src/test/results/clientnegative/invalid_create_tbl2.q.out b/src/ql/src/test/results/clientnegative/invalid_create_tbl2.q.out
index ae28a58..aae9503 100644
--- a/src/ql/src/test/results/clientnegative/invalid_create_tbl2.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_create_tbl2.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:7 Failed to recognize predicate 'tabl'. Failed rule: 'kwRole' in create role
+FAILED: ParseException line 1:7 Failed to recognize predicate 'tabl'. Failed rule: 'kwRole' in create role
 
diff --git a/src/ql/src/test/results/clientnegative/invalid_max_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_max_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_max_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_max_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalid_min_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_min_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_min_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_min_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalid_select_expression.q.out b/src/ql/src/test/results/clientnegative/invalid_select_expression.q.out
index 51ecb3d..a30b90f 100644
--- a/src/ql/src/test/results/clientnegative/invalid_select_expression.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_select_expression.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:32 cannot recognize input near '.' 'foo' '<EOF>' in expression specification
+FAILED: ParseException line 1:32 cannot recognize input near '.' 'foo' '<EOF>' in expression specification
 
diff --git a/src/ql/src/test/results/clientnegative/invalid_std_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_std_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_std_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_std_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalid_stddev_samp_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_stddev_samp_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_stddev_samp_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_stddev_samp_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalid_sum_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_sum_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_sum_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_sum_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalid_t_alter1.q.out b/src/ql/src/test/results/clientnegative/invalid_t_alter1.q.out
index a2e4f45..3513175 100644
--- a/src/ql/src/test/results/clientnegative/invalid_t_alter1.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_t_alter1.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE alter_test (d STRING)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@alter_test
-FAILED: Error in semantic analysis: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
+FAILED: SemanticException [Error 10099]: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
diff --git a/src/ql/src/test/results/clientnegative/invalid_t_alter2.q.out b/src/ql/src/test/results/clientnegative/invalid_t_alter2.q.out
index a2e4f45..3513175 100644
--- a/src/ql/src/test/results/clientnegative/invalid_t_alter2.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_t_alter2.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE alter_test (d STRING)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@alter_test
-FAILED: Error in semantic analysis: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
+FAILED: SemanticException [Error 10099]: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
diff --git a/src/ql/src/test/results/clientnegative/invalid_t_create1.q.out b/src/ql/src/test/results/clientnegative/invalid_t_create1.q.out
index 18dd891..a8839fe 100644
--- a/src/ql/src/test/results/clientnegative/invalid_t_create1.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_t_create1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
+FAILED: SemanticException [Error 10099]: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
diff --git a/src/ql/src/test/results/clientnegative/invalid_t_create2.q.out b/src/ql/src/test/results/clientnegative/invalid_t_create2.q.out
index 18dd891..a8839fe 100644
--- a/src/ql/src/test/results/clientnegative/invalid_t_create2.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_t_create2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
+FAILED: SemanticException [Error 10099]: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
diff --git a/src/ql/src/test/results/clientnegative/invalid_t_transform.q.out b/src/ql/src/test/results/clientnegative/invalid_t_transform.q.out
index 18dd891..a8839fe 100644
--- a/src/ql/src/test/results/clientnegative/invalid_t_transform.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_t_transform.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
+FAILED: SemanticException [Error 10099]: DATE and DATETIME types aren't supported yet. Please use TIMESTAMP instead
diff --git a/src/ql/src/test/results/clientnegative/invalid_tbl_name.q.out b/src/ql/src/test/results/clientnegative/invalid_tbl_name.q.out
index 6556749..c144fbe 100644
--- a/src/ql/src/test/results/clientnegative/invalid_tbl_name.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_tbl_name.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:20 cannot recognize input near '-' 'name' '(' in create table statement
+FAILED: ParseException line 1:20 cannot recognize input near '-' 'name' '(' in create table statement
 
diff --git a/src/ql/src/test/results/clientnegative/invalid_var_samp_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_var_samp_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_var_samp_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_var_samp_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalid_variance_syntax.q.out b/src/ql/src/test/results/clientnegative/invalid_variance_syntax.q.out
index d9886ad..28d65d7 100644
--- a/src/ql/src/test/results/clientnegative/invalid_variance_syntax.q.out
+++ b/src/ql/src/test/results/clientnegative/invalid_variance_syntax.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The specified syntax for UDAF invocation is invalid.
+FAILED: SemanticException The specified syntax for UDAF invocation is invalid.
diff --git a/src/ql/src/test/results/clientnegative/invalidate_view1.q.out b/src/ql/src/test/results/clientnegative/invalidate_view1.q.out
index 94faa3e..400c697 100644
--- a/src/ql/src/test/results/clientnegative/invalidate_view1.q.out
+++ b/src/ql/src/test/results/clientnegative/invalidate_view1.q.out
@@ -41,7 +41,7 @@ POSTHOOK: query: ALTER TABLE xxx10 REPLACE COLUMNS (key int)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@xxx10
 POSTHOOK: Output: default@xxx10
-FAILED: Error in semantic analysis: Line 1:30 Invalid column reference '`value`' in definition of VIEW xxx9 [
+FAILED: SemanticException Line 1:30 Invalid column reference '`value`' in definition of VIEW xxx9 [
 SELECT `xxx10`.`key`, `xxx10`.`value` FROM `default`.`xxx10`
 ] used as xxx at Line 1:39 in definition of VIEW xxx8 [
 SELECT `xxx`.`key`, `xxx`.`value` FROM `default`.`xxx9` `xxx`
diff --git a/src/ql/src/test/results/clientnegative/join2.q.out b/src/ql/src/test/results/clientnegative/join2.q.out
index 246b2bb..b53b3a1 100644
--- a/src/ql/src/test/results/clientnegative/join2.q.out
+++ b/src/ql/src/test/results/clientnegative/join2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: MAPJOIN cannot be performed with OUTER JOIN
+FAILED: SemanticException [Error 10057]: MAPJOIN cannot be performed with OUTER JOIN
diff --git a/src/ql/src/test/results/clientnegative/joinneg.q.out b/src/ql/src/test/results/clientnegative/joinneg.q.out
index 7dd8354..1d75f41 100644
--- a/src/ql/src/test/results/clientnegative/joinneg.q.out
+++ b/src/ql/src/test/results/clientnegative/joinneg.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 5:12 Invalid table alias 'b'
+FAILED: SemanticException [Error 10009]: Line 5:12 Invalid table alias 'b'
diff --git a/src/ql/src/test/results/clientnegative/lateral_view_alias.q.out b/src/ql/src/test/results/clientnegative/lateral_view_alias.q.out
index e50f673..68e918b 100644
--- a/src/ql/src/test/results/clientnegative/lateral_view_alias.q.out
+++ b/src/ql/src/test/results/clientnegative/lateral_view_alias.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 1 aliases but got 2
+FAILED: SemanticException [Error 10083]: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 1 aliases but got 2
diff --git a/src/ql/src/test/results/clientnegative/lateral_view_join.q.out b/src/ql/src/test/results/clientnegative/lateral_view_join.q.out
index c9311cd..4421436 100644
--- a/src/ql/src/test/results/clientnegative/lateral_view_join.q.out
+++ b/src/ql/src/test/results/clientnegative/lateral_view_join.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:59 mismatched input 'AS' expecting Identifier near ')' in table alias
+FAILED: ParseException line 1:59 mismatched input 'AS' expecting Identifier near ')' in table alias
 
diff --git a/src/ql/src/test/results/clientnegative/line_terminator.q.out b/src/ql/src/test/results/clientnegative/line_terminator.q.out
index b493d2d..adae454 100644
--- a/src/ql/src/test/results/clientnegative/line_terminator.q.out
+++ b/src/ql/src/test/results/clientnegative/line_terminator.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: 3:20 LINES TERMINATED BY only supports newline '\n' right now. Error encountered near token '',''
+FAILED: SemanticException 3:20 LINES TERMINATED BY only supports newline '\n' right now. Error encountered near token '',''
diff --git a/src/ql/src/test/results/clientnegative/load_non_native.q.out b/src/ql/src/test/results/clientnegative/load_non_native.q.out
index 400466d..cd0c810 100644
--- a/src/ql/src/test/results/clientnegative/load_non_native.q.out
+++ b/src/ql/src/test/results/clientnegative/load_non_native.q.out
@@ -5,4 +5,4 @@ POSTHOOK: query: CREATE TABLE non_native2(key int, value string)
 STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler'
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@non_native2
-FAILED: Error in semantic analysis: A non-native table cannot be used as target for LOAD
+FAILED: SemanticException [Error 10101]: A non-native table cannot be used as target for LOAD
diff --git a/src/ql/src/test/results/clientnegative/load_part_nospec.q.out b/src/ql/src/test/results/clientnegative/load_part_nospec.q.out
index 20d425a..edb84ce 100644
--- a/src/ql/src/test/results/clientnegative/load_part_nospec.q.out
+++ b/src/ql/src/test/results/clientnegative/load_part_nospec.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table hive_test_src ( col1 string ) partitioned by (pcol1 string) stored as textfile
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@hive_test_src
-FAILED: Error in semantic analysis: Need to specify partition columns because the destination table is partitioned
+FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned
diff --git a/src/ql/src/test/results/clientnegative/load_view_failure.q.out b/src/ql/src/test/results/clientnegative/load_view_failure.q.out
index 3daab56..bb2ecfe 100644
--- a/src/ql/src/test/results/clientnegative/load_view_failure.q.out
+++ b/src/ql/src/test/results/clientnegative/load_view_failure.q.out
@@ -9,4 +9,4 @@ POSTHOOK: query: CREATE VIEW xxx11 AS SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx11
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: A view cannot be used as target table for LOAD or INSERT
+FAILED: SemanticException [Error 10090]: A view cannot be used as target table for LOAD or INSERT
diff --git a/src/ql/src/test/results/clientnegative/load_wrong_noof_part.q.out b/src/ql/src/test/results/clientnegative/load_wrong_noof_part.q.out
index e785322..d05e57b 100644
--- a/src/ql/src/test/results/clientnegative/load_wrong_noof_part.q.out
+++ b/src/ql/src/test/results/clientnegative/load_wrong_noof_part.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE loadpart1(a STRING, b STRING) PARTITIONED BY (ds STRING,ds1 STRING)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@loadpart1
-FAILED: Error in semantic analysis: Line 2:79 Partition not found ''2009-05-05''
+FAILED: SemanticException [Error 10006]: Line 2:79 Partition not found ''2009-05-05''
diff --git a/src/ql/src/test/results/clientnegative/local_mapred_error_cache.q.out b/src/ql/src/test/results/clientnegative/local_mapred_error_cache.q.out
index caec723..4e39b7b 100644
--- a/src/ql/src/test/results/clientnegative/local_mapred_error_cache.q.out
+++ b/src/ql/src/test/results/clientnegative/local_mapred_error_cache.q.out
@@ -13,11 +13,11 @@ Logs:
 
 #### A masked pattern was here ####
 ID: Stage-1
-org.apache.hadoop.hive.ql.metadata.HiveException: Hit error while closing ..
+org.apache.hadoop.hive.ql.metadata.HiveException: [Error 20003]: An error occurred when trying to close the Operator running your custom script.
 #### A masked pattern was here ####
-org.apache.hadoop.hive.ql.metadata.HiveException: Hit error while closing ..
+org.apache.hadoop.hive.ql.metadata.HiveException: [Error 20003]: An error occurred when trying to close the Operator running your custom script.
 #### A masked pattern was here ####
-org.apache.hadoop.hive.ql.metadata.HiveException: Hit error while closing ..
+org.apache.hadoop.hive.ql.metadata.HiveException: [Error 20003]: An error occurred when trying to close the Operator running your custom script.
 #### A masked pattern was here ####
 Ended Job = job_local_0001 with errors
 Error during job, obtaining debugging information...
diff --git a/src/ql/src/test/results/clientnegative/mapreduce_stack_trace.q.out b/src/ql/src/test/results/clientnegative/mapreduce_stack_trace.q.out
index b199241..592ecda 100644
--- a/src/ql/src/test/results/clientnegative/mapreduce_stack_trace.q.out
+++ b/src/ql/src/test/results/clientnegative/mapreduce_stack_trace.q.out
@@ -10,4 +10,4 @@ FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime
 Hive Runtime Error while processing row {"key":"238","value":"val_238"}
 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {"key":"238","value":"val_238"}
 Hive Runtime Error while processing row {"key":"238","value":"val_238"}
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
+FAILED: Execution Error, return code 20000 from org.apache.hadoop.hive.ql.exec.MapRedTask. Unable to initialize custom script.
diff --git a/src/ql/src/test/results/clientnegative/mapreduce_stack_trace_turnoff.q.out b/src/ql/src/test/results/clientnegative/mapreduce_stack_trace_turnoff.q.out
index da9203d..e1cd0c5 100644
--- a/src/ql/src/test/results/clientnegative/mapreduce_stack_trace_turnoff.q.out
+++ b/src/ql/src/test/results/clientnegative/mapreduce_stack_trace_turnoff.q.out
@@ -2,4 +2,4 @@ PREHOOK: query: FROM src SELECT TRANSFORM(key, value) USING 'script_does_not_exi
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
+FAILED: Execution Error, return code 20000 from org.apache.hadoop.hive.ql.exec.MapRedTask. Unable to initialize custom script.
diff --git a/src/ql/src/test/results/clientnegative/merge_negative_1.q.out b/src/ql/src/test/results/clientnegative/merge_negative_1.q.out
index 53b920c..82c02df 100644
--- a/src/ql/src/test/results/clientnegative/merge_negative_1.q.out
+++ b/src/ql/src/test/results/clientnegative/merge_negative_1.q.out
@@ -8,4 +8,4 @@ PREHOOK: type: CREATEINDEX
 POSTHOOK: query: CREATE INDEX src_index_merge_test ON TABLE src2(key) as 'COMPACT' WITH DEFERRED REBUILD
 POSTHOOK: type: CREATEINDEX
 POSTHOOK: Output: default@default__src2_src_index_merge_test__
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: can not do merge because source table src2 is indexed.
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: can not do merge because source table src2 is indexed.
diff --git a/src/ql/src/test/results/clientnegative/merge_negative_2.q.out b/src/ql/src/test/results/clientnegative/merge_negative_2.q.out
index 171ed82..a1cec5b 100644
--- a/src/ql/src/test/results/clientnegative/merge_negative_2.q.out
+++ b/src/ql/src/test/results/clientnegative/merge_negative_2.q.out
@@ -13,4 +13,4 @@ POSTHOOK: Input: default@src
 POSTHOOK: Output: default@srcpart2@ds=2011
 POSTHOOK: Lineage: srcpart2 PARTITION(ds=2011).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: srcpart2 PARTITION(ds=2011).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: source table srcpart2 is partitioned but no partition desc found.
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: source table srcpart2 is partitioned but no partition desc found.
diff --git a/src/ql/src/test/results/clientnegative/minimr_broken_pipe.q.out b/src/ql/src/test/results/clientnegative/minimr_broken_pipe.q.out
index 037f00b..b4d51b4 100644
--- a/src/ql/src/test/results/clientnegative/minimr_broken_pipe.q.out
+++ b/src/ql/src/test/results/clientnegative/minimr_broken_pipe.q.out
@@ -3,4 +3,4 @@ SELECT TRANSFORM(*) USING 'true' AS a, b, c FROM (SELECT * FROM src LIMIT 1) tmp
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
+FAILED: Execution Error, return code 20003 from org.apache.hadoop.hive.ql.exec.MapRedTask. An error occurred when trying to close the Operator running your custom script.
diff --git a/src/ql/src/test/results/clientnegative/no_matching_udf.q.out b/src/ql/src/test/results/clientnegative/no_matching_udf.q.out
index 6e97958..cd3e72d 100644
--- a/src/ql/src/test/results/clientnegative/no_matching_udf.q.out
+++ b/src/ql/src/test/results/clientnegative/no_matching_udf.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: No matching method for class org.apache.hadoop.hive.ql.udf.UDAFPercentile with (double, double). Possible choices: _FUNC_(bigint, array<double>)  _FUNC_(bigint, double)  
+FAILED: NoMatchingMethodException No matching method for class org.apache.hadoop.hive.ql.udf.UDAFPercentile with (double, double). Possible choices: _FUNC_(bigint, array<double>)  _FUNC_(bigint, double)  
diff --git a/src/ql/src/test/results/clientnegative/nonkey_groupby.q.out b/src/ql/src/test/results/clientnegative/nonkey_groupby.q.out
index 062250f..bbe928c 100644
--- a/src/ql/src/test/results/clientnegative/nonkey_groupby.q.out
+++ b/src/ql/src/test/results/clientnegative/nonkey_groupby.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:15 Expression not in GROUP BY key 'key'
+FAILED: SemanticException [Error 10025]: Line 1:15 Expression not in GROUP BY key 'key'
diff --git a/src/ql/src/test/results/clientnegative/nopart_insert.q.out b/src/ql/src/test/results/clientnegative/nopart_insert.q.out
index 1f684f3..28a0762 100644
--- a/src/ql/src/test/results/clientnegative/nopart_insert.q.out
+++ b/src/ql/src/test/results/clientnegative/nopart_insert.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE nopart_insert(a STRING, b STRING) PARTITIONED BY (ds STRING)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@nopart_insert
-FAILED: Error in semantic analysis: 3:23 Need to specify partition columns because the destination table is partitioned. Error encountered near token 'nopart_insert'
+FAILED: SemanticException 3:23 Need to specify partition columns because the destination table is partitioned. Error encountered near token 'nopart_insert'
diff --git a/src/ql/src/test/results/clientnegative/nopart_load.q.out b/src/ql/src/test/results/clientnegative/nopart_load.q.out
index 804748d..49eb120 100644
--- a/src/ql/src/test/results/clientnegative/nopart_load.q.out
+++ b/src/ql/src/test/results/clientnegative/nopart_load.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE nopart_load(a STRING, b STRING) PARTITIONED BY (ds STRING)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@nopart_load
-FAILED: Error in semantic analysis: Need to specify partition columns because the destination table is partitioned
+FAILED: SemanticException [Error 10062]: Need to specify partition columns because the destination table is partitioned
diff --git a/src/ql/src/test/results/clientnegative/notable_alias3.q.out b/src/ql/src/test/results/clientnegative/notable_alias3.q.out
index 02d4297..cadca6e 100644
--- a/src/ql/src/test/results/clientnegative/notable_alias3.q.out
+++ b/src/ql/src/test/results/clientnegative/notable_alias3.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: Line 4:44 Expression not in GROUP BY key 'key'
+FAILED: SemanticException [Error 10025]: Line 4:44 Expression not in GROUP BY key 'key'
diff --git a/src/ql/src/test/results/clientnegative/notable_alias4.q.out b/src/ql/src/test/results/clientnegative/notable_alias4.q.out
index 7ad3f86..7c33af4 100644
--- a/src/ql/src/test/results/clientnegative/notable_alias4.q.out
+++ b/src/ql/src/test/results/clientnegative/notable_alias4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Column key Found in more than One Tables/Subqueries
+FAILED: SemanticException Column key Found in more than One Tables/Subqueries
diff --git a/src/ql/src/test/results/clientnegative/orderbysortby.q.out b/src/ql/src/test/results/clientnegative/orderbysortby.q.out
index 10122a9..40a0b5b 100644
--- a/src/ql/src/test/results/clientnegative/orderbysortby.q.out
+++ b/src/ql/src/test/results/clientnegative/orderbysortby.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: 8:8 Cannot have both ORDER BY and SORT BY clauses. Error encountered near token 'one'
+FAILED: SemanticException 8:8 Cannot have both ORDER BY and SORT BY clauses. Error encountered near token 'one'
diff --git a/src/ql/src/test/results/clientnegative/part_col_complex_type.q.out b/src/ql/src/test/results/clientnegative/part_col_complex_type.q.out
index 9cf2843..9af107b 100644
--- a/src/ql/src/test/results/clientnegative/part_col_complex_type.q.out
+++ b/src/ql/src/test/results/clientnegative/part_col_complex_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Partition column must be of primitive type. Found b of type: map<string,string>
+FAILED: SemanticException [Error 10126]: Partition column must be of primitive type. Found b of type: map<string,string>
diff --git a/src/ql/src/test/results/clientnegative/protectmode_part.q.out b/src/ql/src/test/results/clientnegative/protectmode_part.q.out
index 78d095d..408eee1 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_part.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_part.q.out
@@ -58,4 +58,4 @@ POSTHOOK: query: select * from tbl_protectmode3 where p='p2'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tbl_protectmode3@p=p2
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Query against an offline table or partition Table tbl_protectmode3 Partition p=p1
+FAILED: SemanticException [Error 10113]: Query against an offline table or partition Table tbl_protectmode3 Partition p=p1
diff --git a/src/ql/src/test/results/clientnegative/protectmode_part1.q.out b/src/ql/src/test/results/clientnegative/protectmode_part1.q.out
index c6ed16a..16175ac 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_part1.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_part1.q.out
@@ -77,4 +77,4 @@ POSTHOOK: Output: default@tbl_protectmode5_1
 POSTHOOK: Lineage: tbl_protectmode5_1.col SIMPLE [(tbl_protectmode5)tbl_protectmode5.FieldSchema(name:col, type:string, comment:null), ]
 POSTHOOK: Lineage: tbl_protectmode5_1.col SIMPLE [(tbl_protectmode5)tbl_protectmode5.FieldSchema(name:col, type:string, comment:null), ]
 POSTHOOK: Lineage: tbl_protectmode5_1.col SIMPLE [(tbl_protectmode5)tbl_protectmode5.FieldSchema(name:col, type:string, comment:null), ]
-FAILED: Error in semantic analysis: Query against an offline table or partition Table tbl_protectmode5 Partition p=p1
+FAILED: SemanticException [Error 10113]: Query against an offline table or partition Table tbl_protectmode5 Partition p=p1
diff --git a/src/ql/src/test/results/clientnegative/protectmode_part2.q.out b/src/ql/src/test/results/clientnegative/protectmode_part2.q.out
index 1e8fe8c..dd1940d 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_part2.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_part2.q.out
@@ -33,4 +33,4 @@ POSTHOOK: type: ALTERPARTITION_PROTECTMODE
 POSTHOOK: Input: default@tbl_protectmode6
 POSTHOOK: Input: default@tbl_protectmode6@p=p1
 POSTHOOK: Output: default@tbl_protectmode6@p=p1
-FAILED: Error in semantic analysis: org.apache.hadoop.hive.ql.parse.SemanticException: Query against an offline table or partition tbl_protectmode6:p=p1
+FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Query against an offline table or partition tbl_protectmode6:p=p1
diff --git a/src/ql/src/test/results/clientnegative/protectmode_tbl1.q.out b/src/ql/src/test/results/clientnegative/protectmode_tbl1.q.out
index dd323ad..c8c039a 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_tbl1.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_tbl1.q.out
@@ -27,4 +27,4 @@ POSTHOOK: query: alter table tbl_protectmode_1 enable offline
 POSTHOOK: type: ALTERTABLE_PROTECTMODE
 POSTHOOK: Input: default@tbl_protectmode_1
 POSTHOOK: Output: default@tbl_protectmode_1
-FAILED: Error in semantic analysis: Query against an offline table or partition Table tbl_protectmode_1
+FAILED: SemanticException [Error 10113]: Query against an offline table or partition Table tbl_protectmode_1
diff --git a/src/ql/src/test/results/clientnegative/protectmode_tbl2.q.out b/src/ql/src/test/results/clientnegative/protectmode_tbl2.q.out
index fa3625a..844f055 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_tbl2.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_tbl2.q.out
@@ -50,4 +50,4 @@ col	string
 p	string	
 	 	 
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Query against an offline table or partition Table tbl_protectmode2
+FAILED: SemanticException [Error 10113]: Query against an offline table or partition Table tbl_protectmode2
diff --git a/src/ql/src/test/results/clientnegative/protectmode_tbl3.q.out b/src/ql/src/test/results/clientnegative/protectmode_tbl3.q.out
index 9d232dd..c625cc0 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_tbl3.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_tbl3.q.out
@@ -34,4 +34,4 @@ POSTHOOK: type: DESCTABLE
 col	string	
 	 	 
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Query against an offline table or partition Table tbl_protectmode_4
+FAILED: SemanticException [Error 10113]: Query against an offline table or partition Table tbl_protectmode_4
diff --git a/src/ql/src/test/results/clientnegative/protectmode_tbl4.q.out b/src/ql/src/test/results/clientnegative/protectmode_tbl4.q.out
index daf351a..3c8902d 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_tbl4.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_tbl4.q.out
@@ -59,4 +59,4 @@ col	string
 p	string	
 	 	 
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Query against an offline table or partition Table tbl_protectmode_tbl4
+FAILED: SemanticException [Error 10113]: Query against an offline table or partition Table tbl_protectmode_tbl4
diff --git a/src/ql/src/test/results/clientnegative/protectmode_tbl5.q.out b/src/ql/src/test/results/clientnegative/protectmode_tbl5.q.out
index 814caed..9825f84 100644
--- a/src/ql/src/test/results/clientnegative/protectmode_tbl5.q.out
+++ b/src/ql/src/test/results/clientnegative/protectmode_tbl5.q.out
@@ -59,4 +59,4 @@ col	string
 p	string	
 	 	 
 #### A masked pattern was here ####
-FAILED: Error in semantic analysis: Query against an offline table or partition Table tbl_protectmode_tbl5
+FAILED: SemanticException [Error 10113]: Query against an offline table or partition Table tbl_protectmode_tbl5
diff --git a/src/ql/src/test/results/clientnegative/recursive_view.q.out b/src/ql/src/test/results/clientnegative/recursive_view.q.out
index a82b17d..a65ed8b 100644
--- a/src/ql/src/test/results/clientnegative/recursive_view.q.out
+++ b/src/ql/src/test/results/clientnegative/recursive_view.q.out
@@ -84,4 +84,4 @@ POSTHOOK: type: null
 POSTHOOK: Input: default@r3
 POSTHOOK: Output: default@r0
 POSTHOOK: Output: default@r3
-FAILED: Error in semantic analysis: Recursive view default.r0 detected (cycle: default.r0 -> default.r2 -> default.r1 -> default.r0).
+FAILED: SemanticException Recursive view default.r0 detected (cycle: default.r0 -> default.r2 -> default.r1 -> default.r0).
diff --git a/src/ql/src/test/results/clientnegative/regex_col_1.q.out b/src/ql/src/test/results/clientnegative/regex_col_1.q.out
index d0c8c33..2025aee 100644
--- a/src/ql/src/test/results/clientnegative/regex_col_1.q.out
+++ b/src/ql/src/test/results/clientnegative/regex_col_1.q.out
@@ -1,3 +1,3 @@
-FAILED: Error in semantic analysis: Line 2:7 Invalid column reference '`+++`': Dangling meta character '+' near index 0
+FAILED: SemanticException Line 2:7 Invalid column reference '`+++`': Dangling meta character '+' near index 0
 +++
 ^
diff --git a/src/ql/src/test/results/clientnegative/regex_col_2.q.out b/src/ql/src/test/results/clientnegative/regex_col_2.q.out
index 2261b3a..171a66f 100644
--- a/src/ql/src/test/results/clientnegative/regex_col_2.q.out
+++ b/src/ql/src/test/results/clientnegative/regex_col_2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:7 Invalid column reference '`.a.`'
+FAILED: SemanticException [Error 10002]: Line 2:7 Invalid column reference '`.a.`'
diff --git a/src/ql/src/test/results/clientnegative/regex_col_groupby.q.out b/src/ql/src/test/results/clientnegative/regex_col_groupby.q.out
index 05ade35..0730f14 100644
--- a/src/ql/src/test/results/clientnegative/regex_col_groupby.q.out
+++ b/src/ql/src/test/results/clientnegative/regex_col_groupby.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:44 Invalid table alias or column reference '`..`': (possible column names are: key, value, ds, hr)
+FAILED: SemanticException [Error 10004]: Line 2:44 Invalid table alias or column reference '`..`': (possible column names are: key, value, ds, hr)
diff --git a/src/ql/src/test/results/clientnegative/sample.q.out b/src/ql/src/test/results/clientnegative/sample.q.out
index 02146b9..001440d 100644
--- a/src/ql/src/test/results/clientnegative/sample.q.out
+++ b/src/ql/src/test/results/clientnegative/sample.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Numberator should not be bigger than denaminator in sample clause for table srcbucket
+FAILED: SemanticException [Error 10061]: Numberator should not be bigger than denaminator in sample clause for table srcbucket
diff --git a/src/ql/src/test/results/clientnegative/select_charliteral.q.out b/src/ql/src/test/results/clientnegative/select_charliteral.q.out
index 52d41b4..6ba0e20 100644
--- a/src/ql/src/test/results/clientnegative/select_charliteral.q.out
+++ b/src/ql/src/test/results/clientnegative/select_charliteral.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 3:11 mismatched input ',' expecting \' near '_c17' in character string literal
+FAILED: ParseException line 3:11 mismatched input ',' expecting \' near '_c17' in character string literal
 
diff --git a/src/ql/src/test/results/clientnegative/select_udtf_alias.q.out b/src/ql/src/test/results/clientnegative/select_udtf_alias.q.out
index 77d3d9c..9cc7f2b 100644
--- a/src/ql/src/test/results/clientnegative/select_udtf_alias.q.out
+++ b/src/ql/src/test/results/clientnegative/select_udtf_alias.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 3:49 mismatched input 'LIMIT' expecting FROM near ')' in from clause
+FAILED: ParseException line 3:49 mismatched input 'LIMIT' expecting FROM near ')' in from clause
 
diff --git a/src/ql/src/test/results/clientnegative/semijoin1.q.out b/src/ql/src/test/results/clientnegative/semijoin1.q.out
index 5320df5..21031a9 100644
--- a/src/ql/src/test/results/clientnegative/semijoin1.q.out
+++ b/src/ql/src/test/results/clientnegative/semijoin1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:7 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
+FAILED: SemanticException [Error 10004]: Line 2:7 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
diff --git a/src/ql/src/test/results/clientnegative/semijoin2.q.out b/src/ql/src/test/results/clientnegative/semijoin2.q.out
index f52b3bf..8c9d5f6 100644
--- a/src/ql/src/test/results/clientnegative/semijoin2.q.out
+++ b/src/ql/src/test/results/clientnegative/semijoin2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:70 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
+FAILED: SemanticException [Error 10004]: Line 2:70 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
diff --git a/src/ql/src/test/results/clientnegative/semijoin3.q.out b/src/ql/src/test/results/clientnegative/semijoin3.q.out
index 902298b..ba5f5fc 100644
--- a/src/ql/src/test/results/clientnegative/semijoin3.q.out
+++ b/src/ql/src/test/results/clientnegative/semijoin3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:67 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
+FAILED: SemanticException [Error 10004]: Line 2:67 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
diff --git a/src/ql/src/test/results/clientnegative/semijoin4.q.out b/src/ql/src/test/results/clientnegative/semijoin4.q.out
index fd5d232..25ac9bf 100644
--- a/src/ql/src/test/results/clientnegative/semijoin4.q.out
+++ b/src/ql/src/test/results/clientnegative/semijoin4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:112 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
+FAILED: SemanticException [Error 10004]: Line 2:112 Invalid table alias or column reference 'b': (possible column names are: _col0, _col1)
diff --git a/src/ql/src/test/results/clientnegative/show_tables_bad1.q.out b/src/ql/src/test/results/clientnegative/show_tables_bad1.q.out
index 9e583a3..0ae3af6 100644
--- a/src/ql/src/test/results/clientnegative/show_tables_bad1.q.out
+++ b/src/ql/src/test/results/clientnegative/show_tables_bad1.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:12 mismatched input '<EOF>' expecting set null in Identifier for show statement
+FAILED: ParseException line 1:12 mismatched input '<EOF>' expecting set null in Identifier for show statement
 
diff --git a/src/ql/src/test/results/clientnegative/show_tables_bad2.q.out b/src/ql/src/test/results/clientnegative/show_tables_bad2.q.out
index f983769..331deb3 100644
--- a/src/ql/src/test/results/clientnegative/show_tables_bad2.q.out
+++ b/src/ql/src/test/results/clientnegative/show_tables_bad2.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:25 mismatched input '<EOF>' expecting set null in Identifier for show statement
+FAILED: ParseException line 1:25 mismatched input '<EOF>' expecting set null in Identifier for show statement
 
diff --git a/src/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out b/src/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out
index aac0388..7a5b8c1 100644
--- a/src/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out
+++ b/src/ql/src/test/results/clientnegative/smb_bucketmapjoin.q.out
@@ -34,4 +34,4 @@ POSTHOOK: Lineage: smb_bucket4_1.key EXPRESSION [(src)src.FieldSchema(name:key,
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-FAILED: Error in semantic analysis: MAPJOIN cannot be performed with OUTER JOIN
+FAILED: SemanticException [Error 10057]: MAPJOIN cannot be performed with OUTER JOIN
diff --git a/src/ql/src/test/results/clientnegative/split_sample_out_of_range.q.out b/src/ql/src/test/results/clientnegative/split_sample_out_of_range.q.out
index 48bc25e..54974a7 100644
--- a/src/ql/src/test/results/clientnegative/split_sample_out_of_range.q.out
+++ b/src/ql/src/test/results/clientnegative/split_sample_out_of_range.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: 3:32 Sampling percentage should be between 0 and 100. Error encountered near token '105'
+FAILED: SemanticException 3:32 Sampling percentage should be between 0 and 100. Error encountered near token '105'
diff --git a/src/ql/src/test/results/clientnegative/split_sample_wrong_format.q.out b/src/ql/src/test/results/clientnegative/split_sample_wrong_format.q.out
index 0c31a13..eb6a81d 100644
--- a/src/ql/src/test/results/clientnegative/split_sample_wrong_format.q.out
+++ b/src/ql/src/test/results/clientnegative/split_sample_wrong_format.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: 3:32 Percentage sampling is not supported in org.apache.hadoop.hive.ql.io.HiveInputFormat. Error encountered near token '1'
+FAILED: SemanticException 3:32 Percentage sampling is not supported in org.apache.hadoop.hive.ql.io.HiveInputFormat. Error encountered near token '1'
diff --git a/src/ql/src/test/results/clientnegative/strict_join.q.out b/src/ql/src/test/results/clientnegative/strict_join.q.out
index 0657740..eb17075 100644
--- a/src/ql/src/test/results/clientnegative/strict_join.q.out
+++ b/src/ql/src/test/results/clientnegative/strict_join.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: In strict mode, cartesian product is not allowed. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException [Error 10052]: In strict mode, cartesian product is not allowed. If you really want to perform the operation, set hive.mapred.mode=nonstrict
diff --git a/src/ql/src/test/results/clientnegative/strict_orderby.q.out b/src/ql/src/test/results/clientnegative/strict_orderby.q.out
index 8b37124..82d1f9c 100644
--- a/src/ql/src/test/results/clientnegative/strict_orderby.q.out
+++ b/src/ql/src/test/results/clientnegative/strict_orderby.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: 4:47 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'key'
+FAILED: SemanticException 4:47 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'key'
diff --git a/src/ql/src/test/results/clientnegative/strict_pruning.q.out b/src/ql/src/test/results/clientnegative/strict_pruning.q.out
index eacd8e1..ff50b28 100644
--- a/src/ql/src/test/results/clientnegative/strict_pruning.q.out
+++ b/src/ql/src/test/results/clientnegative/strict_pruning.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: No partition predicate found for Alias "srcpart" Table "srcpart"
+FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "srcpart" Table "srcpart"
diff --git a/src/ql/src/test/results/clientnegative/subq_insert.q.out b/src/ql/src/test/results/clientnegative/subq_insert.q.out
index eaf85cb..620409b 100644
--- a/src/ql/src/test/results/clientnegative/subq_insert.q.out
+++ b/src/ql/src/test/results/clientnegative/subq_insert.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:38 Cannot insert in a subquery. Inserting to table  'src1'
+FAILED: SemanticException [Error 10024]: Line 2:38 Cannot insert in a subquery. Inserting to table  'src1'
diff --git a/src/ql/src/test/results/clientnegative/udaf_invalid_place.q.out b/src/ql/src/test/results/clientnegative/udaf_invalid_place.q.out
index f663f4d..6727889 100644
--- a/src/ql/src/test/results/clientnegative/udaf_invalid_place.q.out
+++ b/src/ql/src/test/results/clientnegative/udaf_invalid_place.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:21 Not yet supported place for UDAF 'sum'
+FAILED: SemanticException [Error 10128]: Line 1:21 Not yet supported place for UDAF 'sum'
diff --git a/src/ql/src/test/results/clientnegative/udf_array_contains_wrong1.q.out b/src/ql/src/test/results/clientnegative/udf_array_contains_wrong1.q.out
index 644247c..00cb3ed 100644
--- a/src/ql/src/test/results/clientnegative/udf_array_contains_wrong1.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_array_contains_wrong1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:22 Argument type mismatch '1': "array" expected at function ARRAY_CONTAINS, but "int" is found
+FAILED: SemanticException [Error 10016]: Line 2:22 Argument type mismatch '1': "array" expected at function ARRAY_CONTAINS, but "int" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_array_contains_wrong2.q.out b/src/ql/src/test/results/clientnegative/udf_array_contains_wrong2.q.out
index 3e6be58..507d322 100644
--- a/src/ql/src/test/results/clientnegative/udf_array_contains_wrong2.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_array_contains_wrong2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:38 Argument type mismatch ''2'': "int" expected at function ARRAY_CONTAINS, but "string" is found
+FAILED: SemanticException [Error 10016]: Line 2:38 Argument type mismatch ''2'': "int" expected at function ARRAY_CONTAINS, but "string" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_case_type_wrong.q.out b/src/ql/src/test/results/clientnegative/udf_case_type_wrong.q.out
index 045513c..3314002 100644
--- a/src/ql/src/test/results/clientnegative/udf_case_type_wrong.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_case_type_wrong.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:13 Argument type mismatch '1': The expressions after WHEN should have the same type with that after CASE: "string" is expected but "int" is found
+FAILED: SemanticException [Error 10016]: Line 2:13 Argument type mismatch '1': The expressions after WHEN should have the same type with that after CASE: "string" is expected but "int" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_case_type_wrong2.q.out b/src/ql/src/test/results/clientnegative/udf_case_type_wrong2.q.out
index 0a8b434..5be7d66 100644
--- a/src/ql/src/test/results/clientnegative/udf_case_type_wrong2.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_case_type_wrong2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 3:20 Argument type mismatch '4': The expressions after THEN should have the same type: "string" is expected but "int" is found
+FAILED: SemanticException [Error 10016]: Line 3:20 Argument type mismatch '4': The expressions after THEN should have the same type: "string" is expected but "int" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_case_type_wrong3.q.out b/src/ql/src/test/results/clientnegative/udf_case_type_wrong3.q.out
index 14e68ea..901d9a9 100644
--- a/src/ql/src/test/results/clientnegative/udf_case_type_wrong3.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_case_type_wrong3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 4:13 Argument type mismatch '7': The expression after ELSE should have the same type as those after THEN: "string" is expected but "int" is found
+FAILED: SemanticException [Error 10016]: Line 4:13 Argument type mismatch '7': The expression after ELSE should have the same type as those after THEN: "string" is expected but "int" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_coalesce.q.out b/src/ql/src/test/results/clientnegative/udf_coalesce.q.out
index 10c72b6..249a83d 100644
--- a/src/ql/src/test/results/clientnegative/udf_coalesce.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_coalesce.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:33 Argument type mismatch ''2.0'': The expressions after COALESCE should all have the same type: "array<string>" is expected but "string" is found
+FAILED: SemanticException [Error 10016]: Line 1:33 Argument type mismatch ''2.0'': The expressions after COALESCE should all have the same type: "array<string>" is expected but "string" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong1.q.out b/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong1.q.out
index 5fa9177..ea31b90 100644
--- a/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong1.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:7 Arguments length mismatch ''-'': The function CONCAT_WS(separator,[string | array(string)]+) needs at least two arguments.
+FAILED: SemanticException [Error 10015]: Line 2:7 Arguments length mismatch ''-'': The function CONCAT_WS(separator,[string | array(string)]+) needs at least two arguments.
diff --git a/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong2.q.out b/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong2.q.out
index 8676068..24edb6a 100644
--- a/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong2.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:23 Argument type mismatch '50': Argument 2 of function CONCAT_WS must be "string or array<string>", but "array<int>" was found.
+FAILED: SemanticException [Error 10016]: Line 2:23 Argument type mismatch '50': Argument 2 of function CONCAT_WS must be "string or array<string>", but "array<int>" was found.
diff --git a/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong3.q.out b/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong3.q.out
index e581670..3c94167 100644
--- a/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong3.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_concat_ws_wrong3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:17 Argument type mismatch '1234': Argument 1 of function CONCAT_WS must be "string or array<string>", but "int" was found.
+FAILED: SemanticException [Error 10016]: Line 2:17 Argument type mismatch '1234': Argument 1 of function CONCAT_WS must be "string or array<string>", but "int" was found.
diff --git a/src/ql/src/test/results/clientnegative/udf_elt_wrong_args_len.q.out b/src/ql/src/test/results/clientnegative/udf_elt_wrong_args_len.q.out
index 041d7d9..53bb579 100644
--- a/src/ql/src/test/results/clientnegative/udf_elt_wrong_args_len.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_elt_wrong_args_len.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Arguments length mismatch '3': The function ELT(N,str1,str2,str3,...) needs at least two arguments.
+FAILED: SemanticException [Error 10015]: Line 1:7 Arguments length mismatch '3': The function ELT(N,str1,str2,str3,...) needs at least two arguments.
diff --git a/src/ql/src/test/results/clientnegative/udf_elt_wrong_type.q.out b/src/ql/src/test/results/clientnegative/udf_elt_wrong_type.q.out
index be62440..33d7a53 100644
--- a/src/ql/src/test/results/clientnegative/udf_elt_wrong_type.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_elt_wrong_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:14 Argument type mismatch 'lintstring': The 2nd argument of function ELT is expected to a primitive type, but list is found
+FAILED: SemanticException [Error 10016]: Line 2:14 Argument type mismatch 'lintstring': The 2nd argument of function ELT is expected to a primitive type, but list is found
diff --git a/src/ql/src/test/results/clientnegative/udf_field_wrong_args_len.q.out b/src/ql/src/test/results/clientnegative/udf_field_wrong_args_len.q.out
index 5da47d0..5276b44 100644
--- a/src/ql/src/test/results/clientnegative/udf_field_wrong_args_len.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_field_wrong_args_len.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Wrong arguments '3': The function FIELD(str, str1, str2, ...) needs at least two arguments.
+FAILED: SemanticException [Error 10014]: Line 1:7 Wrong arguments '3': The function FIELD(str, str1, str2, ...) needs at least two arguments.
diff --git a/src/ql/src/test/results/clientnegative/udf_field_wrong_type.q.out b/src/ql/src/test/results/clientnegative/udf_field_wrong_type.q.out
index 5994402..fa6533f 100644
--- a/src/ql/src/test/results/clientnegative/udf_field_wrong_type.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_field_wrong_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:16 Argument type mismatch 'lintstring': The 2nd argument of function FIELD is expected to a primitive type, but list is found
+FAILED: SemanticException [Error 10016]: Line 2:16 Argument type mismatch 'lintstring': The 2nd argument of function FIELD is expected to a primitive type, but list is found
diff --git a/src/ql/src/test/results/clientnegative/udf_if_not_bool.q.out b/src/ql/src/test/results/clientnegative/udf_if_not_bool.q.out
index c64911d..6e3fafe 100644
--- a/src/ql/src/test/results/clientnegative/udf_if_not_bool.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_if_not_bool.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:10 Argument type mismatch ''STRING'': The first argument of function IF should be "boolean", but "string" is found
+FAILED: SemanticException [Error 10016]: Line 1:10 Argument type mismatch ''STRING'': The first argument of function IF should be "boolean", but "string" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_if_wrong_args_len.q.out b/src/ql/src/test/results/clientnegative/udf_if_wrong_args_len.q.out
index a4943f1..dfc6ff3 100644
--- a/src/ql/src/test/results/clientnegative/udf_if_wrong_args_len.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_if_wrong_args_len.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Arguments length mismatch '1': The function IF(expr1,expr2,expr3) accepts exactly 3 arguments.
+FAILED: SemanticException [Error 10015]: Line 1:7 Arguments length mismatch '1': The function IF(expr1,expr2,expr3) accepts exactly 3 arguments.
diff --git a/src/ql/src/test/results/clientnegative/udf_in.q.out b/src/ql/src/test/results/clientnegative/udf_in.q.out
index 0e7fb59..773230b 100644
--- a/src/ql/src/test/results/clientnegative/udf_in.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_in.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:9 Wrong arguments '3': The arguments for IN should be the same type! Types are: {int IN (array<int>)}
+FAILED: SemanticException [Error 10014]: Line 1:9 Wrong arguments '3': The arguments for IN should be the same type! Types are: {int IN (array<int>)}
diff --git a/src/ql/src/test/results/clientnegative/udf_instr_wrong_args_len.q.out b/src/ql/src/test/results/clientnegative/udf_instr_wrong_args_len.q.out
index 22d5f64..114746f 100644
--- a/src/ql/src/test/results/clientnegative/udf_instr_wrong_args_len.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_instr_wrong_args_len.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Arguments length mismatch ''abcd'': The function INSTR accepts exactly 2 arguments.
+FAILED: SemanticException [Error 10015]: Line 1:7 Arguments length mismatch ''abcd'': The function INSTR accepts exactly 2 arguments.
diff --git a/src/ql/src/test/results/clientnegative/udf_instr_wrong_type.q.out b/src/ql/src/test/results/clientnegative/udf_instr_wrong_type.q.out
index ed09a39..2b2f29f 100644
--- a/src/ql/src/test/results/clientnegative/udf_instr_wrong_type.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_instr_wrong_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:21 Argument type mismatch 'lintstring': The 2nd argument of function INSTR is expected to a primitive type, but list is found
+FAILED: SemanticException [Error 10016]: Line 2:21 Argument type mismatch 'lintstring': The 2nd argument of function INSTR is expected to a primitive type, but list is found
diff --git a/src/ql/src/test/results/clientnegative/udf_locate_wrong_args_len.q.out b/src/ql/src/test/results/clientnegative/udf_locate_wrong_args_len.q.out
index 331f53c..b3e353b 100644
--- a/src/ql/src/test/results/clientnegative/udf_locate_wrong_args_len.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_locate_wrong_args_len.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Arguments length mismatch '2': The function LOCATE accepts exactly 2 or 3 arguments.
+FAILED: SemanticException [Error 10015]: Line 1:7 Arguments length mismatch '2': The function LOCATE accepts exactly 2 or 3 arguments.
diff --git a/src/ql/src/test/results/clientnegative/udf_locate_wrong_type.q.out b/src/ql/src/test/results/clientnegative/udf_locate_wrong_type.q.out
index 162db30..eb724a5 100644
--- a/src/ql/src/test/results/clientnegative/udf_locate_wrong_type.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_locate_wrong_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:22 Argument type mismatch 'lintstring': The 2nd argument of function LOCATE is expected to a primitive type, but list is found
+FAILED: SemanticException [Error 10016]: Line 2:22 Argument type mismatch 'lintstring': The 2nd argument of function LOCATE is expected to a primitive type, but list is found
diff --git a/src/ql/src/test/results/clientnegative/udf_map_keys_arg_num.q.out b/src/ql/src/test/results/clientnegative/udf_map_keys_arg_num.q.out
index 5951998..44fcc58 100644
--- a/src/ql/src/test/results/clientnegative/udf_map_keys_arg_num.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_map_keys_arg_num.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Arguments length mismatch '"2"': The function MAP_KEYS only accepts one argument.
+FAILED: SemanticException [Error 10015]: Line 1:7 Arguments length mismatch '"2"': The function MAP_KEYS only accepts one argument.
diff --git a/src/ql/src/test/results/clientnegative/udf_map_keys_arg_type.q.out b/src/ql/src/test/results/clientnegative/udf_map_keys_arg_type.q.out
index 3f32522..5a190ce 100644
--- a/src/ql/src/test/results/clientnegative/udf_map_keys_arg_type.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_map_keys_arg_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:16 Argument type mismatch '3': "map" is expected at function MAP_KEYS, but "array<int>" is found
+FAILED: SemanticException [Error 10016]: Line 1:16 Argument type mismatch '3': "map" is expected at function MAP_KEYS, but "array<int>" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_map_values_arg_num.q.out b/src/ql/src/test/results/clientnegative/udf_map_values_arg_num.q.out
index 33e05c9..f5deee4 100644
--- a/src/ql/src/test/results/clientnegative/udf_map_values_arg_num.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_map_values_arg_num.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:7 Arguments length mismatch '"2"': The function MAP_VALUES only accepts 1 argument.
+FAILED: SemanticException [Error 10015]: Line 1:7 Arguments length mismatch '"2"': The function MAP_VALUES only accepts 1 argument.
diff --git a/src/ql/src/test/results/clientnegative/udf_map_values_arg_type.q.out b/src/ql/src/test/results/clientnegative/udf_map_values_arg_type.q.out
index 54f4436..affb070 100644
--- a/src/ql/src/test/results/clientnegative/udf_map_values_arg_type.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_map_values_arg_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:18 Argument type mismatch '4': "map" is expected at function MAP_VALUES, but "array<int>" is found
+FAILED: SemanticException [Error 10016]: Line 1:18 Argument type mismatch '4': "map" is expected at function MAP_VALUES, but "array<int>" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_max.q.out b/src/ql/src/test/results/clientnegative/udf_max.q.out
index d5e7f44..40b9650 100644
--- a/src/ql/src/test/results/clientnegative/udf_max.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_max.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Cannot support comparison of map<> type or complex type containing map<>.
+FAILED: UDFArgumentTypeException Cannot support comparison of map<> type or complex type containing map<>.
diff --git a/src/ql/src/test/results/clientnegative/udf_min.q.out b/src/ql/src/test/results/clientnegative/udf_min.q.out
index d5e7f44..40b9650 100644
--- a/src/ql/src/test/results/clientnegative/udf_min.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_min.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Cannot support comparison of map<> type or complex type containing map<>.
+FAILED: UDFArgumentTypeException Cannot support comparison of map<> type or complex type containing map<>.
diff --git a/src/ql/src/test/results/clientnegative/udf_printf_wrong1.q.out b/src/ql/src/test/results/clientnegative/udf_printf_wrong1.q.out
index 8bdbb07..41359d0 100644
--- a/src/ql/src/test/results/clientnegative/udf_printf_wrong1.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_printf_wrong1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:7 Arguments length mismatch 'printf': The function PRINTF(String format, Obj... args) needs at least one arguments.
+FAILED: SemanticException [Error 10015]: Line 2:7 Arguments length mismatch 'printf': The function PRINTF(String format, Obj... args) needs at least one arguments.
diff --git a/src/ql/src/test/results/clientnegative/udf_printf_wrong2.q.out b/src/ql/src/test/results/clientnegative/udf_printf_wrong2.q.out
index e89d44b..6a7c8e6 100644
--- a/src/ql/src/test/results/clientnegative/udf_printf_wrong2.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_printf_wrong2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:14 Argument type mismatch '100': Argument 1 of function PRINTF must be "string", but "int" was found.
+FAILED: SemanticException [Error 10016]: Line 2:14 Argument type mismatch '100': Argument 1 of function PRINTF must be "string", but "int" was found.
diff --git a/src/ql/src/test/results/clientnegative/udf_printf_wrong3.q.out b/src/ql/src/test/results/clientnegative/udf_printf_wrong3.q.out
index 47e4306..c020fda 100644
--- a/src/ql/src/test/results/clientnegative/udf_printf_wrong3.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_printf_wrong3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:32 Argument type mismatch '"argument"': Argument 2 of function PRINTF must be "PRIMITIVE", but "array<string>" was found.
+FAILED: SemanticException [Error 10016]: Line 2:32 Argument type mismatch '"argument"': Argument 2 of function PRINTF must be "PRIMITIVE", but "array<string>" was found.
diff --git a/src/ql/src/test/results/clientnegative/udf_printf_wrong4.q.out b/src/ql/src/test/results/clientnegative/udf_printf_wrong4.q.out
index 47e4306..c020fda 100644
--- a/src/ql/src/test/results/clientnegative/udf_printf_wrong4.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_printf_wrong4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:32 Argument type mismatch '"argument"': Argument 2 of function PRINTF must be "PRIMITIVE", but "array<string>" was found.
+FAILED: SemanticException [Error 10016]: Line 2:32 Argument type mismatch '"argument"': Argument 2 of function PRINTF must be "PRIMITIVE", but "array<string>" was found.
diff --git a/src/ql/src/test/results/clientnegative/udf_size_wrong_args_len.q.out b/src/ql/src/test/results/clientnegative/udf_size_wrong_args_len.q.out
index 7ee87b6..45b41fa 100644
--- a/src/ql/src/test/results/clientnegative/udf_size_wrong_args_len.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_size_wrong_args_len.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:7 Arguments length mismatch 'lintstring': The function SIZE only accepts 1 argument.
+FAILED: SemanticException [Error 10015]: Line 2:7 Arguments length mismatch 'lintstring': The function SIZE only accepts 1 argument.
diff --git a/src/ql/src/test/results/clientnegative/udf_size_wrong_type.q.out b/src/ql/src/test/results/clientnegative/udf_size_wrong_type.q.out
index 81dc947..4100701 100644
--- a/src/ql/src/test/results/clientnegative/udf_size_wrong_type.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_size_wrong_type.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 1:12 Argument type mismatch ''wrong type: string'': "map" or "list" is expected at function SIZE, but "string" is found
+FAILED: SemanticException [Error 10016]: Line 1:12 Argument type mismatch ''wrong type: string'': "map" or "list" is expected at function SIZE, but "string" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_sort_array_wrong1.q.out b/src/ql/src/test/results/clientnegative/udf_sort_array_wrong1.q.out
index 6620f98..075f49b 100644
--- a/src/ql/src/test/results/clientnegative/udf_sort_array_wrong1.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_sort_array_wrong1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:7 Arguments length mismatch '3': The function SORT_ARRAY(array(obj1, obj2,...)) needs one argument.
+FAILED: SemanticException [Error 10015]: Line 2:7 Arguments length mismatch '3': The function SORT_ARRAY(array(obj1, obj2,...)) needs one argument.
diff --git a/src/ql/src/test/results/clientnegative/udf_sort_array_wrong2.q.out b/src/ql/src/test/results/clientnegative/udf_sort_array_wrong2.q.out
index cf9aee7..c068ecd 100644
--- a/src/ql/src/test/results/clientnegative/udf_sort_array_wrong2.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_sort_array_wrong2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:18 Argument type mismatch '"Invalid"': Argument 1 of function SORT_ARRAY must be array<PRIMITIVE>, but string was found.
+FAILED: SemanticException [Error 10016]: Line 2:18 Argument type mismatch '"Invalid"': Argument 1 of function SORT_ARRAY must be array<PRIMITIVE>, but string was found.
diff --git a/src/ql/src/test/results/clientnegative/udf_sort_array_wrong3.q.out b/src/ql/src/test/results/clientnegative/udf_sort_array_wrong3.q.out
index 1faca60..abf7124 100644
--- a/src/ql/src/test/results/clientnegative/udf_sort_array_wrong3.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_sort_array_wrong3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 2:18 Argument type mismatch '13': Argument 1 of function SORT_ARRAY must be array<PRIMITIVE>, but array<array<int>> was found.
+FAILED: SemanticException [Error 10016]: Line 2:18 Argument type mismatch '13': Argument 1 of function SORT_ARRAY must be array<PRIMITIVE>, but array<array<int>> was found.
diff --git a/src/ql/src/test/results/clientnegative/udf_when_type_wrong.q.out b/src/ql/src/test/results/clientnegative/udf_when_type_wrong.q.out
index e7264ff..7c7582d 100644
--- a/src/ql/src/test/results/clientnegative/udf_when_type_wrong.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_when_type_wrong.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 3:13 Argument type mismatch ''1'': "boolean" is expected after WHEN, but "string" is found
+FAILED: SemanticException [Error 10016]: Line 3:13 Argument type mismatch ''1'': "boolean" is expected after WHEN, but "string" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_when_type_wrong2.q.out b/src/ql/src/test/results/clientnegative/udf_when_type_wrong2.q.out
index 0b7bc85..e94e6e3 100644
--- a/src/ql/src/test/results/clientnegative/udf_when_type_wrong2.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_when_type_wrong2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 3:22 Argument type mismatch '4': The expressions after THEN should have the same type: "string" is expected but "int" is found
+FAILED: SemanticException [Error 10016]: Line 3:22 Argument type mismatch '4': The expressions after THEN should have the same type: "string" is expected but "int" is found
diff --git a/src/ql/src/test/results/clientnegative/udf_when_type_wrong3.q.out b/src/ql/src/test/results/clientnegative/udf_when_type_wrong3.q.out
index be01ab9..7d4c12f 100644
--- a/src/ql/src/test/results/clientnegative/udf_when_type_wrong3.q.out
+++ b/src/ql/src/test/results/clientnegative/udf_when_type_wrong3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Line 4:13 Argument type mismatch '5.3': The expression after ELSE should have the same type as those after THEN: "string" is expected but "double" is found
+FAILED: SemanticException [Error 10016]: Line 4:13 Argument type mismatch '5.3': The expression after ELSE should have the same type as those after THEN: "string" is expected but "double" is found
diff --git a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported1.q.out b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported1.q.out
index ef8f4e0..23eb0df 100644
--- a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported1.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: GROUP BY is not supported with a UDTF in the SELECT clause
+FAILED: SemanticException [Error 10077]: GROUP BY is not supported with a UDTF in the SELECT clause
diff --git a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported2.q.out b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported2.q.out
index a57e7d8..54d63d7 100644
--- a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported2.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 2 aliases but got 3
+FAILED: SemanticException [Error 10083]: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 2 aliases but got 3
diff --git a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported3.q.out b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported3.q.out
index 8c35779..ba4b6c3 100644
--- a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported3.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: explode() takes only one argument
+FAILED: UDFArgumentException explode() takes only one argument
diff --git a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported4.q.out b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported4.q.out
index ef8f4e0..23eb0df 100644
--- a/src/ql/src/test/results/clientnegative/udtf_explode_not_supported4.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_explode_not_supported4.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: GROUP BY is not supported with a UDTF in the SELECT clause
+FAILED: SemanticException [Error 10077]: GROUP BY is not supported with a UDTF in the SELECT clause
diff --git a/src/ql/src/test/results/clientnegative/udtf_invalid_place.q.out b/src/ql/src/test/results/clientnegative/udtf_invalid_place.q.out
index 90d498b..9710c13 100644
--- a/src/ql/src/test/results/clientnegative/udtf_invalid_place.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_invalid_place.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: UDTF's are not supported outside the SELECT clause, nor nested in expressions
+FAILED: SemanticException [Error 10081]: UDTF's are not supported outside the SELECT clause, nor nested in expressions
diff --git a/src/ql/src/test/results/clientnegative/udtf_not_supported1.q.out b/src/ql/src/test/results/clientnegative/udtf_not_supported1.q.out
index 694af76..0dcb730 100644
--- a/src/ql/src/test/results/clientnegative/udtf_not_supported1.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_not_supported1.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: 1:39 Only a single expression in the SELECT clause is supported with UDTF's. Error encountered near token 'key'
+FAILED: SemanticException 1:39 Only a single expression in the SELECT clause is supported with UDTF's. Error encountered near token 'key'
diff --git a/src/ql/src/test/results/clientnegative/udtf_not_supported2.q.out b/src/ql/src/test/results/clientnegative/udtf_not_supported2.q.out
index 4e46415..2c62107 100644
--- a/src/ql/src/test/results/clientnegative/udtf_not_supported2.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_not_supported2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: 1:7 UDTF's require an AS clause. Error encountered near token '3'
+FAILED: SemanticException 1:7 UDTF's require an AS clause. Error encountered near token '3'
diff --git a/src/ql/src/test/results/clientnegative/udtf_not_supported3.q.out b/src/ql/src/test/results/clientnegative/udtf_not_supported3.q.out
index ef8f4e0..23eb0df 100644
--- a/src/ql/src/test/results/clientnegative/udtf_not_supported3.q.out
+++ b/src/ql/src/test/results/clientnegative/udtf_not_supported3.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: GROUP BY is not supported with a UDTF in the SELECT clause
+FAILED: SemanticException [Error 10077]: GROUP BY is not supported with a UDTF in the SELECT clause
diff --git a/src/ql/src/test/results/clientnegative/union.q.out b/src/ql/src/test/results/clientnegative/union.q.out
index 971ce00..b66d394 100644
--- a/src/ql/src/test/results/clientnegative/union.q.out
+++ b/src/ql/src/test/results/clientnegative/union.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: 2:45 Top level UNION is not supported currently; use a subquery for the UNION. Error encountered near token 'value'
+FAILED: SemanticException 2:45 Top level UNION is not supported currently; use a subquery for the UNION. Error encountered near token 'value'
diff --git a/src/ql/src/test/results/clientnegative/union2.q.out b/src/ql/src/test/results/clientnegative/union2.q.out
index 12ecf65..db3804e 100644
--- a/src/ql/src/test/results/clientnegative/union2.q.out
+++ b/src/ql/src/test/results/clientnegative/union2.q.out
@@ -8,4 +8,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table if not exists union2_t2(s string, c string, v string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@union2_t2
-FAILED: Error in semantic analysis: 8:47 Schema of both sides of union should match: Column v is of type array<string> on first table and type double on second table. Error encountered near token 'union2_t2'
+FAILED: SemanticException 8:47 Schema of both sides of union should match: Column v is of type array<string> on first table and type double on second table. Error encountered near token 'union2_t2'
diff --git a/src/ql/src/test/results/clientnegative/union3.q.out b/src/ql/src/test/results/clientnegative/union3.q.out
index 09e9401..61d10e4 100644
--- a/src/ql/src/test/results/clientnegative/union3.q.out
+++ b/src/ql/src/test/results/clientnegative/union3.q.out
@@ -7,4 +7,4 @@ POSTHOOK: query: -- Ensure that UNION ALL columns are in the correct order on bo
 CREATE TABLE IF NOT EXISTS union3  (bar int, baz int)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@union3
-FAILED: Error in semantic analysis: 2:85 Schema of both sides of union should match: field bar: appears on the left side of the UNION at column position: 0, and on the right side of the UNION at column position: 1. Column positions should match for a UNION. Error encountered near token 'union3'
+FAILED: SemanticException 2:85 Schema of both sides of union should match: field bar: appears on the left side of the UNION at column position: 0, and on the right side of the UNION at column position: 1. Column positions should match for a UNION. Error encountered near token 'union3'
diff --git a/src/ql/src/test/results/clientnegative/uniquejoin.q.out b/src/ql/src/test/results/clientnegative/uniquejoin.q.out
index a12f25f..b641699 100644
--- a/src/ql/src/test/results/clientnegative/uniquejoin.q.out
+++ b/src/ql/src/test/results/clientnegative/uniquejoin.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Subqueries are not supported in UNIQUEJOIN
+FAILED: SemanticException Subqueries are not supported in UNIQUEJOIN
diff --git a/src/ql/src/test/results/clientnegative/uniquejoin2.q.out b/src/ql/src/test/results/clientnegative/uniquejoin2.q.out
index 6b88cf0..5a7dd28 100644
--- a/src/ql/src/test/results/clientnegative/uniquejoin2.q.out
+++ b/src/ql/src/test/results/clientnegative/uniquejoin2.q.out
@@ -1 +1 @@
-FAILED: Error in semantic analysis: Tables with different or invalid number of keys in UNIQUEJOIN
+FAILED: SemanticException Tables with different or invalid number of keys in UNIQUEJOIN
diff --git a/src/ql/src/test/results/clientnegative/uniquejoin3.q.out b/src/ql/src/test/results/clientnegative/uniquejoin3.q.out
index 8e8857c..192939c 100644
--- a/src/ql/src/test/results/clientnegative/uniquejoin3.q.out
+++ b/src/ql/src/test/results/clientnegative/uniquejoin3.q.out
@@ -1,2 +1,2 @@
-FAILED: Parse Error: line 1:54 required (...)+ loop did not match anything at input 'JOIN' in statement
+FAILED: ParseException line 1:54 required (...)+ loop did not match anything at input 'JOIN' in statement
 
diff --git a/src/ql/src/test/results/clientnegative/wrong_column_type.q.out b/src/ql/src/test/results/clientnegative/wrong_column_type.q.out
index cd02d19..ab33949 100644
--- a/src/ql/src/test/results/clientnegative/wrong_column_type.q.out
+++ b/src/ql/src/test/results/clientnegative/wrong_column_type.q.out
@@ -3,4 +3,4 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE dest1(a float)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@dest1
-FAILED: Error in semantic analysis: No matching method for class org.apache.hadoop.hive.ql.udf.UDFToFloat with (array<double>). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
+FAILED: NoMatchingMethodException No matching method for class org.apache.hadoop.hive.ql.udf.UDFToFloat with (array<double>). Possible choices: _FUNC_(void)  _FUNC_(boolean)  _FUNC_(tinyint)  _FUNC_(smallint)  _FUNC_(int)  _FUNC_(bigint)  _FUNC_(double)  _FUNC_(string)  _FUNC_(timestamp)  
diff --git a/src/ql/src/test/results/clientpositive/auto_join25.q.out b/src/ql/src/test/results/clientpositive/auto_join25.q.out
index 60b1538..cabdcf8 100644
--- a/src/ql/src/test/results/clientpositive/auto_join25.q.out
+++ b/src/ql/src/test/results/clientpositive/auto_join25.q.out
@@ -13,7 +13,7 @@ PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
 PREHOOK: Output: default@dest1
-Execution failed with exit status: 2
+Execution failed with exit status: 3
 Obtaining error information
 
 Task failed!
@@ -23,7 +23,7 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
+FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
 ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask
 POSTHOOK: query: FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
 INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value 
@@ -60,7 +60,7 @@ INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@dest_j2
-Execution failed with exit status: 2
+Execution failed with exit status: 3
 Obtaining error information
 
 Task failed!
@@ -70,9 +70,9 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
+FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
 ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask
-Execution failed with exit status: 2
+Execution failed with exit status: 3
 Obtaining error information
 
 Task failed!
@@ -82,7 +82,7 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
+FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
 ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask
 POSTHOOK: query: FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
 INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value
@@ -120,7 +120,7 @@ INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@dest_j1
-Execution failed with exit status: 2
+Execution failed with exit status: 3
 Obtaining error information
 
 Task failed!
@@ -130,7 +130,7 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
+FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
 ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask
 POSTHOOK: query: FROM src src1 JOIN src src2 ON (src1.key = src2.key)
 INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value
diff --git a/src/ql/src/test/results/clientpositive/mapjoin_hook.q.out b/src/ql/src/test/results/clientpositive/mapjoin_hook.q.out
index fd32808..5384b90 100644
--- a/src/ql/src/test/results/clientpositive/mapjoin_hook.q.out
+++ b/src/ql/src/test/results/clientpositive/mapjoin_hook.q.out
@@ -25,7 +25,7 @@ PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
 PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
 PREHOOK: Output: default@dest1
-Execution failed with exit status: 2
+Execution failed with exit status: 3
 Obtaining error information
 
 Task failed!
@@ -35,7 +35,7 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
+FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
 ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask
 [MapJoinCounter PostHook] CONVERTED_LOCAL_MAPJOIN: 1 CONVERTED_MAPJOIN: 0 LOCAL_MAPJOIN: 0 COMMON_JOIN: 0 BACKUP_COMMON_JOIN: 1
 PREHOOK: query: FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
@@ -43,7 +43,7 @@ INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@dest1
-Execution failed with exit status: 2
+Execution failed with exit status: 3
 Obtaining error information
 
 Task failed!
@@ -53,9 +53,9 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
+FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
 ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask
-Execution failed with exit status: 2
+Execution failed with exit status: 3
 Obtaining error information
 
 Task failed!
@@ -65,6 +65,6 @@ Task ID:
 Logs:
 
 #### A masked pattern was here ####
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
+FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.MapredLocalTask
 ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask
 [MapJoinCounter PostHook] CONVERTED_LOCAL_MAPJOIN: 2 CONVERTED_MAPJOIN: 0 LOCAL_MAPJOIN: 0 COMMON_JOIN: 0 BACKUP_COMMON_JOIN: 2
-- 
1.7.0.4

