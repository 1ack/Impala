From 498a319a31dd82db858a0b0dc95dcded58ea5c9b Mon Sep 17 00:00:00 2001
From: Kevin Wilfong <kevinwilfong@apache.org>
Date: Tue, 10 Apr 2012 16:37:33 +0000
Subject: [PATCH 002/144] HIVE-2907. Hive error when dropping a table with large number of partitions. (mousom via kevinwilfong)

git-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1311850 13f79535-47bb-0310-9956-ffa450edef68
---
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    3 +
 conf/hive-default.xml.template                     |    6 +
 .../hadoop/hive/metastore/HiveAlterHandler.java    |    2 +-
 .../apache/hadoop/hive/metastore/ObjectStore.java  |   27 ++-
 .../hadoop/hive/metastore/TestHiveMetaStore.java   |  368 ++++++++++++++++----
 .../org/apache/hadoop/hive/ql/exec/DDLTask.java    |   17 +-
 .../queries/clientnegative/drop_table_failure3.q   |   12 +
 ql/src/test/queries/clientpositive/drop_table2.q   |   15 +
 .../clientnegative/drop_table_failure3.q.out       |   49 +++
 .../test/results/clientpositive/drop_table2.q.out  |   58 +++
 10 files changed, 469 insertions(+), 88 deletions(-)
 create mode 100644 ql/src/test/queries/clientnegative/drop_table_failure3.q
 create mode 100644 ql/src/test/queries/clientpositive/drop_table2.q
 create mode 100644 ql/src/test/results/clientnegative/drop_table_failure3.q.out
 create mode 100644 ql/src/test/results/clientpositive/drop_table2.q.out

diff --git a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 422d29e..8aefbdc 100644
--- a/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/src/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -122,6 +122,7 @@ public class HiveConf extends Configuration {
       HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL,
       HiveConf.ConfVars.METASTORE_END_FUNCTION_LISTENERS,
       HiveConf.ConfVars.METASTORE_PART_INHERIT_TBL_PROPS,
+      HiveConf.ConfVars.METASTORE_BATCH_RETRIEVE_TABLE_PARTITION_MAX,
       HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS,
       };
 
@@ -290,6 +291,8 @@ public class HiveConf extends Configuration {
     METASTORE_IDENTIFIER_FACTORY("datanucleus.identifierFactory", "datanucleus"),
     METASTORE_PLUGIN_REGISTRY_BUNDLE_CHECK("datanucleus.plugin.pluginRegistryBundleCheck", "LOG"),
     METASTORE_BATCH_RETRIEVE_MAX("hive.metastore.batch.retrieve.max", 300),
+    METASTORE_BATCH_RETRIEVE_TABLE_PARTITION_MAX(
+      "hive.metastore.batch.retrieve.table.partition.max", 1000),
     METASTORE_PRE_EVENT_LISTENERS("hive.metastore.pre.event.listeners", ""),
     METASTORE_EVENT_LISTENERS("hive.metastore.event.listeners", ""),
     // should we do checks against the storage (usually hdfs) for operations like drop_partition
diff --git a/src/conf/hive-default.xml.template b/src/conf/hive-default.xml.template
index e0d9c28..64e7640 100644
--- a/src/conf/hive-default.xml.template
+++ b/src/conf/hive-default.xml.template
@@ -298,6 +298,12 @@
 </property>
 
 <property>
+  <name>hive.metastore.batch.retrieve.table.partition.max</name>
+  <value>1000</value>
+  <description>Maximum number of table partitions that metastore internally retrieves in one batch.</description>
+</property>
+
+<property>
   <name>hive.default.fileformat</name>
   <value>TextFile</value>
   <description>Default file format for CREATE TABLE statement. Options are TextFile and SequenceFile. Users can explicitly say CREATE TABLE ... STORED AS &lt;TEXTFILE|SEQUENCEFILE&gt; to override</description>
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
index 5c4a591..2120a5f 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
@@ -158,7 +158,7 @@ public class HiveAlterHandler implements AlterHandler {
               + newt.getTableName());
         }
         // also the location field in partition
-        List<Partition> parts = msdb.getPartitions(dbname, name, 0);
+        List<Partition> parts = msdb.getPartitions(dbname, name, -1);
         for (Partition part : parts) {
           String oldPartLoc = part.getSd().getLocation();
           Path oldPartLocPath = new Path(oldPartLoc);
diff --git a/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index 0bf4527..c64dc16 100644
--- a/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/src/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -71,6 +71,7 @@ import org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet;
 import org.apache.hadoop.hive.metastore.api.PrincipalType;
 import org.apache.hadoop.hive.metastore.api.PrivilegeBag;
 import org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo;
+import org.apache.hadoop.hive.metastore.api.RegionStorageDescriptor;
 import org.apache.hadoop.hive.metastore.api.Role;
 import org.apache.hadoop.hive.metastore.api.SerDeInfo;
 import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
@@ -90,6 +91,7 @@ import org.apache.hadoop.hive.metastore.model.MPartition;
 import org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege;
 import org.apache.hadoop.hive.metastore.model.MPartitionEvent;
 import org.apache.hadoop.hive.metastore.model.MPartitionPrivilege;
+import org.apache.hadoop.hive.metastore.model.MRegionStorageDescriptor;
 import org.apache.hadoop.hive.metastore.model.MRole;
 import org.apache.hadoop.hive.metastore.model.MRoleMap;
 import org.apache.hadoop.hive.metastore.model.MSerDeInfo;
@@ -703,10 +705,16 @@ public class ObjectStore implements RawStore, Configurable {
           pm.deletePersistentAll(partColGrants);
         }
 
+        int partitionBatchSize = HiveConf.getIntVar(getConf(),
+          ConfVars.METASTORE_BATCH_RETRIEVE_TABLE_PARTITION_MAX);
+
         // call dropPartition on each of the table's partitions to follow the
         // procedure for cleanly dropping partitions.
-        List<MPartition> partsToDelete = listMPartitions(dbName, tableName, -1);
-        if (partsToDelete != null) {
+        while(true) {
+          List<MPartition> partsToDelete = listMPartitions(dbName, tableName, partitionBatchSize);
+          if (partsToDelete == null || partsToDelete.isEmpty()) {
+            break;
+          }
           for (MPartition mpart : partsToDelete) {
             dropPartitionCommon(mpart);
           }
@@ -1265,20 +1273,19 @@ public class ObjectStore implements RawStore, Configurable {
   public List<Partition> getPartitions(String dbName, String tableName, int max)
       throws MetaException {
     openTransaction();
-    List<Partition> parts = convertToParts(listMPartitions(dbName, tableName,
-        max));
+    List<Partition> parts = convertToParts(listMPartitions(dbName, tableName, max));
     commitTransaction();
     return parts;
   }
 
   @Override
   public List<Partition> getPartitionsWithAuth(String dbName, String tblName,
-      short maxParts, String userName, List<String> groupNames)
+      short max, String userName, List<String> groupNames)
       throws MetaException, NoSuchObjectException, InvalidObjectException {
     boolean success = false;
     try {
       openTransaction();
-      List<MPartition> mparts = listMPartitions(dbName, tblName, maxParts);
+      List<MPartition> mparts = listMPartitions(dbName, tblName, max);
       List<Partition> parts = new ArrayList<Partition>(mparts.size());
       if (mparts != null && mparts.size()>0) {
         for (MPartition mpart : mparts) {
@@ -1372,6 +1379,10 @@ public class ObjectStore implements RawStore, Configurable {
           + "order by partitionName asc");
       q.declareParameters("java.lang.String t1, java.lang.String t2");
       q.setResult("partitionName");
+
+      if(max > 0) {
+        q.setRange(0, max);
+      }
       Collection names = (Collection) q.execute(dbName, tableName);
       for (Iterator i = names.iterator(); i.hasNext();) {
         pns.add((String) i.next());
@@ -1507,6 +1518,7 @@ public class ObjectStore implements RawStore, Configurable {
   // TODO:pc implement max
   private List<MPartition> listMPartitions(String dbName, String tableName,
       int max) {
+
     boolean success = false;
     List<MPartition> mparts = null;
     try {
@@ -1518,6 +1530,9 @@ public class ObjectStore implements RawStore, Configurable {
           "table.tableName == t1 && table.database.name == t2");
       query.declareParameters("java.lang.String t1, java.lang.String t2");
       query.setOrdering("partitionName ascending");
+      if(max > 0) {
+        query.setRange(0, max);
+      }
       mparts = (List<MPartition>) query.execute(tableName, dbName);
       LOG.debug("Done executing query for listMPartitions");
       pm.retrieveAll(mparts);
diff --git a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
index b518a2d..554e3d1 100644
--- a/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
+++ b/src/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java
@@ -456,6 +456,119 @@ public abstract class TestHiveMetaStore extends TestCase {
     return part4;
   }
 
+  public void testListPartitions() throws Throwable {
+    // create a table with multiple partitions
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String typeName = "Person";
+
+    cleanUp(dbName, tblName, typeName);
+
+    List<List<String>> values = new ArrayList<List<String>>();
+    values.add(makeVals("2008-07-01 14:13:12", "14"));
+    values.add(makeVals("2008-07-01 14:13:12", "15"));
+    values.add(makeVals("2008-07-02 14:13:12", "15"));
+    values.add(makeVals("2008-07-03 14:13:12", "151"));
+
+    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
+
+    List<Partition> partitions = client.listPartitions(dbName, tblName, (short)-1);
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions", values.size(), partitions.size());
+
+    partitions = client.listPartitions(dbName, tblName, (short)(values.size()/2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() / 2 +
+      " partitions",values.size() / 2, partitions.size());
+
+
+    partitions = client.listPartitions(dbName, tblName, (short) (values.size() * 2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions",values.size(), partitions.size());
+
+    cleanUp(dbName, tblName, typeName);
+
+  }
+
+
+
+  public void testListPartitionNames() throws Throwable {
+    // create a table with multiple partitions
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String typeName = "Person";
+
+    cleanUp(dbName, tblName, typeName);
+
+    List<List<String>> values = new ArrayList<List<String>>();
+    values.add(makeVals("2008-07-01 14:13:12", "14"));
+    values.add(makeVals("2008-07-01 14:13:12", "15"));
+    values.add(makeVals("2008-07-02 14:13:12", "15"));
+    values.add(makeVals("2008-07-03 14:13:12", "151"));
+
+
+
+    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
+
+    List<String> partitions = client.listPartitionNames(dbName, tblName, (short)-1);
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions", values.size(), partitions.size());
+
+    partitions = client.listPartitionNames(dbName, tblName, (short)(values.size()/2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() / 2 +
+      " partitions",values.size() / 2, partitions.size());
+
+
+    partitions = client.listPartitionNames(dbName, tblName, (short) (values.size() * 2));
+
+    assertNotNull("should have returned partitions", partitions);
+    assertEquals(" should have returned " + values.size() +
+      " partitions",values.size(), partitions.size());
+
+    cleanUp(dbName, tblName, typeName);
+
+  }
+
+
+  public void testDropTable() throws Throwable {
+    // create a table with multiple partitions
+    String dbName = "compdb";
+    String tblName = "comptbl";
+    String typeName = "Person";
+
+    cleanUp(dbName, tblName, typeName);
+
+    List<List<String>> values = new ArrayList<List<String>>();
+    values.add(makeVals("2008-07-01 14:13:12", "14"));
+    values.add(makeVals("2008-07-01 14:13:12", "15"));
+    values.add(makeVals("2008-07-02 14:13:12", "15"));
+    values.add(makeVals("2008-07-03 14:13:12", "151"));
+
+    createMultiPartitionTableSchema(dbName, tblName, typeName, values);
+
+    client.dropTable(dbName, tblName);
+    client.dropType(typeName);
+
+    boolean exceptionThrown = false;
+    try {
+      client.getTable(dbName, tblName);
+    } catch(Exception e) {
+      assertEquals("table should not have existed",
+          NoSuchObjectException.class, e.getClass());
+      exceptionThrown = true;
+    }
+    assertTrue("Table " + tblName + " should have been dropped ", exceptionThrown);
+
+  }
+
+
   public void testAlterPartition() throws Throwable {
 
     try {
@@ -1689,11 +1802,9 @@ public abstract class TestHiveMetaStore extends TestCase {
       vals3.add("p12");
       vals3.add("p21");
 
-      silentDropDatabase(dbName);
+      cleanUp(dbName, tblName, null);
 
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
+      createDb(dbName);
 
       ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
       cols.add(new FieldSchema("c1", Constants.STRING_TYPE_NAME, ""));
@@ -1703,26 +1814,16 @@ public abstract class TestHiveMetaStore extends TestCase {
       partCols.add(new FieldSchema("p1", Constants.STRING_TYPE_NAME, ""));
       partCols.add(new FieldSchema("p2", Constants.STRING_TYPE_NAME, ""));
 
+      Map<String, String> serdParams = new HashMap<String, String>();
+      serdParams.put(Constants.SERIALIZATION_FORMAT, "1");
+      StorageDescriptor sd = createStorageDescriptor(tblName, partCols, null, serdParams);
+
       Table tbl = new Table();
       tbl.setDbName(dbName);
       tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
       tbl.setSd(sd);
-      sd.setCols(cols);
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.setBucketCols(new ArrayList<String>());
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters()
-          .put(Constants.SERIALIZATION_FORMAT, "1");
-      sd.setSortCols(new ArrayList<Order>());
-
       tbl.setPartitionKeys(partCols);
       client.createTable(tbl);
-
       tbl = client.getTable(dbName, tblName);
 
       add_partition(client, tbl, vals, "part1");
@@ -1738,8 +1839,7 @@ public abstract class TestHiveMetaStore extends TestCase {
       checkFilter(client, dbName, tblName, "p2 like \"p2.*\"", 3);
       checkFilter(client, dbName, tblName, "p2 like \"p.*2\"", 1);
 
-      client.dropTable(dbName, tblName);
-      client.dropDatabase(dbName);
+      cleanUp(dbName, tblName, null);
   }
 
   private void checkFilter(HiveMetaStoreClient client, String dbName,
@@ -1910,47 +2010,33 @@ public abstract class TestHiveMetaStore extends TestCase {
     }
   }
 
-  private Table createTableForTestFilter(String dbName, String tableName, String owner, int lastAccessTime, boolean hasSecondParam) throws Exception {
-    client.dropTable(dbName, tableName);
+  private Table createTableForTestFilter(String dbName, String tableName, String owner,
+    int lastAccessTime, boolean hasSecondParam) throws Exception {
 
     ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
     cols.add(new FieldSchema("name", Constants.STRING_TYPE_NAME, ""));
     cols.add(new FieldSchema("income", Constants.INT_TYPE_NAME, ""));
 
-    Table tbl = new Table();
-    tbl.setDbName(dbName);
-    tbl.setTableName(tableName);
-    tbl.setParameters(new HashMap<String, String>());
-    tbl.getParameters().put("test_param_1", "hi");
-    if (hasSecondParam) {
-      tbl.getParameters().put("test_param_2", "50");
-    }
-    StorageDescriptor sd = new StorageDescriptor();
-    tbl.setSd(sd);
-    sd.setCols(cols);
-    sd.setCompressed(false);
-    sd.setNumBuckets(1);
-    sd.setParameters(new HashMap<String, String>());
-    sd.getParameters().put("sd_param_1", "Use this for comments etc");
-    sd.setBucketCols(new ArrayList<String>(2));
-    sd.getBucketCols().add("name");
-    sd.setSerdeInfo(new SerDeInfo());
-    sd.getSerdeInfo().setName(tbl.getTableName());
-    sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-    sd.getSerdeInfo().getParameters()
-        .put(Constants.SERIALIZATION_FORMAT, "1");
-    sd.setSortCols(new ArrayList<Order>());
+    Map<String, String> params = new HashMap<String, String>();
+    params.put("sd_param_1", "Use this for comments etc");
 
-    tbl.setOwner(owner);
-    tbl.setLastAccessTime(lastAccessTime);
+    Map<String, String> serdParams = new HashMap<String, String>();
+    serdParams.put(Constants.SERIALIZATION_FORMAT, "1");
 
-    tbl.setPartitionKeys(new ArrayList<FieldSchema>(2));
-    tbl.getPartitionKeys().add(
-        new FieldSchema("ds", Constants.STRING_TYPE_NAME, ""));
-    tbl.getPartitionKeys().add(
-        new FieldSchema("hr", Constants.INT_TYPE_NAME, ""));
+    StorageDescriptor sd = createStorageDescriptor(tableName, cols, params, serdParams);
 
-    client.createTable(tbl);
+    Map<String, String> partitionKeys = new HashMap<String, String>();
+    partitionKeys.put("ds", Constants.STRING_TYPE_NAME);
+    partitionKeys.put("hr", Constants.INT_TYPE_NAME);
+
+    Map<String, String> tableParams =  new HashMap<String, String>();
+    tableParams.put("test_param_1", "hi");
+    if(hasSecondParam) {
+      tableParams.put("test_param_2", "50");
+    }
+
+    Table tbl = createTable(dbName, tableName, owner, tableParams,
+        partitionKeys, sd, lastAccessTime);
 
     if (isThriftClient) {
       // the createTable() above does not update the location in the 'tbl'
@@ -1961,7 +2047,6 @@ public abstract class TestHiveMetaStore extends TestCase {
     }
     return tbl;
   }
-
   /**
    * Verify that if another  client, either a metastore Thrift server or  a Hive CLI instance
    * renames a table recently created by this instance, and hence potentially in its cache, the
@@ -1974,36 +2059,23 @@ public abstract class TestHiveMetaStore extends TestCase {
     String renameTblName = "rename_concurrenttbl";
 
     try {
-      client.dropTable(dbName, tblName);
-      silentDropDatabase(dbName);
+      cleanUp(dbName, tblName, null);
 
-      Database db = new Database();
-      db.setName(dbName);
-      client.createDatabase(db);
+      createDb(dbName);
 
       ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);
       cols.add(new FieldSchema("c1", Constants.STRING_TYPE_NAME, ""));
       cols.add(new FieldSchema("c2", Constants.INT_TYPE_NAME, ""));
 
-      Table tbl = new Table();
-      tbl.setDbName(dbName);
-      tbl.setTableName(tblName);
-      StorageDescriptor sd = new StorageDescriptor();
-      tbl.setSd(sd);
-      sd.setCols(cols);
-      sd.setCompressed(false);
-      sd.setNumBuckets(1);
-      sd.setParameters(new HashMap<String, String>());
-      sd.getParameters().put("test_param_1", "Use this for comments etc");
-      sd.setBucketCols(new ArrayList<String>(2));
-      sd.getBucketCols().add("name");
-      sd.setSerdeInfo(new SerDeInfo());
-      sd.getSerdeInfo().setName(tbl.getTableName());
-      sd.getSerdeInfo().setParameters(new HashMap<String, String>());
-      sd.getSerdeInfo().getParameters().put(
-          org.apache.hadoop.hive.serde.Constants.SERIALIZATION_FORMAT, "1");
+      Map<String, String> params = new HashMap<String, String>();
+      params.put("test_param_1", "Use this for comments etc");
 
-      client.createTable(tbl);
+      Map<String, String> serdParams = new HashMap<String, String>();
+      serdParams.put(Constants.SERIALIZATION_FORMAT, "1");
+
+      StorageDescriptor sd =  createStorageDescriptor(tblName, cols, params, serdParams);
+
+      createTable(dbName, tblName, null, null, null, sd, 0);
 
       // get the table from the client, verify the name is correct
       Table tbl2 = client.getTable(dbName, tblName);
@@ -2061,4 +2133,146 @@ public abstract class TestHiveMetaStore extends TestCase {
 
     Utilities.executeWithRetry(execUpdate, updateStmt, interval, attempts);
   }
+
+  private void cleanUp(String dbName, String tableName, String typeName) throws Exception {
+    if(dbName != null && tableName != null) {
+      client.dropTable(dbName, tableName);
+    }
+    if(dbName != null) {
+      silentDropDatabase(dbName);
+    }
+    if(typeName != null) {
+      client.dropType(typeName);
+    }
+  }
+
+  private Database createDb(String dbName) throws Exception {
+    if(null == dbName) { return null; }
+    Database db = new Database();
+    db.setName(dbName);
+    client.createDatabase(db);
+    return db;
+  }
+
+  private Type createType(String typeName, Map<String, String> fields) throws Throwable {
+    Type typ1 = new Type();
+    typ1.setName(typeName);
+    typ1.setFields(new ArrayList<FieldSchema>(fields.size()));
+    for(String fieldName : fields.keySet()) {
+      typ1.getFields().add(
+          new FieldSchema(fieldName, fields.get(fieldName), ""));
+    }
+    client.createType(typ1);
+    return typ1;
+  }
+
+  private Table createTable(String dbName, String tblName, String owner,
+      Map<String,String> tableParams, Map<String, String> partitionKeys,
+      StorageDescriptor sd, int lastAccessTime) throws Exception {
+    Table tbl = new Table();
+    tbl.setDbName(dbName);
+    tbl.setTableName(tblName);
+    if(tableParams != null) {
+      tbl.setParameters(tableParams);
+    }
+
+    if(owner != null) {
+      tbl.setOwner(owner);
+    }
+
+    if(partitionKeys != null) {
+      tbl.setPartitionKeys(new ArrayList<FieldSchema>(partitionKeys.size()));
+      for(String key : partitionKeys.keySet()) {
+        tbl.getPartitionKeys().add(
+            new FieldSchema(key, partitionKeys.get(key), ""));
+      }
+    }
+
+    tbl.setSd(sd);
+    tbl.setLastAccessTime(lastAccessTime);
+
+    client.createTable(tbl);
+    return tbl;
+  }
+
+  private StorageDescriptor createStorageDescriptor(String tableName,
+    List<FieldSchema> cols, Map<String, String> params, Map<String, String> serdParams)  {
+    StorageDescriptor sd = new StorageDescriptor();
+
+    sd.setCols(cols);
+    sd.setCompressed(false);
+    sd.setNumBuckets(1);
+    sd.setParameters(params);
+    sd.setBucketCols(new ArrayList<String>(2));
+    sd.getBucketCols().add("name");
+    sd.setSerdeInfo(new SerDeInfo());
+    sd.getSerdeInfo().setName(tableName);
+    sd.getSerdeInfo().setParameters(serdParams);
+    sd.getSerdeInfo().getParameters()
+        .put(Constants.SERIALIZATION_FORMAT, "1");
+    sd.setSortCols(new ArrayList<Order>());
+
+    return sd;
+  }
+
+  private List<Partition> createPartitions(String dbName, Table tbl,
+      List<List<String>> values)  throws Throwable {
+    int i = 1;
+    List<Partition> partitions = new ArrayList<Partition>();
+    for(List<String> vals : values) {
+      Partition part = makePartitionObject(dbName, tbl.getTableName(), vals, tbl, "/part"+i);
+      i++;
+      // check if the partition exists (it shouldn't)
+      boolean exceptionThrown = false;
+      try {
+        Partition p = client.getPartition(dbName, tbl.getTableName(), vals);
+      } catch(Exception e) {
+        assertEquals("partition should not have existed",
+            NoSuchObjectException.class, e.getClass());
+        exceptionThrown = true;
+      }
+      assertTrue("getPartition() should have thrown NoSuchObjectException", exceptionThrown);
+      Partition retp = client.add_partition(part);
+      assertNotNull("Unable to create partition " + part, retp);
+      partitions.add(retp);
+    }
+    return partitions;
+  }
+
+  private void createMultiPartitionTableSchema(String dbName, String tblName,
+      String typeName, List<List<String>> values)
+      throws Throwable, MetaException, TException, NoSuchObjectException {
+    createDb(dbName);
+
+    Map<String, String> fields = new HashMap<String, String>();
+    fields.put("name", Constants.STRING_TYPE_NAME);
+    fields.put("income", Constants.INT_TYPE_NAME);
+
+    Type typ1 = createType(typeName, fields);
+
+    Map<String , String> partitionKeys = new HashMap<String, String>();
+    partitionKeys.put("ds", Constants.STRING_TYPE_NAME);
+    partitionKeys.put("hr", Constants.STRING_TYPE_NAME);
+
+    Map<String, String> params = new HashMap<String, String>();
+    params.put("test_param_1", "Use this for comments etc");
+
+    Map<String, String> serdParams = new HashMap<String, String>();
+    serdParams.put(Constants.SERIALIZATION_FORMAT, "1");
+
+    StorageDescriptor sd =  createStorageDescriptor(tblName, typ1.getFields(), params, serdParams);
+
+    Table tbl = createTable(dbName, tblName, null, null, partitionKeys, sd, 0);
+
+    if (isThriftClient) {
+      // the createTable() above does not update the location in the 'tbl'
+      // object when the client is a thrift client and the code below relies
+      // on the location being present in the 'tbl' object - so get the table
+      // from the metastore
+      tbl = client.getTable(dbName, tblName);
+    }
+
+    createPartitions(dbName, tbl, values);
+  }
+
 }
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
index 5a439d4..bf7a1aa 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
@@ -2974,15 +2974,24 @@ public class DDLTask extends Task<DDLWork> implements Serializable {
             " is protected from being dropped");
       }
 
+      int partitionBatchSize = HiveConf.getIntVar(conf,
+        ConfVars.METASTORE_BATCH_RETRIEVE_TABLE_PARTITION_MAX);
+
       // We should check that all the partitions of the table can be dropped
       if (tbl != null && tbl.isPartitioned()) {
-        List<Partition> listPartitions = db.getPartitions(tbl);
-        for (Partition p: listPartitions) {
+        List<String> partitionNames = db.getPartitionNames(tbl.getTableName(), (short)-1);
+
+        for(int i=0; i < partitionNames.size(); i+= partitionBatchSize) {
+          List<String> partNames = partitionNames.subList(i, Math.min(i+partitionBatchSize,
+            partitionNames.size()));
+          List<Partition> listPartitions = db.getPartitionsByNames(tbl, partNames);
+          for (Partition p: listPartitions) {
             if (!p.canDrop()) {
               throw new HiveException("Table " + tbl.getTableName() +
-                  " Partition" + p.getName() +
-                  " is protected from being dropped");
+                " Partition" + p.getName() +
+                " is protected from being dropped");
             }
+          }
         }
       }
 
diff --git a/src/ql/src/test/queries/clientnegative/drop_table_failure3.q b/src/ql/src/test/queries/clientnegative/drop_table_failure3.q
new file mode 100644
index 0000000..534ce0b
--- /dev/null
+++ b/src/ql/src/test/queries/clientnegative/drop_table_failure3.q
@@ -0,0 +1,12 @@
+create database dtf3;
+use dtf3; 
+
+create table drop_table_failure_temp(col STRING) partitioned by (p STRING);
+
+alter table drop_table_failure_temp add partition (p ='p1');
+alter table drop_table_failure_temp add partition (p ='p2');
+alter table drop_table_failure_temp add partition (p ='p3');
+
+alter table drop_table_failure_temp partition (p ='p3') ENABLE NO_DROP;
+
+drop table drop_table_failure_temp;
diff --git a/src/ql/src/test/queries/clientpositive/drop_table2.q b/src/ql/src/test/queries/clientpositive/drop_table2.q
new file mode 100644
index 0000000..a3e8c5c
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/drop_table2.q
@@ -0,0 +1,15 @@
+SET hive.metastore.batch.retrieve.max=1;
+create table if not exists temp(col STRING) partitioned by (p STRING);
+alter table temp add if not exists partition (p ='p1');
+alter table temp add if not exists partition (p ='p2');
+alter table temp add if not exists partition (p ='p3');
+
+show partitions temp;
+
+drop table temp;
+
+create table if not exists temp(col STRING) partitioned by (p STRING);
+
+show partitions temp;
+
+drop table temp;
diff --git a/src/ql/src/test/results/clientnegative/drop_table_failure3.q.out b/src/ql/src/test/results/clientnegative/drop_table_failure3.q.out
new file mode 100644
index 0000000..655bf41
--- /dev/null
+++ b/src/ql/src/test/results/clientnegative/drop_table_failure3.q.out
@@ -0,0 +1,49 @@
+PREHOOK: query: create database dtf3
+PREHOOK: type: CREATEDATABASE
+POSTHOOK: query: create database dtf3
+POSTHOOK: type: CREATEDATABASE
+PREHOOK: query: use dtf3
+PREHOOK: type: SWITCHDATABASE
+POSTHOOK: query: use dtf3
+POSTHOOK: type: SWITCHDATABASE
+PREHOOK: query: create table drop_table_failure_temp(col STRING) partitioned by (p STRING)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table drop_table_failure_temp(col STRING) partitioned by (p STRING)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: dtf3@drop_table_failure_temp
+PREHOOK: query: alter table drop_table_failure_temp add partition (p ='p1')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: dtf3@drop_table_failure_temp
+POSTHOOK: query: alter table drop_table_failure_temp add partition (p ='p1')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: dtf3@drop_table_failure_temp
+POSTHOOK: Output: dtf3@drop_table_failure_temp@p=p1
+PREHOOK: query: alter table drop_table_failure_temp add partition (p ='p2')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: dtf3@drop_table_failure_temp
+POSTHOOK: query: alter table drop_table_failure_temp add partition (p ='p2')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: dtf3@drop_table_failure_temp
+POSTHOOK: Output: dtf3@drop_table_failure_temp@p=p2
+PREHOOK: query: alter table drop_table_failure_temp add partition (p ='p3')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: dtf3@drop_table_failure_temp
+POSTHOOK: query: alter table drop_table_failure_temp add partition (p ='p3')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: dtf3@drop_table_failure_temp
+POSTHOOK: Output: dtf3@drop_table_failure_temp@p=p3
+PREHOOK: query: alter table drop_table_failure_temp partition (p ='p3') ENABLE NO_DROP
+PREHOOK: type: ALTERPARTITION_PROTECTMODE
+PREHOOK: Input: dtf3@drop_table_failure_temp
+PREHOOK: Output: dtf3@drop_table_failure_temp@p=p3
+POSTHOOK: query: alter table drop_table_failure_temp partition (p ='p3') ENABLE NO_DROP
+POSTHOOK: type: ALTERPARTITION_PROTECTMODE
+POSTHOOK: Input: dtf3@drop_table_failure_temp
+POSTHOOK: Input: dtf3@drop_table_failure_temp@p=p3
+POSTHOOK: Output: dtf3@drop_table_failure_temp@p=p3
+PREHOOK: query: drop table drop_table_failure_temp
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: dtf3@drop_table_failure_temp
+PREHOOK: Output: dtf3@drop_table_failure_temp
+FAILED: Error in metadata: Table drop_table_failure_temp Partitionp=p3 is protected from being dropped
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
diff --git a/src/ql/src/test/results/clientpositive/drop_table2.q.out b/src/ql/src/test/results/clientpositive/drop_table2.q.out
new file mode 100644
index 0000000..b7588fa
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/drop_table2.q.out
@@ -0,0 +1,58 @@
+PREHOOK: query: create table if not exists temp(col STRING) partitioned by (p STRING)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table if not exists temp(col STRING) partitioned by (p STRING)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@temp
+PREHOOK: query: alter table temp add if not exists partition (p ='p1')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: default@temp
+POSTHOOK: query: alter table temp add if not exists partition (p ='p1')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: default@temp
+POSTHOOK: Output: default@temp@p=p1
+PREHOOK: query: alter table temp add if not exists partition (p ='p2')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: default@temp
+POSTHOOK: query: alter table temp add if not exists partition (p ='p2')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: default@temp
+POSTHOOK: Output: default@temp@p=p2
+PREHOOK: query: alter table temp add if not exists partition (p ='p3')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Input: default@temp
+POSTHOOK: query: alter table temp add if not exists partition (p ='p3')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Input: default@temp
+POSTHOOK: Output: default@temp@p=p3
+PREHOOK: query: show partitions temp
+PREHOOK: type: SHOWPARTITIONS
+POSTHOOK: query: show partitions temp
+POSTHOOK: type: SHOWPARTITIONS
+p=p1
+p=p2
+p=p3
+PREHOOK: query: drop table temp
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@temp
+PREHOOK: Output: default@temp
+POSTHOOK: query: drop table temp
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@temp
+POSTHOOK: Output: default@temp
+PREHOOK: query: create table if not exists temp(col STRING) partitioned by (p STRING)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table if not exists temp(col STRING) partitioned by (p STRING)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@temp
+PREHOOK: query: show partitions temp
+PREHOOK: type: SHOWPARTITIONS
+POSTHOOK: query: show partitions temp
+POSTHOOK: type: SHOWPARTITIONS
+PREHOOK: query: drop table temp
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@temp
+PREHOOK: Output: default@temp
+POSTHOOK: query: drop table temp
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@temp
+POSTHOOK: Output: default@temp
-- 
1.7.0.4

