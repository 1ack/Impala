From 8058533a4ac675de77c198fc832f7ceb3f7fe550 Mon Sep 17 00:00:00 2001
From: Zhenxiao Luo <zhenxiao@cloudera.com>
Date: Tue, 5 Jun 2012 14:02:38 -0700
Subject: [PATCH 026/148] HIVE-2979. Implement INCLUDE_HADOOP_MAJOR_VERSION test macro

Reason: Bug
Author: Zhenxiao Luo
Ref: CDH-5678
---
 .../test/org/apache/hadoop/hive/ql/QTestUtil.java  |   68 +++++---
 .../test/queries/clientnegative/archive_corrupt.q  |   18 ++
 .../test/queries/clientpositive/archive_corrupt.q  |   11 +-
 ql/src/test/queries/clientpositive/combine2.q      |   12 ++
 .../clientpositive/sample_islocalmode_hook.q       |   17 +-
 ql/src/test/queries/clientpositive/split_sample.q  |   26 ++--
 .../results/clientnegative/archive_corrupt.q.out   |   25 +++
 .../results/clientpositive/archive_corrupt.q.out   |   30 +++-
 ql/src/test/results/clientpositive/combine2.q.out  |   24 +++-
 .../clientpositive/sample_islocalmode_hook.q.out   |   37 ++---
 .../test/results/clientpositive/split_sample.q.out |  172 +-------------------
 11 files changed, 188 insertions(+), 252 deletions(-)
 create mode 100644 ql/src/test/queries/clientnegative/archive_corrupt.q
 create mode 100644 ql/src/test/results/clientnegative/archive_corrupt.q.out

diff --git a/src/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java b/src/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
index b67d371..d74885e 100644
--- a/src/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/src/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -23,6 +23,7 @@ import static org.apache.hadoop.hive.metastore.MetaStoreUtils.DEFAULT_DATABASE_N
 import java.io.BufferedInputStream;
 import java.io.BufferedReader;
 import java.io.BufferedWriter;
+import java.io.DataInputStream;
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.FileNotFoundException;
@@ -33,6 +34,7 @@ import java.io.InputStreamReader;
 import java.io.PrintStream;
 import java.io.Serializable;
 import java.io.UnsupportedEncodingException;
+import java.lang.UnsupportedOperationException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Deque;
@@ -45,6 +47,7 @@ import java.util.TreeMap;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
+import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileStatus;
@@ -276,53 +279,72 @@ public class QTestUtil {
   }
 
   public void addFile(File qf) throws Exception {
-
     FileInputStream fis = new FileInputStream(qf);
     BufferedInputStream bis = new BufferedInputStream(fis);
     BufferedReader br = new BufferedReader(new InputStreamReader(bis, "UTF8"));
     StringBuilder qsb = new StringBuilder();
 
     // Look for a hint to not run a test on some Hadoop versions
-    Pattern pattern = Pattern.compile("-- EXCLUDE_HADOOP_MAJOR_VERSIONS(.*)");
-
-
-    // Read the entire query
+    Pattern pattern = Pattern.compile("-- (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS\\((.*)\\)");
+    
     boolean excludeQuery = false;
+    boolean includeQuery = false;
+    Set<String> versionSet = new HashSet<String>();
     String hadoopVer = ShimLoader.getMajorVersion();
     String line;
+
+    // Read the entire query
     while ((line = br.readLine()) != null) {
 
-      // While we are reading the lines, detect whether this query wants to be
-      // excluded from running because the Hadoop version is incorrect
+      // Each qfile may include at most one INCLUDE or EXCLUDE directive.
+      //
+      // If a qfile contains an INCLUDE directive, and hadoopVer does
+      // not appear in the list of versions to include, then the qfile
+      // is skipped.
+      //
+      // If a qfile contains an EXCLUDE directive, and hadoopVer is
+      // listed in the list of versions to EXCLUDE, then the qfile is
+      // skipped.
+      //
+      // Otherwise, the qfile is included.
       Matcher matcher = pattern.matcher(line);
       if (matcher.find()) {
-        String group = matcher.group();
-        int start = group.indexOf('(');
-        int end = group.indexOf(')');
-        assert end > start;
-        // versions might be something like '0.17, 0.19'
-        String versions = group.substring(start+1, end);
-
-        Set<String> excludedVersionSet = new HashSet<String>();
-        for (String s : versions.split("\\,")) {
-          s = s.trim();
-          excludedVersionSet.add(s);
+        if (excludeQuery || includeQuery) {
+          String message = "QTestUtil: qfile " + qf.getName()
+            + " contains more than one reference to (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS";
+          throw new UnsupportedOperationException(message);
         }
-        if (excludedVersionSet.contains(hadoopVer)) {
+        
+        String prefix = matcher.group(1);
+        if ("EX".equals(prefix)) {
           excludeQuery = true;
+        } else {
+          includeQuery = true;
+        }
+
+        String versions = matcher.group(2);
+        for (String s : versions.split("\\,")) {
+          s = s.trim();
+          versionSet.add(s);
         }
       }
       qsb.append(line + "\n");
     }
     qMap.put(qf.getName(), qsb.toString());
-    if(excludeQuery) {
-      System.out.println("Due to the Hadoop Version ("+ hadoopVer + "), " +
-          "adding query " + qf.getName() + " to the set of tests to skip");
+    
+    if (excludeQuery && versionSet.contains(hadoopVer)) {
+      System.out.println("QTestUtil: " + qf.getName()
+        + " EXCLUDE list contains Hadoop Version " + hadoopVer + ". Skipping...");
+      qSkipSet.add(qf.getName());
+    } else if (includeQuery && !versionSet.contains(hadoopVer)) {
+      System.out.println("QTestUtil: " + qf.getName()
+        + " INCLUDE list does not contain Hadoop Version " + hadoopVer + ". Skipping...");
       qSkipSet.add(qf.getName());
-     }
+    }
     br.close();
   }
 
+
   /**
    * Clear out any side effects of running tests
    */
diff --git a/src/ql/src/test/queries/clientnegative/archive_corrupt.q b/src/ql/src/test/queries/clientnegative/archive_corrupt.q
new file mode 100644
index 0000000..bea2539
--- /dev/null
+++ b/src/ql/src/test/queries/clientnegative/archive_corrupt.q
@@ -0,0 +1,18 @@
+USE default;
+
+set hive.archive.enabled = true;
+set hive.enforce.bucketing = true;
+
+drop table tstsrcpart;
+
+create table tstsrcpart like srcpart;
+
+-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
+-- The version of GzipCodec that is provided in Hadoop 0.20 silently ignores
+-- file format errors. However, versions of Hadoop that include
+-- HADOOP-6835 (e.g. 0.23 and 1.x) cause a Wrong File Format exception
+-- to be thrown during the LOAD step. This former behavior is tested
+-- in clientpositive/archive_corrupt.q
+
+load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11');
+
diff --git a/src/ql/src/test/queries/clientpositive/archive_corrupt.q b/src/ql/src/test/queries/clientpositive/archive_corrupt.q
index 0e05983..b83eab5 100644
--- a/src/ql/src/test/queries/clientpositive/archive_corrupt.q
+++ b/src/ql/src/test/queries/clientpositive/archive_corrupt.q
@@ -1,3 +1,5 @@
+USE default;
+
 set hive.archive.enabled = true;
 set hive.enforce.bucketing = true;
 
@@ -5,6 +7,13 @@ drop table tstsrcpart;
 
 create table tstsrcpart like srcpart;
 
+-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
+-- The version of GzipCodec provided in Hadoop 0.20 silently ignores
+-- file format errors. However, versions of Hadoop that include
+-- HADOOP-6835 (e.g. 0.23 and 1.x) cause a Wrong File Format exception
+-- to be thrown during the LOAD step. This behavior is now tested in
+-- clientnegative/archive_corrupt.q
+
 load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11');
 
 insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='12')
@@ -16,8 +25,6 @@ select key, value from srcpart where ds='2008-04-09' and hr='11';
 insert overwrite table tstsrcpart partition (ds='2008-04-09', hr='12')
 select key, value from srcpart where ds='2008-04-09' and hr='12';
 
--- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)
-
 describe extended tstsrcpart partition (ds='2008-04-08', hr='11');
 
 alter table tstsrcpart archive partition (ds='2008-04-08', hr='11');
diff --git a/src/ql/src/test/queries/clientpositive/combine2.q b/src/ql/src/test/queries/clientpositive/combine2.q
index e2466d4..c0baa44 100644
--- a/src/ql/src/test/queries/clientpositive/combine2.q
+++ b/src/ql/src/test/queries/clientpositive/combine2.q
@@ -1,3 +1,5 @@
+USE default;
+
 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
 set mapred.min.split.size=256;
 set mapred.min.split.size.per.node=256;
@@ -8,8 +10,18 @@ set hive.exec.dynamic.partition.mode=nonstrict;
 set mapred.cache.shared.enabled=false;
 set hive.merge.smallfiles.avgsize=0;
 
+
+
 create table combine2(key string) partitioned by (value string);
 
+-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.22, 0.23)
+-- This test sets mapred.max.split.size=256 and hive.merge.smallfiles.avgsize=0
+-- in an attempt to force the generation of multiple splits and multiple output files.
+-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
+-- when using CombineFileInputFormat, so only one split is generated. This has a
+-- significant impact on the results results of this test.
+-- This issue was fixed in MAPREDUCE-2046 which is included in 0.22.
+
 insert overwrite table combine2 partition(value) 
 select * from (
    select key, value from src where key < 10
diff --git a/src/ql/src/test/queries/clientpositive/sample_islocalmode_hook.q b/src/ql/src/test/queries/clientpositive/sample_islocalmode_hook.q
index 871ae05..1647270 100644
--- a/src/ql/src/test/queries/clientpositive/sample_islocalmode_hook.q
+++ b/src/ql/src/test/queries/clientpositive/sample_islocalmode_hook.q
@@ -1,6 +1,4 @@
-drop table if exists sih_i_part;
-drop table if exists sih_src;
-drop table if exists sih_src2;
+USE default;
 
 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
 set mapred.max.split.size=300;
@@ -22,7 +20,14 @@ set hive.exec.post.hooks = org.apache.hadoop.hive.ql.hooks.VerifyIsLocalModeHook
 set mapred.job.tracker=does.notexist.com:666;
 set hive.exec.mode.local.auto.input.files.max=1;
 
--- sample split, running locally limited by num tasks
+-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.22, 0.23)
+-- This test sets mapred.max.split.size=300 and hive.merge.smallfiles.avgsize=1
+-- in an attempt to force the generation of multiple splits and multiple output files.
+-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
+-- when using CombineFileInputFormat, so only one split is generated. This has a
+-- significant impact on the results of the TABLESAMPLE(x PERCENT). This issue was
+-- fixed in MAPREDUCE-2046 which is included in 0.22.
+-- Sample split, running locally limited by num tasks
 select count(1) from sih_src tablesample(1 percent);
 
 set mapred.job.tracker=does.notexist.com:666;
@@ -36,7 +41,3 @@ set mapred.job.tracker=does.notexist.com:666;
 
 -- sample split, running locally limited by max bytes
 select count(1) from sih_src tablesample(1 percent);
-
-drop table sih_i_part;
-drop table sih_src;
-drop table sih_src2;
diff --git a/src/ql/src/test/queries/clientpositive/split_sample.q b/src/ql/src/test/queries/clientpositive/split_sample.q
index 1e913d9..cc7597f 100644
--- a/src/ql/src/test/queries/clientpositive/split_sample.q
+++ b/src/ql/src/test/queries/clientpositive/split_sample.q
@@ -1,10 +1,4 @@
-drop table ss_src1;
-drop table ss_src2;
-drop table ss_src3;
-drop table ss_i_part;
-drop table ss_t3;
-drop table ss_t4;
-drop table ss_t5;
+USE default;
 
 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
 set mapred.max.split.size=300;
@@ -19,6 +13,15 @@ insert overwrite table ss_i_part partition (p='1') select key, value from src;
 insert overwrite table ss_i_part partition (p='2') select key, value from src;
 insert overwrite table ss_i_part partition (p='3') select key, value from src;
 create table ss_src2 as select key, value from ss_i_part;
+
+-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.22, 0.23)
+-- This test sets mapred.max.split.size=300 and hive.merge.smallfiles.avgsize=1
+-- in an attempt to force the generation of multiple splits and multiple output files.
+-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
+-- when using CombineFileInputFormat, so only one split is generated. This has a
+-- significant impact on the results of the TABLESAMPLE(x PERCENT). This issue was
+-- fixed in MAPREDUCE-2046 which is included in 0.22.
+
 select count(1) from ss_src2 tablesample(1 percent);
 
 -- sample first split
@@ -75,12 +78,3 @@ set mapred.min.split.size.per.node=300000;
 set mapred.min.split.size.per.rack=300000;
 select count(1) from ss_src2 tablesample(1 percent);
 select count(1) from ss_src2 tablesample(50 percent);
-
-
-drop table ss_src1;
-drop table ss_src2;
-drop table ss_src3;
-drop table ss_i_part;
-drop table ss_t3;
-drop table ss_t4;
-drop table ss_t5;
diff --git a/src/ql/src/test/results/clientnegative/archive_corrupt.q.out b/src/ql/src/test/results/clientnegative/archive_corrupt.q.out
new file mode 100644
index 0000000..de2f7dd
--- /dev/null
+++ b/src/ql/src/test/results/clientnegative/archive_corrupt.q.out
@@ -0,0 +1,25 @@
+PREHOOK: query: USE default
+PREHOOK: type: SWITCHDATABASE
+POSTHOOK: query: USE default
+POSTHOOK: type: SWITCHDATABASE
+PREHOOK: query: drop table tstsrcpart
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table tstsrcpart
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table tstsrcpart like srcpart
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table tstsrcpart like srcpart
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@tstsrcpart
+PREHOOK: query: -- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
+-- The version of GzipCodec that is provided in Hadoop 0.20 silently ignores
+-- file format errors. However, versions of Hadoop that include
+-- HADOOP-6835 (e.g. 0.23 and 1.x) cause a Wrong File Format exception
+-- to be thrown during the LOAD step. This former behavior is tested
+-- in clientpositive/archive_corrupt.q
+
+load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11')
+PREHOOK: type: LOAD
+PREHOOK: Output: default@tstsrcpart
+Failed with exception Wrong file format. Please check the file's format.
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask
diff --git a/src/ql/src/test/results/clientpositive/archive_corrupt.q.out b/src/ql/src/test/results/clientpositive/archive_corrupt.q.out
index 536e6a5..f1ac0e4 100644
--- a/src/ql/src/test/results/clientpositive/archive_corrupt.q.out
+++ b/src/ql/src/test/results/clientpositive/archive_corrupt.q.out
@@ -1,3 +1,7 @@
+PREHOOK: query: USE default
+PREHOOK: type: SWITCHDATABASE
+POSTHOOK: query: USE default
+POSTHOOK: type: SWITCHDATABASE
 PREHOOK: query: drop table tstsrcpart
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: drop table tstsrcpart
@@ -7,10 +11,24 @@ PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table tstsrcpart like srcpart
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@tstsrcpart
-PREHOOK: query: load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11')
+PREHOOK: query: -- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
+-- The version of GzipCodec provided in Hadoop 0.20 silently ignores
+-- file format errors. However, versions of Hadoop that include
+-- HADOOP-6835 (e.g. 0.23 and 1.x) cause a Wrong File Format exception
+-- to be thrown during the LOAD step. This behavior is now tested in
+-- clientnegative/archive_corrupt.q
+
+load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11')
 PREHOOK: type: LOAD
 PREHOOK: Output: default@tstsrcpart
-POSTHOOK: query: load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11')
+POSTHOOK: query: -- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
+-- The version of GzipCodec provided in Hadoop 0.20 silently ignores
+-- file format errors. However, versions of Hadoop that include
+-- HADOOP-6835 (e.g. 0.23 and 1.x) cause a Wrong File Format exception
+-- to be thrown during the LOAD step. This behavior is now tested in
+-- clientnegative/archive_corrupt.q
+
+load data local inpath '../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11')
 POSTHOOK: type: LOAD
 POSTHOOK: Output: default@tstsrcpart
 POSTHOOK: Output: default@tstsrcpart@ds=2008-04-08/hr=11
@@ -56,13 +74,9 @@ POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-09,hr=11).key SIMPLE [(srcpar
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-09,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-09,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: -- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)
-
-describe extended tstsrcpart partition (ds='2008-04-08', hr='11')
+PREHOOK: query: describe extended tstsrcpart partition (ds='2008-04-08', hr='11')
 PREHOOK: type: DESCTABLE
-POSTHOOK: query: -- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)
-
-describe extended tstsrcpart partition (ds='2008-04-08', hr='11')
+POSTHOOK: query: describe extended tstsrcpart partition (ds='2008-04-08', hr='11')
 POSTHOOK: type: DESCTABLE
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tstsrcpart PARTITION(ds=2008-04-08,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]
diff --git a/src/ql/src/test/results/clientpositive/combine2.q.out b/src/ql/src/test/results/clientpositive/combine2.q.out
index 6c56de5..73d4bc5 100644
--- a/src/ql/src/test/results/clientpositive/combine2.q.out
+++ b/src/ql/src/test/results/clientpositive/combine2.q.out
@@ -1,9 +1,21 @@
+PREHOOK: query: USE default
+PREHOOK: type: SWITCHDATABASE
+POSTHOOK: query: USE default
+POSTHOOK: type: SWITCHDATABASE
 PREHOOK: query: create table combine2(key string) partitioned by (value string)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: create table combine2(key string) partitioned by (value string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@combine2
-PREHOOK: query: insert overwrite table combine2 partition(value) 
+PREHOOK: query: -- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.22)
+-- This test sets mapred.max.split.size=256 and hive.merge.smallfiles.avgsize=0
+-- in an attempt to force the generation of multiple splits and multiple output files.
+-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
+-- when using CombineFileInputFormat, so only one split is generated. This has a
+-- significant impact on the results results of this test.
+-- This issue was fixed in MAPREDUCE-2046 which is included in 0.22.
+
+insert overwrite table combine2 partition(value) 
 select * from (
    select key, value from src where key < 10
    union all 
@@ -13,7 +25,15 @@ select * from (
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@combine2
-POSTHOOK: query: insert overwrite table combine2 partition(value) 
+POSTHOOK: query: -- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.22)
+-- This test sets mapred.max.split.size=256 and hive.merge.smallfiles.avgsize=0
+-- in an attempt to force the generation of multiple splits and multiple output files.
+-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
+-- when using CombineFileInputFormat, so only one split is generated. This has a
+-- significant impact on the results results of this test.
+-- This issue was fixed in MAPREDUCE-2046 which is included in 0.22.
+
+insert overwrite table combine2 partition(value) 
 select * from (
    select key, value from src where key < 10
    union all 
diff --git a/src/ql/src/test/results/clientpositive/sample_islocalmode_hook.q.out b/src/ql/src/test/results/clientpositive/sample_islocalmode_hook.q.out
index 9f35f2a..d623d9f 100644
--- a/src/ql/src/test/results/clientpositive/sample_islocalmode_hook.q.out
+++ b/src/ql/src/test/results/clientpositive/sample_islocalmode_hook.q.out
@@ -1,15 +1,7 @@
-PREHOOK: query: drop table if exists sih_i_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table if exists sih_i_part
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table if exists sih_src
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table if exists sih_src
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table if exists sih_src2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table if exists sih_src2
-POSTHOOK: type: DROPTABLE
+PREHOOK: query: USE default
+PREHOOK: type: SWITCHDATABASE
+POSTHOOK: query: USE default
+POSTHOOK: type: SWITCHDATABASE
 PREHOOK: query: -- create file inputs
 create table sih_i_part (key int, value string) partitioned by (p string)
 PREHOOK: type: CREATETABLE
@@ -83,7 +75,14 @@ POSTHOOK: Lineage: sih_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchem
 POSTHOOK: Lineage: sih_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: sih_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: sih_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: -- sample split, running locally limited by num tasks
+PREHOOK: query: -- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.22)
+-- This test sets mapred.max.split.size=300 and hive.merge.smallfiles.avgsize=1
+-- in an attempt to force the generation of multiple splits and multiple output files.
+-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
+-- when using CombineFileInputFormat, so only one split is generated. This has a
+-- significant impact on the results of the TABLESAMPLE(x PERCENT). This issue was
+-- fixed in MAPREDUCE-2046 which is included in 0.22.
+-- Sample split, running locally limited by num tasks
 select count(1) from sih_src tablesample(1 percent)
 PREHOOK: type: QUERY
 PREHOOK: Input: default@sih_src
@@ -102,15 +101,3 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@sih_src
 #### A masked pattern was here ####
 1500
-PREHOOK: query: drop table sih_i_part
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@sih_i_part
-PREHOOK: Output: default@sih_i_part
-PREHOOK: query: drop table sih_src
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@sih_src
-PREHOOK: Output: default@sih_src
-PREHOOK: query: drop table sih_src2
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@sih_src2
-PREHOOK: Output: default@sih_src2
diff --git a/src/ql/src/test/results/clientpositive/split_sample.q.out b/src/ql/src/test/results/clientpositive/split_sample.q.out
index cc622c8..b1882d7 100644
--- a/src/ql/src/test/results/clientpositive/split_sample.q.out
+++ b/src/ql/src/test/results/clientpositive/split_sample.q.out
@@ -1,31 +1,7 @@
-PREHOOK: query: drop table ss_src1
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table ss_src1
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table ss_src2
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table ss_src2
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table ss_src3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table ss_src3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table ss_i_part
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table ss_i_part
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table ss_t3
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table ss_t3
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table ss_t4
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table ss_t4
-POSTHOOK: type: DROPTABLE
-PREHOOK: query: drop table ss_t5
-PREHOOK: type: DROPTABLE
-POSTHOOK: query: drop table ss_t5
-POSTHOOK: type: DROPTABLE
+PREHOOK: query: USE default
+PREHOOK: type: SWITCHDATABASE
+POSTHOOK: query: USE default
+POSTHOOK: type: SWITCHDATABASE
 PREHOOK: query: -- create multiple file inputs (two enable multiple splits)
 create table ss_i_part (key int, value string) partitioned by (p string)
 PREHOOK: type: CREATETABLE
@@ -4309,143 +4285,3 @@ POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(n
 POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 1000
-PREHOOK: query: drop table ss_src1
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@ss_src1
-PREHOOK: Output: default@ss_src1
-POSTHOOK: query: drop table ss_src1
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@ss_src1
-POSTHOOK: Output: default@ss_src1
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table ss_src2
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@ss_src2
-PREHOOK: Output: default@ss_src2
-POSTHOOK: query: drop table ss_src2
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@ss_src2
-POSTHOOK: Output: default@ss_src2
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table ss_src3
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@ss_src3
-PREHOOK: Output: default@ss_src3
-POSTHOOK: query: drop table ss_src3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@ss_src3
-POSTHOOK: Output: default@ss_src3
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table ss_i_part
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@ss_i_part
-PREHOOK: Output: default@ss_i_part
-POSTHOOK: query: drop table ss_i_part
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@ss_i_part
-POSTHOOK: Output: default@ss_i_part
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table ss_t3
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@ss_t3
-PREHOOK: Output: default@ss_t3
-POSTHOOK: query: drop table ss_t3
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@ss_t3
-POSTHOOK: Output: default@ss_t3
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table ss_t4
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@ss_t4
-PREHOOK: Output: default@ss_t4
-POSTHOOK: query: drop table ss_t4
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@ss_t4
-POSTHOOK: Output: default@ss_t4
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: drop table ss_t5
-PREHOOK: type: DROPTABLE
-PREHOOK: Input: default@ss_t5
-PREHOOK: Output: default@ss_t5
-POSTHOOK: query: drop table ss_t5
-POSTHOOK: type: DROPTABLE
-POSTHOOK: Input: default@ss_t5
-POSTHOOK: Output: default@ss_t5
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-- 
1.7.0.4

