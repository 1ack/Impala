From a960c3e5dd1f9b736cd93f89b5d200d073d80861 Mon Sep 17 00:00:00 2001
From: Matteo Bertozzi <matteo.bertozzi@cloudera.com>
Date: Sat, 29 Dec 2012 11:30:04 +0000
Subject: [PATCH 112/196] HBASE-7207 Consolidate snapshot related classes into fewer packages

Reason: Snapshots
Author: Jonathan Hsieh
Ref: CDH-9551
---
 .../org/apache/hadoop/hbase/client/HBaseAdmin.java |    8 +-
 .../org/apache/hadoop/hbase/master/HMaster.java    |    2 +-
 .../master/snapshot/CloneSnapshotHandler.java      |    4 +-
 .../snapshot/DisabledTableSnapshotHandler.java     |    9 +-
 .../master/snapshot/MasterSnapshotVerifier.java    |    4 +-
 .../master/snapshot/RestoreSnapshotHandler.java    |    4 +-
 .../hbase/master/snapshot/SnapshotLogCleaner.java  |    1 -
 .../hbase/master/snapshot/SnapshotManager.java     |  719 +++++++++++++++++++
 .../hbase/master/snapshot/TakeSnapshotHandler.java |    4 +-
 .../master/snapshot/manage/SnapshotManager.java    |  725 --------------------
 .../hbase/server/snapshot/TakeSnapshotUtils.java   |  325 ---------
 .../snapshot/task/CopyRecoveredEditsTask.java      |   90 ---
 .../snapshot/task/ReferenceRegionHFilesTask.java   |  128 ----
 .../snapshot/task/ReferenceServerWALsTask.java     |  107 ---
 .../hbase/server/snapshot/task/SnapshotTask.java   |   65 --
 .../server/snapshot/task/TableInfoCopyTask.java    |   73 --
 .../hbase/snapshot/CopyRecoveredEditsTask.java     |   90 +++
 .../hbase/snapshot/CorruptedSnapshotException.java |   56 ++
 .../hadoop/hbase/snapshot/ExportSnapshot.java      |  719 +++++++++++++++++++
 .../hbase/snapshot/ExportSnapshotException.java    |   43 ++
 .../hbase/snapshot/HBaseSnapshotException.java     |   77 ++
 .../hbase/snapshot/ReferenceRegionHFilesTask.java  |  127 ++++
 .../hbase/snapshot/ReferenceServerWALsTask.java    |  105 +++
 .../hbase/snapshot/RestoreSnapshotException.java   |   43 ++
 .../hbase/snapshot/RestoreSnapshotHelper.java      |  424 ++++++++++++
 .../hbase/snapshot/SnapshotCreationException.java  |   54 ++
 .../hbase/snapshot/SnapshotDescriptionUtils.java   |    2 -
 .../snapshot/SnapshotDoesNotExistException.java    |   45 ++
 .../hbase/snapshot/SnapshotExistsException.java    |   40 ++
 .../hadoop/hbase/snapshot/SnapshotLogSplitter.java |  196 ++++++
 .../apache/hadoop/hbase/snapshot/SnapshotTask.java |   65 ++
 .../hadoop/hbase/snapshot/TableInfoCopyTask.java   |   72 ++
 .../snapshot/TablePartiallyOpenException.java      |   51 ++
 .../hadoop/hbase/snapshot/TakeSnapshotUtils.java   |  324 +++++++++
 .../snapshot/UnexpectedSnapshotException.java      |   42 ++
 .../hbase/snapshot/UnknownSnapshotException.java   |   42 ++
 .../exception/CorruptedSnapshotException.java      |   56 --
 .../exception/ExportSnapshotException.java         |   43 --
 .../snapshot/exception/HBaseSnapshotException.java |   77 --
 .../exception/RestoreSnapshotException.java        |   43 --
 .../exception/SnapshotCreationException.java       |   54 --
 .../exception/SnapshotDoesNotExistException.java   |   45 --
 .../exception/SnapshotExistsException.java         |   40 --
 .../exception/TablePartiallyOpenException.java     |   51 --
 .../exception/UnexpectedSnapshotException.java     |   42 --
 .../exception/UnknownSnapshotException.java        |   42 --
 .../snapshot/restore/RestoreSnapshotHelper.java    |  425 ------------
 .../snapshot/restore/SnapshotLogSplitter.java      |  200 ------
 .../hadoop/hbase/snapshot/tool/ExportSnapshot.java |  702 -------------------
 .../client/TestRestoreSnapshotFromClient.java      |    2 +-
 .../hbase/client/TestSnapshotFromClient.java       |    2 +-
 .../master/cleaner/TestSnapshotFromMaster.java     |    2 +-
 .../master/snapshot/TestSnapshotFileCache.java     |    2 +-
 .../hbase/master/snapshot/TestSnapshotManager.java |   75 ++
 .../snapshot/manage/TestSnapshotManager.java       |   82 ---
 .../snapshot/task/TestCopyRecoveredEditsTask.java  |  127 ----
 .../task/TestReferenceRegionHFilesTask.java        |   91 ---
 .../server/snapshot/task/TestSnapshotTask.java     |   57 --
 .../server/snapshot/task/TestWALReferenceTask.java |  102 ---
 .../hbase/snapshot/SnapshotTestingUtils.java       |    4 +-
 .../hbase/snapshot/TestCopyRecoveredEditsTask.java |  126 ++++
 .../hadoop/hbase/snapshot/TestExportSnapshot.java  |  255 +++++++
 .../snapshot/TestReferenceRegionHFilesTask.java    |   92 +++
 .../hbase/snapshot/TestSnapshotLogSplitter.java    |  176 +++++
 .../hadoop/hbase/snapshot/TestSnapshotTask.java    |   58 ++
 .../hbase/snapshot/TestWALReferenceTask.java       |  103 +++
 .../snapshot/restore/TestSnapshotLogSplitter.java  |  176 -----
 .../hbase/snapshot/tool/TestExportSnapshot.java    |  254 -------
 68 files changed, 4242 insertions(+), 4249 deletions(-)
 create mode 100644 src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/CopyRecoveredEditsTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/CorruptedSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceRegionHFilesTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceServerWALsTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDoesNotExistException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotExistsException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotLogSplitter.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/TableInfoCopyTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/TablePartiallyOpenException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/TakeSnapshotUtils.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/UnexpectedSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/ExportSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/RestoreSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/restore/RestoreSnapshotHelper.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/restore/SnapshotLogSplitter.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/tool/ExportSnapshot.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotManager.java
 delete mode 100644 src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java
 delete mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java
 delete mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java
 delete mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java
 delete mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/TestCopyRecoveredEditsTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/TestReferenceRegionHFilesTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotLogSplitter.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/TestWALReferenceTask.java
 delete mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/restore/TestSnapshotLogSplitter.java
 delete mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/tool/TestExportSnapshot.java

diff --git a/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java b/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index 4ba4f99..d90e763 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -60,12 +60,12 @@ import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.CompactionState;
 import org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException;
-import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
-import org.apache.hadoop.hbase.snapshot.exception.RestoreSnapshotException;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
-import org.apache.hadoop.hbase.snapshot.exception.UnknownSnapshotException;
+import org.apache.hadoop.hbase.snapshot.HBaseSnapshotException;
 import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.RestoreSnapshotException;
+import org.apache.hadoop.hbase.snapshot.SnapshotCreationException;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.UnknownSnapshotException;
 import org.apache.hadoop.hbase.util.Addressing;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
diff --git a/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index e46825b..a517ed7 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -89,7 +89,7 @@ import org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler;
 import org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler;
 import org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler;
 import org.apache.hadoop.hbase.master.metrics.MasterMetrics;
-import org.apache.hadoop.hbase.master.snapshot.manage.SnapshotManager;
+import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
index 9848d5d..d2128fc 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
@@ -39,9 +39,9 @@ import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.SnapshotSentinel;
 import org.apache.hadoop.hbase.master.handler.CreateTableHandler;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.RestoreSnapshotException;
+import org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.exception.RestoreSnapshotException;
-import org.apache.hadoop.hbase.snapshot.restore.RestoreSnapshotHelper;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
index f304e95..030d809 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
@@ -30,15 +30,14 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.errorhandling.ForeignException;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
 import org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
-import org.apache.hadoop.hbase.server.snapshot.task.CopyRecoveredEditsTask;
-import org.apache.hadoop.hbase.server.snapshot.task.ReferenceRegionHFilesTask;
-import org.apache.hadoop.hbase.server.snapshot.task.TableInfoCopyTask;
+import org.apache.hadoop.hbase.snapshot.CopyRecoveredEditsTask;
+import org.apache.hadoop.hbase.snapshot.ReferenceRegionHFilesTask;
+import org.apache.hadoop.hbase.snapshot.TableInfoCopyTask;
+import org.apache.hadoop.hbase.snapshot.TakeSnapshotUtils;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.zookeeper.KeeperException;
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java
index 898d6ec..6dec308 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java
@@ -37,9 +37,9 @@ import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescriptio
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription.Type;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
-import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.exception.CorruptedSnapshotException;
+import org.apache.hadoop.hbase.snapshot.TakeSnapshotUtils;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java
index 0f88ed2..795b14e 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java
@@ -39,9 +39,9 @@ import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.SnapshotSentinel;
 import org.apache.hadoop.hbase.master.handler.TableEventHandler;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.RestoreSnapshotException;
+import org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.exception.RestoreSnapshotException;
-import org.apache.hadoop.hbase.snapshot.restore.RestoreSnapshotHelper;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotLogCleaner.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotLogCleaner.java
index 80321cc..5fbc67f 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotLogCleaner.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotLogCleaner.java
@@ -27,7 +27,6 @@ import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate;
 import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
 import org.apache.hadoop.hbase.util.FSUtils;
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
new file mode 100644
index 0000000..541e6e1
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
@@ -0,0 +1,719 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.snapshot;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.Stoppable;
+import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.errorhandling.ForeignException;
+import org.apache.hadoop.hbase.executor.ExecutorService;
+import org.apache.hadoop.hbase.master.AssignmentManager;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.master.MasterFileSystem;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.SnapshotSentinel;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription.Type;
+import org.apache.hadoop.hbase.snapshot.HBaseSnapshotException;
+import org.apache.hadoop.hbase.snapshot.RestoreSnapshotException;
+import org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper;
+import org.apache.hadoop.hbase.snapshot.SnapshotCreationException;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.SnapshotDoesNotExistException;
+import org.apache.hadoop.hbase.snapshot.SnapshotExistsException;
+import org.apache.hadoop.hbase.snapshot.TablePartiallyOpenException;
+import org.apache.hadoop.hbase.snapshot.UnknownSnapshotException;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
+
+/**
+ * This class manages the procedure of taking and restoring snapshots. There is only one
+ * SnapshotManager for the master.
+ * <p>
+ * The class provides methods for monitoring in-progress snapshot actions.
+ * <p>
+ * Note: Currently there can only one snapshot being taken at a time over the cluster.  This is a
+ * simplification in the current implementation.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public class SnapshotManager implements Stoppable {
+  private static final Log LOG = LogFactory.getLog(SnapshotManager.class);
+
+  /** By default, check to see if the snapshot is complete every WAKE MILLIS (ms) */
+  public static final int SNAPSHOT_WAKE_MILLIS_DEFAULT = 500;
+
+  /**
+   * Conf key for # of ms elapsed between checks for snapshot errors while waiting for
+   * completion.
+   */
+  public static final String SNAPSHOT_WAKE_MILLIS_KEY = "hbase.snapshot.master.wakeMillis";
+
+  /** By default, check to see if the snapshot is complete (ms) */
+  public static final int SNAPSHOT_TIMEOUT_MILLIS_DEFAULT = 5000;
+
+  /**
+   * Conf key for # of ms elapsed before injecting a snapshot timeout error when waiting for
+   * completion.
+   */
+  public static final String SNAPSHOT_TIMEMOUT_MILLIS_KEY = "hbase.snapshot.master.timeoutMillis";
+
+  /** Name of the operation to use in the controller */
+  public static final String ONLINE_SNAPSHOT_CONTROLLER_DESCRIPTION = "online-snapshot";
+
+  // TODO - enable having multiple snapshots with multiple monitors/threads
+  // this needs to be configuration based when running multiple snapshots is implemented
+  /** number of current operations running on the master */
+  private static final int opThreads = 1;
+
+  private boolean stopped;
+  private final long wakeFrequency;
+  private final MasterServices master;  // Needed by TableEventHandlers
+
+  // A reference to a handler.  If the handler is non-null, then it is assumed that a snapshot is
+  // in progress currently
+  // TODO: this is a bad smell;  likely replace with a collection in the future.  Also this gets
+  // reset by every operation.
+  private TakeSnapshotHandler handler;
+
+  private final Path rootDir;
+  private final ExecutorService executorService;
+
+  // Restore Sentinels map, with table name as key
+  private Map<String, SnapshotSentinel> restoreHandlers = new HashMap<String, SnapshotSentinel>();
+
+  /**
+   * Construct a snapshot manager.
+   * @param master
+   * @param comms
+   */
+  public SnapshotManager(final MasterServices master) throws IOException {
+    this.master = master;
+
+    // get the configuration for the coordinator
+    Configuration conf = master.getConfiguration();
+    this.wakeFrequency = conf.getInt(SNAPSHOT_WAKE_MILLIS_KEY, SNAPSHOT_WAKE_MILLIS_DEFAULT);
+    this.rootDir = master.getMasterFileSystem().getRootDir();
+    this.executorService = master.getExecutorService();
+    resetTempDir();
+  }
+
+  /**
+   * Gets the list of all completed snapshots.
+   * @return list of SnapshotDescriptions
+   * @throws IOException File system exception
+   */
+  public List<SnapshotDescription> getCompletedSnapshots() throws IOException {
+    List<SnapshotDescription> snapshotDescs = new ArrayList<SnapshotDescription>();
+    // first create the snapshot root path and check to see if it exists
+    Path snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);
+    FileSystem fs = master.getMasterFileSystem().getFileSystem();
+
+    // if there are no snapshots, return an empty list
+    if (!fs.exists(snapshotDir)) {
+      return snapshotDescs;
+    }
+
+    // ignore all the snapshots in progress
+    FileStatus[] snapshots = fs.listStatus(snapshotDir,
+      new SnapshotDescriptionUtils.CompletedSnaphotDirectoriesFilter(fs));
+    // loop through all the completed snapshots
+    for (FileStatus snapshot : snapshots) {
+      Path info = new Path(snapshot.getPath(), SnapshotDescriptionUtils.SNAPSHOTINFO_FILE);
+      // if the snapshot is bad
+      if (!fs.exists(info)) {
+        LOG.error("Snapshot information for " + snapshot.getPath() + " doesn't exist");
+        continue;
+      }
+      FSDataInputStream in = null;
+      try {
+        in = fs.open(info);
+        SnapshotDescription desc = SnapshotDescription.parseFrom(in);
+        snapshotDescs.add(desc);
+      } catch (IOException e) {
+        LOG.warn("Found a corrupted snapshot " + snapshot.getPath(), e);
+      } finally {
+        if (in != null) {
+          in.close();
+        }
+      }
+    }
+    return snapshotDescs;
+  }
+
+  /**
+   * Cleans up any snapshots in the snapshot/.tmp directory that were left from failed
+   * snapshot attempts.
+   *
+   * @throws IOException if we can't reach the filesystem
+   */
+  void resetTempDir() throws IOException {
+    // cleanup any existing snapshots.
+    Path tmpdir = SnapshotDescriptionUtils.getWorkingSnapshotDir(rootDir);
+    if (master.getMasterFileSystem().getFileSystem().delete(tmpdir, true)) {
+      LOG.warn("Couldn't delete working snapshot directory: " + tmpdir);
+    }
+  }
+
+  /**
+   * Delete the specified snapshot
+   * @param snapshot
+   * @throws SnapshotDoesNotExistException If the specified snapshot does not exist.
+   * @throws IOException For filesystem IOExceptions
+   */
+  public void deleteSnapshot(SnapshotDescription snapshot) throws SnapshotDoesNotExistException, IOException {
+
+    // call coproc pre hook
+    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
+    if (cpHost != null) {
+      cpHost.preDeleteSnapshot(snapshot);
+    }
+
+    // check to see if it is completed
+    if (!isSnapshotCompleted(snapshot)) {
+      throw new SnapshotDoesNotExistException(snapshot);
+    }
+
+    String snapshotName = snapshot.getName();
+    LOG.debug("Deleting snapshot: " + snapshotName);
+    // first create the snapshot description and check to see if it exists
+    MasterFileSystem fs = master.getMasterFileSystem();
+    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
+
+    // delete the existing snapshot
+    if (!fs.getFileSystem().delete(snapshotDir, true)) {
+      throw new HBaseSnapshotException("Failed to delete snapshot directory: " + snapshotDir);
+    }
+
+    // call coproc post hook
+    if (cpHost != null) {
+      cpHost.postDeleteSnapshot(snapshot);
+    }
+
+  }
+
+  /**
+   * Return the handler if it is currently running and has the same snapshot target name.
+   * @param snapshot
+   * @return null if doesn't match, else a live handler.
+   */
+  TakeSnapshotHandler getTakeSnapshotHandler(SnapshotDescription snapshot) {
+    TakeSnapshotHandler h = this.handler;
+    if (h == null) {
+      return null;
+    }
+
+    if (!h.getSnapshot().getName().equals(snapshot.getName())) {
+      // specified snapshot is to the one currently running
+      return null;
+    }
+
+    return h;
+  }
+
+  /**
+   * Check if the specified snapshot is done
+   * @param expected
+   * @return true if snapshot is ready to be restored, false if it is still being taken.
+   * @throws IOException IOException if error from HDFS or RPC
+   * @throws UnknownSnapshotException if snapshot is invalid or does not exist.
+   */
+  public boolean isSnapshotDone(SnapshotDescription expected) throws IOException {
+    // check the request to make sure it has a snapshot
+    if (expected == null) {
+      throw new UnknownSnapshotException(
+         "No snapshot name passed in request, can't figure out which snapshot you want to check.");
+    }
+
+    // check to see if the sentinel exists
+    TakeSnapshotHandler handler = getTakeSnapshotHandler(expected);
+    if (handler == null) {
+      // doesn't exist, check if it is already completely done.
+      if (!isSnapshotCompleted(expected)) {
+        throw new UnknownSnapshotException("Snapshot:" + expected.getName()
+            + " is not currently running or one of the known completed snapshots.");
+      }
+      // was done, return true;
+      return true;
+    }
+
+    // pass on any failure we find in the sentinel
+    try {
+      handler.rethrowException();
+    } catch (ForeignException e) {
+      throw new HBaseSnapshotException("Snapshot error from RS", e, expected);
+    }
+
+    // check to see if we are done
+    if (handler.isFinished()) {
+      LOG.debug("Snapshot '" + expected.getName() + "' has completed, notifying client.");
+      return true;
+    } else if (LOG.isDebugEnabled()) {
+      LOG.debug("Sentinel isn't finished with snapshot '" + expected.getName() + "'!");
+    }
+    return false;
+  }
+
+  /**
+   * Check to see if there are any snapshots in progress currently.  Currently we have a
+   * limitation only allowing a single snapshot attempt at a time.
+   * @return <tt>true</tt> if there any snapshots in progress, <tt>false</tt> otherwise
+   * @throws SnapshotCreationException if the snapshot failed
+   */
+  synchronized boolean isTakingSnapshot() throws SnapshotCreationException {
+    // TODO later when we handle multiple there would be a map with ssname to handler.
+    return handler != null && !handler.isFinished();
+  }
+
+  /**
+   * Check to see if the specified table has a snapshot in progress.  Currently we have a
+   * limitation only allowing a single snapshot attempt at a time.
+   * @param tableName name of the table being snapshotted.
+   * @return <tt>true</tt> if there is a snapshot in progress on the specified table.
+   */
+  private boolean isTakingSnapshot(final String tableName) {
+    if (handler != null && handler.getSnapshot().getTable().equals(tableName)) {
+      return !handler.isFinished();
+    }
+    return false;
+  }
+
+  /**
+   * Check to make sure that we are OK to run the passed snapshot. Checks to make sure that we
+   * aren't already running a snapshot.
+   * @param snapshot description of the snapshot we want to start
+   * @throws HBaseSnapshotException if the filesystem could not be prepared to start the snapshot
+   */
+  private synchronized void prepareToTakeSnapshot(SnapshotDescription snapshot)
+      throws HBaseSnapshotException {
+    FileSystem fs = master.getMasterFileSystem().getFileSystem();
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
+
+    // make sure we aren't already running a snapshot
+    if (isTakingSnapshot()) {
+      throw new SnapshotCreationException("Already running another snapshot:"
+          + this.handler.getSnapshot(), snapshot);
+    }
+
+    // make sure we aren't running a restore on the same table
+    if (isRestoringTable(snapshot.getTable())) {
+      throw new SnapshotCreationException("Restore in progress on the same table snapshot:"
+          + this.handler.getSnapshot(), snapshot);
+    }
+
+    try {
+      // delete the working directory, since we aren't running the snapshot.  Likely leftovers
+      // from a failed attempt.
+      fs.delete(workingDir, true);
+
+      // recreate the working directory for the snapshot
+      if (!fs.mkdirs(workingDir)) {
+        throw new SnapshotCreationException("Couldn't create working directory (" + workingDir
+            + ") for snapshot.", snapshot);
+      }
+    } catch (HBaseSnapshotException e) {
+      throw e;
+    } catch (IOException e) {
+      throw new SnapshotCreationException(
+          "Exception while checking to see if snapshot could be started.", e, snapshot);
+    }
+  }
+
+  /**
+   * Take a snapshot based on the enabled/disabled state of the table.
+   *
+   * @param snapshot
+   * @throws HBaseSnapshotException when a snapshot specific exception occurs.
+   * @throws IOException when some sort of generic IO exception occurs.
+   */
+  public void takeSnapshot(SnapshotDescription snapshot) throws HBaseSnapshotException, IOException {
+    // check to see if we already completed the snapshot
+    if (isSnapshotCompleted(snapshot)) {
+      throw new SnapshotExistsException("Snapshot '" + snapshot.getName()
+          + "' already stored on the filesystem.", snapshot);
+    }
+
+    LOG.debug("No existing snapshot, attempting snapshot...");
+
+    // check to see if the table exists
+    HTableDescriptor desc = null;
+    try {
+      desc = master.getTableDescriptors().get(snapshot.getTable());
+    } catch (FileNotFoundException e) {
+      String msg = "Table:" + snapshot.getTable() + " info doesn't exist!";
+      LOG.error(msg);
+      throw new SnapshotCreationException(msg, e, snapshot);
+    } catch (IOException e) {
+      throw new SnapshotCreationException("Error while geting table description for table "
+          + snapshot.getTable(), e, snapshot);
+    }
+    if (desc == null) {
+      throw new SnapshotCreationException("Table '" + snapshot.getTable()
+          + "' doesn't exist, can't take snapshot.", snapshot);
+    }
+
+    // set the snapshot version, now that we are ready to take it
+    snapshot = snapshot.toBuilder().setVersion(SnapshotDescriptionUtils.SNAPSHOT_LAYOUT_VERSION)
+        .build();
+
+    // call pre coproc hook
+    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
+    if (cpHost != null) {
+      cpHost.preSnapshot(snapshot, desc);
+    }
+
+    // setup the snapshot
+    prepareToTakeSnapshot(snapshot);
+
+    // if the table is enabled, then have the RS run actually the snapshot work
+    AssignmentManager assignmentMgr = master.getAssignmentManager();
+    if (assignmentMgr.getZKTable().isEnabledTable(snapshot.getTable())) {
+      LOG.debug("Table enabled, starting distributed snapshot.");
+      throw new UnsupportedOperationException("Snapshots of enabled tables is not yet supported");
+    }
+    // For disabled table, snapshot is created by the master
+    else if (assignmentMgr.getZKTable().isDisabledTable(snapshot.getTable())) {
+      LOG.debug("Table is disabled, running snapshot entirely on master.");
+      snapshotDisabledTable(snapshot);
+      LOG.debug("Started snapshot: " + snapshot);
+    } else {
+      LOG.error("Can't snapshot table '" + snapshot.getTable()
+          + "', isn't open or closed, we don't know what to do!");
+      TablePartiallyOpenException tpoe = new TablePartiallyOpenException(snapshot.getTable()
+          + " isn't fully open.");
+      throw new SnapshotCreationException("Table is not entirely open or closed", tpoe, snapshot);
+    }
+
+    // call post coproc hook
+    if (cpHost != null) {
+      cpHost.postSnapshot(snapshot, desc);
+    }
+  }
+
+  /**
+   * Take a snapshot of a disabled table.
+   * <p>
+   * Ensures the snapshot won't be started if there is another snapshot already running. Does
+   * <b>not</b> check to see if another snapshot of the same name already exists.
+   * @param snapshot description of the snapshot to take. Modified to be {@link Type#DISABLED}.
+   * @throws HBaseSnapshotException if the snapshot could not be started
+   */
+  private synchronized void snapshotDisabledTable(SnapshotDescription snapshot)
+      throws HBaseSnapshotException {
+
+    // set the snapshot to be a disabled snapshot, since the client doesn't know about that
+    snapshot = snapshot.toBuilder().setType(Type.DISABLED).build();
+
+    DisabledTableSnapshotHandler handler;
+    try {
+      handler = new DisabledTableSnapshotHandler(snapshot, this.master);
+      this.handler = handler;
+      this.executorService.submit(handler);
+    } catch (IOException e) {
+      // cleanup the working directory by trying to delete it from the fs.
+      Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
+      try {
+        if (this.master.getMasterFileSystem().getFileSystem().delete(workingDir, true)) {
+          LOG.error("Couldn't delete working directory (" + workingDir + " for snapshot:"
+              + snapshot);
+        }
+      } catch (IOException e1) {
+        LOG.error("Couldn't delete working directory (" + workingDir + " for snapshot:" + snapshot);
+      }
+      // fail the snapshot
+      throw new SnapshotCreationException("Could not build snapshot handler", e, snapshot);
+    }
+  }
+
+  /**
+   * Set the handler for the current snapshot
+   * <p>
+   * Exposed for TESTING
+   * @param handler handler the master should use
+   *
+   * TODO get rid of this if possible, repackaging, modify tests.
+   */
+  public synchronized void setSnapshotHandlerForTesting(TakeSnapshotHandler handler) {
+    this.handler = handler;
+  }
+
+  /**
+   * Check to see if the snapshot is one of the currently completed snapshots
+   * @param expected snapshot to check
+   * @return <tt>true</tt> if the snapshot is stored on the {@link FileSystem}, <tt>false</tt> if is
+   *         not stored
+   * @throws IOException if the filesystem throws an unexpected exception,
+   * @throws IllegalArgumentException if snapshot name is invalid.
+   */
+  private boolean isSnapshotCompleted(SnapshotDescription snapshot) throws IOException {
+    try {
+      final Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshot, rootDir);
+      FileSystem fs = master.getMasterFileSystem().getFileSystem();
+
+      // check to see if the snapshot already exists
+      return fs.exists(snapshotDir);
+    } catch (IllegalArgumentException iae) {
+      throw new UnknownSnapshotException("Unexpected exception thrown", iae);
+    }
+  }
+
+  /**
+   * Restore the specified snapshot.
+   * The restore will fail if the destination table has a snapshot or restore in progress.
+   *
+   * @param snapshot Snapshot Descriptor
+   * @param hTableDescriptor Table Descriptor of the table to create
+   * @param waitTime timeout before considering the clone failed
+   */
+  synchronized void cloneSnapshot(final SnapshotDescription snapshot,
+      final HTableDescriptor hTableDescriptor) throws HBaseSnapshotException {
+    String tableName = hTableDescriptor.getNameAsString();
+
+    // make sure we aren't running a snapshot on the same table
+    if (isTakingSnapshot(tableName)) {
+      throw new RestoreSnapshotException("Snapshot in progress on the restore table=" + tableName);
+    }
+
+    // make sure we aren't running a restore on the same table
+    if (isRestoringTable(tableName)) {
+      throw new RestoreSnapshotException("Restore already in progress on the table=" + tableName);
+    }
+
+    try {
+      CloneSnapshotHandler handler =
+        new CloneSnapshotHandler(master, snapshot, hTableDescriptor);
+      this.executorService.submit(handler);
+      restoreHandlers.put(tableName, handler);
+    } catch (Exception e) {
+      String msg = "Couldn't clone the snapshot=" + snapshot + " on table=" + tableName;
+      LOG.error(msg, e);
+      throw new RestoreSnapshotException(msg, e);
+    }
+  }
+
+  /**
+   * Restore the specified snapshot
+   * @param reqSnapshot
+   * @throws IOException
+   */
+  public void restoreSnapshot(SnapshotDescription reqSnapshot) throws IOException {
+    FileSystem fs = master.getMasterFileSystem().getFileSystem();
+    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(reqSnapshot, rootDir);
+    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
+
+    // check if the snapshot exists
+    if (!fs.exists(snapshotDir)) {
+      LOG.error("A Snapshot named '" + reqSnapshot.getName() + "' does not exist.");
+      throw new SnapshotDoesNotExistException(reqSnapshot);
+    }
+
+    // read snapshot information
+    SnapshotDescription fsSnapshot = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
+    HTableDescriptor snapshotTableDesc = FSTableDescriptors.getTableDescriptor(fs, snapshotDir);
+    String tableName = reqSnapshot.getTable();
+
+    // stop tracking completed restores
+    cleanupRestoreSentinels();
+
+    // Execute the restore/clone operation
+    if (MetaReader.tableExists(master.getCatalogTracker(), tableName)) {
+      if (master.getAssignmentManager().getZKTable().isEnabledTable(fsSnapshot.getTable())) {
+        throw new UnsupportedOperationException("Table '" +
+          fsSnapshot.getTable() + "' must be disabled in order to perform a restore operation.");
+      }
+
+      // call coproc pre hook
+      if (cpHost != null) {
+        cpHost.preRestoreSnapshot(reqSnapshot, snapshotTableDesc);
+      }
+      restoreSnapshot(fsSnapshot, snapshotTableDesc);
+      LOG.info("Restore snapshot=" + fsSnapshot.getName() + " as table=" + tableName);
+
+      if (cpHost != null) {
+        cpHost.postRestoreSnapshot(reqSnapshot, snapshotTableDesc);
+      }
+    } else {
+      HTableDescriptor htd = RestoreSnapshotHelper.cloneTableSchema(snapshotTableDesc,
+                                                         Bytes.toBytes(tableName));
+      if (cpHost != null) {
+        cpHost.preCloneSnapshot(reqSnapshot, htd);
+      }
+      cloneSnapshot(fsSnapshot, htd);
+      LOG.info("Clone snapshot=" + fsSnapshot.getName() + " as table=" + tableName);
+
+      if (cpHost != null) {
+        cpHost.postCloneSnapshot(reqSnapshot, htd);
+      }
+    }
+  }
+
+  /**
+   * Restore the specified snapshot.
+   * The restore will fail if the destination table has a snapshot or restore in progress.
+   *
+   * @param snapshot Snapshot Descriptor
+   * @param hTableDescriptor Table Descriptor
+   * @param waitTime timeout before considering the restore failed
+   */
+  private synchronized void restoreSnapshot(final SnapshotDescription snapshot,
+      final HTableDescriptor hTableDescriptor) throws HBaseSnapshotException {
+    String tableName = hTableDescriptor.getNameAsString();
+
+    // make sure we aren't running a snapshot on the same table
+    if (isTakingSnapshot(tableName)) {
+      throw new RestoreSnapshotException("Snapshot in progress on the restore table=" + tableName);
+    }
+
+    // make sure we aren't running a restore on the same table
+    if (isRestoringTable(tableName)) {
+      throw new RestoreSnapshotException("Restore already in progress on the table=" + tableName);
+    }
+
+    try {
+      RestoreSnapshotHandler handler =
+        new RestoreSnapshotHandler(master, snapshot, hTableDescriptor);
+      this.executorService.submit(handler);
+      restoreHandlers.put(hTableDescriptor.getNameAsString(), handler);
+    } catch (Exception e) {
+      String msg = "Couldn't restore the snapshot=" + snapshot + " on table=" + tableName;
+      LOG.error(msg, e);
+      throw new RestoreSnapshotException(msg, e);
+    }
+  }
+
+  /**
+   * Verify if the the restore of the specified table is in progress.
+   *
+   * @param tableName table under restore
+   * @return <tt>true</tt> if there is a restore in progress of the specified table.
+   */
+  private boolean isRestoringTable(final String tableName) {
+    SnapshotSentinel sentinel = restoreHandlers.get(tableName);
+    return(sentinel != null && !sentinel.isFinished());
+  }
+
+  /**
+   * Returns status of a restore request, specifically comparing source snapshot and target table
+   * names.  Throws exception if not a known snapshot.
+   * @param snapshot
+   * @return true if in progress, false if is not.
+   * @throws UnknownSnapshotException if specified source snapshot does not exit.
+   * @throws IOException if there was some sort of IO failure
+   */
+  public boolean isRestoringTable(final SnapshotDescription snapshot) throws IOException {
+    // check to see if the snapshot is already on the fs
+    if (!isSnapshotCompleted(snapshot)) {
+      throw new UnknownSnapshotException("Snapshot:" + snapshot.getName()
+          + " is not one of the known completed snapshots.");
+    }
+
+    SnapshotSentinel sentinel = getRestoreSnapshotSentinel(snapshot.getTable());
+    if (sentinel == null) {
+      // there is no sentinel so restore is not in progress.
+      return false;
+    }
+    if (!sentinel.getSnapshot().getName().equals(snapshot.getName())) {
+      // another handler is trying to restore to the table, but it isn't the same snapshot source.
+      return false;
+    }
+
+    LOG.debug("Verify snapshot=" + snapshot.getName() + " against="
+        + sentinel.getSnapshot().getName() + " table=" + snapshot.getTable());
+    ForeignException e = sentinel.getExceptionIfFailed();
+    if (e != null) throw e;
+
+    // check to see if we are done
+    if (sentinel.isFinished()) {
+      LOG.debug("Restore snapshot=" + snapshot + " has completed. Notifying the client.");
+      return false;
+    }
+
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Sentinel is not yet finished with restoring snapshot=" + snapshot);
+    }
+    return true;
+  }
+
+  /**
+   * Get the restore snapshot sentinel for the specified table
+   * @param tableName table under restore
+   * @return the restore snapshot handler
+   */
+  private synchronized SnapshotSentinel getRestoreSnapshotSentinel(final String tableName) {
+    try {
+      return restoreHandlers.get(tableName);
+    } finally {
+      cleanupRestoreSentinels();
+    }
+  }
+
+  /**
+   * Scan the restore handlers and remove the finished ones.
+   */
+  private void cleanupRestoreSentinels() {
+    Iterator<Map.Entry<String, SnapshotSentinel>> it = restoreHandlers.entrySet().iterator();
+    while (it.hasNext()) {
+        Map.Entry<String, SnapshotSentinel> entry = it.next();
+        SnapshotSentinel sentinel = entry.getValue();
+        if (sentinel.isFinished()) {
+          it.remove();
+        }
+    }
+  }
+
+  //
+  // Implementing Stoppable interface
+  //
+
+  @Override
+  public void stop(String why) {
+    // short circuit
+    if (this.stopped) return;
+    // make sure we get stop
+    this.stopped = true;
+    // pass the stop onto take snapshot handlers
+    if (this.handler != null) this.handler.cancel(why);
+
+    // pass the stop onto all the restore handlers
+    for (SnapshotSentinel restoreHandler: this.restoreHandlers.values()) {
+      restoreHandler.cancel(why);
+    }
+  }
+
+  @Override
+  public boolean isStopped() {
+    return this.stopped;
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
index 23ebaa8..ac1d75f 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
@@ -41,9 +41,9 @@ import org.apache.hadoop.hbase.executor.EventHandler;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.SnapshotSentinel;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.server.snapshot.task.TableInfoCopyTask;
+import org.apache.hadoop.hbase.snapshot.SnapshotCreationException;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
+import org.apache.hadoop.hbase.snapshot.TableInfoCopyTask;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.zookeeper.KeeperException;
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java
deleted file mode 100644
index 3f1c51c..0000000
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java
+++ /dev/null
@@ -1,725 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.master.snapshot.manage;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.Stoppable;
-import org.apache.hadoop.hbase.catalog.MetaReader;
-import org.apache.hadoop.hbase.errorhandling.ForeignException;
-import org.apache.hadoop.hbase.executor.ExecutorService;
-import org.apache.hadoop.hbase.master.AssignmentManager;
-import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
-import org.apache.hadoop.hbase.master.MasterFileSystem;
-import org.apache.hadoop.hbase.master.MasterServices;
-import org.apache.hadoop.hbase.master.SnapshotSentinel;
-import org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler;
-import org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler;
-import org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler;
-import org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription.Type;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
-import org.apache.hadoop.hbase.snapshot.exception.RestoreSnapshotException;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotDoesNotExistException;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotExistsException;
-import org.apache.hadoop.hbase.snapshot.exception.TablePartiallyOpenException;
-import org.apache.hadoop.hbase.snapshot.exception.UnknownSnapshotException;
-import org.apache.hadoop.hbase.snapshot.restore.RestoreSnapshotHelper;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.FSTableDescriptors;
-
-import com.google.protobuf.ServiceException;
-
-/**
- * This class manages the procedure of taking and restoring snapshots. There is only one
- * SnapshotManager for the master.
- * <p>
- * The class provides methods for monitoring in-progress snapshot actions.
- * <p>
- * Note: Currently there can only one snapshot being taken at a time over the cluster.  This is a
- * simplification in the current implementation.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Unstable
-public class SnapshotManager implements Stoppable {
-  private static final Log LOG = LogFactory.getLog(SnapshotManager.class);
-
-  /** By default, check to see if the snapshot is complete every WAKE MILLIS (ms) */
-  public static final int SNAPSHOT_WAKE_MILLIS_DEFAULT = 500;
-
-  /**
-   * Conf key for # of ms elapsed between checks for snapshot errors while waiting for
-   * completion.
-   */
-  public static final String SNAPSHOT_WAKE_MILLIS_KEY = "hbase.snapshot.master.wakeMillis";
-
-  /** By default, check to see if the snapshot is complete (ms) */
-  public static final int SNAPSHOT_TIMEOUT_MILLIS_DEFAULT = 5000;
-
-  /**
-   * Conf key for # of ms elapsed before injecting a snapshot timeout error when waiting for
-   * completion.
-   */
-  public static final String SNAPSHOT_TIMEMOUT_MILLIS_KEY = "hbase.snapshot.master.timeoutMillis";
-
-  /** Name of the operation to use in the controller */
-  public static final String ONLINE_SNAPSHOT_CONTROLLER_DESCRIPTION = "online-snapshot";
-
-  // TODO - enable having multiple snapshots with multiple monitors/threads
-  // this needs to be configuration based when running multiple snapshots is implemented
-  /** number of current operations running on the master */
-  private static final int opThreads = 1;
-
-  private boolean stopped;
-  private final long wakeFrequency;
-  private final MasterServices master;  // Needed by TableEventHandlers
-
-  // A reference to a handler.  If the handler is non-null, then it is assumed that a snapshot is
-  // in progress currently
-  // TODO: this is a bad smell;  likely replace with a collection in the future.  Also this gets
-  // reset by every operation.
-  private TakeSnapshotHandler handler;
-
-  private final Path rootDir;
-  private final ExecutorService executorService;
-
-  // Restore Sentinels map, with table name as key
-  private Map<String, SnapshotSentinel> restoreHandlers = new HashMap<String, SnapshotSentinel>();
-
-  /**
-   * Construct a snapshot manager.
-   * @param master
-   * @param comms
-   */
-  public SnapshotManager(final MasterServices master) throws IOException {
-    this.master = master;
-
-    // get the configuration for the coordinator
-    Configuration conf = master.getConfiguration();
-    this.wakeFrequency = conf.getInt(SNAPSHOT_WAKE_MILLIS_KEY, SNAPSHOT_WAKE_MILLIS_DEFAULT);
-    this.rootDir = master.getMasterFileSystem().getRootDir();
-    this.executorService = master.getExecutorService();
-    resetTempDir();
-  }
-
-  /**
-   * Gets the list of all completed snapshots.
-   * @return list of SnapshotDescriptions
-   * @throws IOException File system exception
-   */
-  public List<SnapshotDescription> getCompletedSnapshots() throws IOException {
-    List<SnapshotDescription> snapshotDescs = new ArrayList<SnapshotDescription>();
-    // first create the snapshot root path and check to see if it exists
-    Path snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);
-    FileSystem fs = master.getMasterFileSystem().getFileSystem();
-
-    // if there are no snapshots, return an empty list
-    if (!fs.exists(snapshotDir)) {
-      return snapshotDescs;
-    }
-
-    // ignore all the snapshots in progress
-    FileStatus[] snapshots = fs.listStatus(snapshotDir,
-      new SnapshotDescriptionUtils.CompletedSnaphotDirectoriesFilter(fs));
-    // loop through all the completed snapshots
-    for (FileStatus snapshot : snapshots) {
-      Path info = new Path(snapshot.getPath(), SnapshotDescriptionUtils.SNAPSHOTINFO_FILE);
-      // if the snapshot is bad
-      if (!fs.exists(info)) {
-        LOG.error("Snapshot information for " + snapshot.getPath() + " doesn't exist");
-        continue;
-      }
-      FSDataInputStream in = null;
-      try {
-        in = fs.open(info);
-        SnapshotDescription desc = SnapshotDescription.parseFrom(in);
-        snapshotDescs.add(desc);
-      } catch (IOException e) {
-        LOG.warn("Found a corrupted snapshot " + snapshot.getPath(), e);
-      } finally {
-        if (in != null) {
-          in.close();
-        }
-      }
-    }
-    return snapshotDescs;
-  }
-
-  /**
-   * Cleans up any snapshots in the snapshot/.tmp directory that were left from failed
-   * snapshot attempts.
-   *
-   * @throws IOException if we can't reach the filesystem
-   */
-  void resetTempDir() throws IOException {
-    // cleanup any existing snapshots.
-    Path tmpdir = SnapshotDescriptionUtils.getWorkingSnapshotDir(rootDir);
-    if (master.getMasterFileSystem().getFileSystem().delete(tmpdir, true)) {
-      LOG.warn("Couldn't delete working snapshot directory: " + tmpdir);
-    }
-  }
-
-  /**
-   * Delete the specified snapshot
-   * @param snapshot
-   * @throws SnapshotDoesNotExistException If the specified snapshot does not exist.
-   * @throws IOException For filesystem IOExceptions
-   */
-  public void deleteSnapshot(SnapshotDescription snapshot) throws SnapshotDoesNotExistException, IOException {
-
-    // call coproc pre hook
-    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
-    if (cpHost != null) {
-      cpHost.preDeleteSnapshot(snapshot);
-    }
-
-    // check to see if it is completed
-    if (!isSnapshotCompleted(snapshot)) {
-      throw new SnapshotDoesNotExistException(snapshot);
-    }
-
-    String snapshotName = snapshot.getName();
-    LOG.debug("Deleting snapshot: " + snapshotName);
-    // first create the snapshot description and check to see if it exists
-    MasterFileSystem fs = master.getMasterFileSystem();
-    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
-
-    // delete the existing snapshot
-    if (!fs.getFileSystem().delete(snapshotDir, true)) {
-      throw new HBaseSnapshotException("Failed to delete snapshot directory: " + snapshotDir);
-    }
-
-    // call coproc post hook
-    if (cpHost != null) {
-      cpHost.postDeleteSnapshot(snapshot);
-    }
-
-  }
-
-  /**
-   * Return the handler if it is currently running and has the same snapshot target name.
-   * @param snapshot
-   * @return null if doesn't match, else a live handler.
-   */
-  TakeSnapshotHandler getTakeSnapshotHandler(SnapshotDescription snapshot) {
-    TakeSnapshotHandler h = this.handler;
-    if (h == null) {
-      return null;
-    }
-
-    if (!h.getSnapshot().getName().equals(snapshot.getName())) {
-      // specified snapshot is to the one currently running
-      return null;
-    }
-
-    return h;
-  }
-
-  /**
-   * Check if the specified snapshot is done
-   * @param expected
-   * @return true if snapshot is ready to be restored, false if it is still being taken.
-   * @throws IOException IOException if error from HDFS or RPC
-   * @throws UnknownSnapshotException if snapshot is invalid or does not exist.
-   */
-  public boolean isSnapshotDone(SnapshotDescription expected) throws IOException {
-    // check the request to make sure it has a snapshot
-    if (expected == null) {
-      throw new UnknownSnapshotException(
-         "No snapshot name passed in request, can't figure out which snapshot you want to check.");
-    }
-
-    // check to see if the sentinel exists
-    TakeSnapshotHandler handler = getTakeSnapshotHandler(expected);
-    if (handler == null) {
-      // doesn't exist, check if it is already completely done.
-      if (!isSnapshotCompleted(expected)) {
-        throw new UnknownSnapshotException("Snapshot:" + expected.getName()
-            + " is not currently running or one of the known completed snapshots.");
-      }
-      // was done, return true;
-      return true;
-    }
-
-    // pass on any failure we find in the sentinel
-    try {
-      handler.rethrowException();
-    } catch (ForeignException e) {
-      throw new HBaseSnapshotException("Snapshot error from RS", e, expected);
-    }
-
-    // check to see if we are done
-    if (handler.isFinished()) {
-      LOG.debug("Snapshot '" + expected.getName() + "' has completed, notifying client.");
-      return true;
-    } else if (LOG.isDebugEnabled()) {
-      LOG.debug("Sentinel isn't finished with snapshot '" + expected.getName() + "'!");
-    }
-    return false;
-  }
-
-  /**
-   * Check to see if there are any snapshots in progress currently.  Currently we have a
-   * limitation only allowing a single snapshot attempt at a time.
-   * @return <tt>true</tt> if there any snapshots in progress, <tt>false</tt> otherwise
-   * @throws SnapshotCreationException if the snapshot failed
-   */
-  synchronized boolean isTakingSnapshot() throws SnapshotCreationException {
-    // TODO later when we handle multiple there would be a map with ssname to handler.
-    return handler != null && !handler.isFinished();
-  }
-
-  /**
-   * Check to see if the specified table has a snapshot in progress.  Currently we have a
-   * limitation only allowing a single snapshot attempt at a time.
-   * @param tableName name of the table being snapshotted.
-   * @return <tt>true</tt> if there is a snapshot in progress on the specified table.
-   */
-  private boolean isTakingSnapshot(final String tableName) {
-    if (handler != null && handler.getSnapshot().getTable().equals(tableName)) {
-      return !handler.isFinished();
-    }
-    return false;
-  }
-
-  /**
-   * Check to make sure that we are OK to run the passed snapshot. Checks to make sure that we
-   * aren't already running a snapshot.
-   * @param snapshot description of the snapshot we want to start
-   * @throws HBaseSnapshotException if the filesystem could not be prepared to start the snapshot
-   */
-  private synchronized void prepareToTakeSnapshot(SnapshotDescription snapshot)
-      throws HBaseSnapshotException {
-    FileSystem fs = master.getMasterFileSystem().getFileSystem();
-    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
-
-    // make sure we aren't already running a snapshot
-    if (isTakingSnapshot()) {
-      throw new SnapshotCreationException("Already running another snapshot:"
-          + this.handler.getSnapshot(), snapshot);
-    }
-
-    // make sure we aren't running a restore on the same table
-    if (isRestoringTable(snapshot.getTable())) {
-      throw new SnapshotCreationException("Restore in progress on the same table snapshot:"
-          + this.handler.getSnapshot(), snapshot);
-    }
-
-    try {
-      // delete the working directory, since we aren't running the snapshot.  Likely leftovers
-      // from a failed attempt.
-      fs.delete(workingDir, true);
-
-      // recreate the working directory for the snapshot
-      if (!fs.mkdirs(workingDir)) {
-        throw new SnapshotCreationException("Couldn't create working directory (" + workingDir
-            + ") for snapshot.", snapshot);
-      }
-    } catch (HBaseSnapshotException e) {
-      throw e;
-    } catch (IOException e) {
-      throw new SnapshotCreationException(
-          "Exception while checking to see if snapshot could be started.", e, snapshot);
-    }
-  }
-
-  /**
-   * Take a snapshot based on the enabled/disabled state of the table.
-   *
-   * @param snapshot
-   * @throws HBaseSnapshotException when a snapshot specific exception occurs.
-   * @throws IOException when some sort of generic IO exception occurs.
-   */
-  public void takeSnapshot(SnapshotDescription snapshot) throws HBaseSnapshotException, IOException {
-    // check to see if we already completed the snapshot
-    if (isSnapshotCompleted(snapshot)) {
-      throw new SnapshotExistsException("Snapshot '" + snapshot.getName()
-          + "' already stored on the filesystem.", snapshot);
-    }
-
-    LOG.debug("No existing snapshot, attempting snapshot...");
-
-    // check to see if the table exists
-    HTableDescriptor desc = null;
-    try {
-      desc = master.getTableDescriptors().get(snapshot.getTable());
-    } catch (FileNotFoundException e) {
-      String msg = "Table:" + snapshot.getTable() + " info doesn't exist!";
-      LOG.error(msg);
-      throw new SnapshotCreationException(msg, e, snapshot);
-    } catch (IOException e) {
-      throw new SnapshotCreationException("Error while geting table description for table "
-          + snapshot.getTable(), e, snapshot);
-    }
-    if (desc == null) {
-      throw new SnapshotCreationException("Table '" + snapshot.getTable()
-          + "' doesn't exist, can't take snapshot.", snapshot);
-    }
-
-    // set the snapshot version, now that we are ready to take it
-    snapshot = snapshot.toBuilder().setVersion(SnapshotDescriptionUtils.SNAPSHOT_LAYOUT_VERSION)
-        .build();
-
-    // call pre coproc hook
-    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
-    if (cpHost != null) {
-      cpHost.preSnapshot(snapshot, desc);
-    }
-
-    // setup the snapshot
-    prepareToTakeSnapshot(snapshot);
-
-    // if the table is enabled, then have the RS run actually the snapshot work
-    AssignmentManager assignmentMgr = master.getAssignmentManager();
-    if (assignmentMgr.getZKTable().isEnabledTable(snapshot.getTable())) {
-      LOG.debug("Table enabled, starting distributed snapshot.");
-      throw new UnsupportedOperationException("Snapshots of enabled tables is not yet supported");
-    }
-    // For disabled table, snapshot is created by the master
-    else if (assignmentMgr.getZKTable().isDisabledTable(snapshot.getTable())) {
-      LOG.debug("Table is disabled, running snapshot entirely on master.");
-      snapshotDisabledTable(snapshot);
-      LOG.debug("Started snapshot: " + snapshot);
-    } else {
-      LOG.error("Can't snapshot table '" + snapshot.getTable()
-          + "', isn't open or closed, we don't know what to do!");
-      TablePartiallyOpenException tpoe = new TablePartiallyOpenException(snapshot.getTable()
-          + " isn't fully open.");
-      throw new SnapshotCreationException("Table is not entirely open or closed", tpoe, snapshot);
-    }
-
-    // call post coproc hook
-    if (cpHost != null) {
-      cpHost.postSnapshot(snapshot, desc);
-    }
-  }
-
-  /**
-   * Take a snapshot of a disabled table.
-   * <p>
-   * Ensures the snapshot won't be started if there is another snapshot already running. Does
-   * <b>not</b> check to see if another snapshot of the same name already exists.
-   * @param snapshot description of the snapshot to take. Modified to be {@link Type#DISABLED}.
-   * @throws HBaseSnapshotException if the snapshot could not be started
-   */
-  private synchronized void snapshotDisabledTable(SnapshotDescription snapshot)
-      throws HBaseSnapshotException {
-
-    // set the snapshot to be a disabled snapshot, since the client doesn't know about that
-    snapshot = snapshot.toBuilder().setType(Type.DISABLED).build();
-
-    DisabledTableSnapshotHandler handler;
-    try {
-      handler = new DisabledTableSnapshotHandler(snapshot, this.master);
-      this.handler = handler;
-      this.executorService.submit(handler);
-    } catch (IOException e) {
-      // cleanup the working directory by trying to delete it from the fs.
-      Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
-      try {
-        if (this.master.getMasterFileSystem().getFileSystem().delete(workingDir, true)) {
-          LOG.error("Couldn't delete working directory (" + workingDir + " for snapshot:"
-              + snapshot);
-        }
-      } catch (IOException e1) {
-        LOG.error("Couldn't delete working directory (" + workingDir + " for snapshot:" + snapshot);
-      }
-      // fail the snapshot
-      throw new SnapshotCreationException("Could not build snapshot handler", e, snapshot);
-    }
-  }
-
-  /**
-   * Set the handler for the current snapshot
-   * <p>
-   * Exposed for TESTING
-   * @param handler handler the master should use
-   *
-   * TODO get rid of this if possible, repackaging, modify tests.
-   */
-  public synchronized void setSnapshotHandlerForTesting(TakeSnapshotHandler handler) {
-    this.handler = handler;
-  }
-
-  /**
-   * Check to see if the snapshot is one of the currently completed snapshots
-   * @param expected snapshot to check
-   * @return <tt>true</tt> if the snapshot is stored on the {@link FileSystem}, <tt>false</tt> if is
-   *         not stored
-   * @throws IOException if the filesystem throws an unexpected exception,
-   * @throws IllegalArgumentException if snapshot name is invalid.
-   */
-  private boolean isSnapshotCompleted(SnapshotDescription snapshot) throws IOException {
-    try {
-      final Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshot, rootDir);
-      FileSystem fs = master.getMasterFileSystem().getFileSystem();
-
-      // check to see if the snapshot already exists
-      return fs.exists(snapshotDir);
-    } catch (IllegalArgumentException iae) {
-      throw new UnknownSnapshotException("Unexpected exception thrown", iae);
-    }
-  }
-
-  /**
-   * Restore the specified snapshot.
-   * The restore will fail if the destination table has a snapshot or restore in progress.
-   *
-   * @param snapshot Snapshot Descriptor
-   * @param hTableDescriptor Table Descriptor of the table to create
-   * @param waitTime timeout before considering the clone failed
-   */
-  synchronized void cloneSnapshot(final SnapshotDescription snapshot,
-      final HTableDescriptor hTableDescriptor) throws HBaseSnapshotException {
-    String tableName = hTableDescriptor.getNameAsString();
-
-    // make sure we aren't running a snapshot on the same table
-    if (isTakingSnapshot(tableName)) {
-      throw new RestoreSnapshotException("Snapshot in progress on the restore table=" + tableName);
-    }
-
-    // make sure we aren't running a restore on the same table
-    if (isRestoringTable(tableName)) {
-      throw new RestoreSnapshotException("Restore already in progress on the table=" + tableName);
-    }
-
-    try {
-      CloneSnapshotHandler handler =
-        new CloneSnapshotHandler(master, snapshot, hTableDescriptor);
-      this.executorService.submit(handler);
-      restoreHandlers.put(tableName, handler);
-    } catch (Exception e) {
-      String msg = "Couldn't clone the snapshot=" + snapshot + " on table=" + tableName;
-      LOG.error(msg, e);
-      throw new RestoreSnapshotException(msg, e);
-    }
-  }
-
-  /**
-   * Restore the specified snapshot
-   * @param reqSnapshot
-   * @throws IOException
-   */
-  public void restoreSnapshot(SnapshotDescription reqSnapshot) throws IOException {
-    FileSystem fs = master.getMasterFileSystem().getFileSystem();
-    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(reqSnapshot, rootDir);
-    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
-
-    // check if the snapshot exists
-    if (!fs.exists(snapshotDir)) {
-      LOG.error("A Snapshot named '" + reqSnapshot.getName() + "' does not exist.");
-      throw new SnapshotDoesNotExistException(reqSnapshot);
-    }
-
-    // read snapshot information
-    SnapshotDescription fsSnapshot = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
-    HTableDescriptor snapshotTableDesc = FSTableDescriptors.getTableDescriptor(fs, snapshotDir);
-    String tableName = reqSnapshot.getTable();
-
-    // stop tracking completed restores
-    cleanupRestoreSentinels();
-
-    // Execute the restore/clone operation
-    if (MetaReader.tableExists(master.getCatalogTracker(), tableName)) {
-      if (master.getAssignmentManager().getZKTable().isEnabledTable(fsSnapshot.getTable())) {
-        throw new UnsupportedOperationException("Table '" +
-          fsSnapshot.getTable() + "' must be disabled in order to perform a restore operation.");
-      }
-
-      // call coproc pre hook
-      if (cpHost != null) {
-        cpHost.preRestoreSnapshot(reqSnapshot, snapshotTableDesc);
-      }
-      restoreSnapshot(fsSnapshot, snapshotTableDesc);
-      LOG.info("Restore snapshot=" + fsSnapshot.getName() + " as table=" + tableName);
-
-      if (cpHost != null) {
-        cpHost.postRestoreSnapshot(reqSnapshot, snapshotTableDesc);
-      }
-    } else {
-      HTableDescriptor htd = RestoreSnapshotHelper.cloneTableSchema(snapshotTableDesc,
-                                                         Bytes.toBytes(tableName));
-      if (cpHost != null) {
-        cpHost.preCloneSnapshot(reqSnapshot, htd);
-      }
-      cloneSnapshot(fsSnapshot, htd);
-      LOG.info("Clone snapshot=" + fsSnapshot.getName() + " as table=" + tableName);
-
-      if (cpHost != null) {
-        cpHost.postCloneSnapshot(reqSnapshot, htd);
-      }
-    }
-  }
-
-  /**
-   * Restore the specified snapshot.
-   * The restore will fail if the destination table has a snapshot or restore in progress.
-   *
-   * @param snapshot Snapshot Descriptor
-   * @param hTableDescriptor Table Descriptor
-   * @param waitTime timeout before considering the restore failed
-   */
-  private synchronized void restoreSnapshot(final SnapshotDescription snapshot,
-      final HTableDescriptor hTableDescriptor) throws HBaseSnapshotException {
-    String tableName = hTableDescriptor.getNameAsString();
-
-    // make sure we aren't running a snapshot on the same table
-    if (isTakingSnapshot(tableName)) {
-      throw new RestoreSnapshotException("Snapshot in progress on the restore table=" + tableName);
-    }
-
-    // make sure we aren't running a restore on the same table
-    if (isRestoringTable(tableName)) {
-      throw new RestoreSnapshotException("Restore already in progress on the table=" + tableName);
-    }
-
-    try {
-      RestoreSnapshotHandler handler =
-        new RestoreSnapshotHandler(master, snapshot, hTableDescriptor);
-      this.executorService.submit(handler);
-      restoreHandlers.put(hTableDescriptor.getNameAsString(), handler);
-    } catch (Exception e) {
-      String msg = "Couldn't restore the snapshot=" + snapshot + " on table=" + tableName;
-      LOG.error(msg, e);
-      throw new RestoreSnapshotException(msg, e);
-    }
-  }
-
-  /**
-   * Verify if the the restore of the specified table is in progress.
-   *
-   * @param tableName table under restore
-   * @return <tt>true</tt> if there is a restore in progress of the specified table.
-   */
-  private boolean isRestoringTable(final String tableName) {
-    SnapshotSentinel sentinel = restoreHandlers.get(tableName);
-    return(sentinel != null && !sentinel.isFinished());
-  }
-
-  /**
-   * Returns status of a restore request, specifically comparing source snapshot and target table
-   * names.  Throws exception if not a known snapshot.
-   * @param snapshot
-   * @return true if in progress, false if is not.
-   * @throws UnknownSnapshotException if specified source snapshot does not exit.
-   * @throws IOException if there was some sort of IO failure
-   */
-  public boolean isRestoringTable(final SnapshotDescription snapshot) throws IOException {
-    // check to see if the snapshot is already on the fs
-    if (!isSnapshotCompleted(snapshot)) {
-      throw new UnknownSnapshotException("Snapshot:" + snapshot.getName()
-          + " is not one of the known completed snapshots.");
-    }
-
-    SnapshotSentinel sentinel = getRestoreSnapshotSentinel(snapshot.getTable());
-    if (sentinel == null) {
-      // there is no sentinel so restore is not in progress.
-      return false;
-    }
-    if (!sentinel.getSnapshot().getName().equals(snapshot.getName())) {
-      // another handler is trying to restore to the table, but it isn't the same snapshot source.
-      return false;
-    }
-
-    LOG.debug("Verify snapshot=" + snapshot.getName() + " against="
-        + sentinel.getSnapshot().getName() + " table=" + snapshot.getTable());
-    ForeignException e = sentinel.getExceptionIfFailed();
-    if (e != null) throw e;
-
-    // check to see if we are done
-    if (sentinel.isFinished()) {
-      LOG.debug("Restore snapshot=" + snapshot + " has completed. Notifying the client.");
-      return false;
-    }
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Sentinel is not yet finished with restoring snapshot=" + snapshot);
-    }
-    return true;
-  }
-
-  /**
-   * Get the restore snapshot sentinel for the specified table
-   * @param tableName table under restore
-   * @return the restore snapshot handler
-   */
-  private synchronized SnapshotSentinel getRestoreSnapshotSentinel(final String tableName) {
-    try {
-      return restoreHandlers.get(tableName);
-    } finally {
-      cleanupRestoreSentinels();
-    }
-  }
-
-  /**
-   * Scan the restore handlers and remove the finished ones.
-   */
-  private void cleanupRestoreSentinels() {
-    Iterator<Map.Entry<String, SnapshotSentinel>> it = restoreHandlers.entrySet().iterator();
-    while (it.hasNext()) {
-        Map.Entry<String, SnapshotSentinel> entry = it.next();
-        SnapshotSentinel sentinel = entry.getValue();
-        if (sentinel.isFinished()) {
-          it.remove();
-        }
-    }
-  }
-
-  //
-  // Implementing Stoppable interface
-  //
-
-  @Override
-  public void stop(String why) {
-    // short circuit
-    if (this.stopped) return;
-    // make sure we get stop
-    this.stopped = true;
-    // pass the stop onto take snapshot handlers
-    if (this.handler != null) this.handler.cancel(why);
-
-    // pass the stop onto all the restore handlers
-    for (SnapshotSentinel restoreHandler: this.restoreHandlers.values()) {
-      restoreHandler.cancel(why);
-    }
-  }
-
-  @Override
-  public boolean isStopped() {
-    return this.stopped;
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java
deleted file mode 100644
index 806cc41..0000000
--- a/src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java
+++ /dev/null
@@ -1,325 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map.Entry;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener;
-import org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.Store;
-import org.apache.hadoop.hbase.regionserver.wal.HLog;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.exception.CorruptedSnapshotException;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.FSUtils;
-
-import com.google.common.collect.HashMultimap;
-import com.google.common.collect.Multimap;
-
-/**
- * Utilities for useful when taking a snapshot
- */
-public class TakeSnapshotUtils {
-
-  private static final Log LOG = LogFactory.getLog(TakeSnapshotUtils.class);
-
-  private TakeSnapshotUtils() {
-    // private constructor for util class
-  }
-
-  /**
-   * Get the per-region snapshot description location.
-   * <p>
-   * Under the per-snapshot directory, specific files per-region are kept in a similar layout as per
-   * the current directory layout.
-   * @param desc description of the snapshot
-   * @param rootDir root directory for the hbase installation
-   * @param regionName encoded name of the region (see {@link HRegionInfo#encodeRegionName(byte[])})
-   * @return path to the per-region directory for the snapshot
-   */
-  public static Path getRegionSnapshotDirectory(SnapshotDescription desc, Path rootDir,
-      String regionName) {
-    Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(desc, rootDir);
-    return HRegion.getRegionDir(snapshotDir, regionName);
-  }
-
-  /**
-   * Get the home directory for store-level snapshot files.
-   * <p>
-   * Specific files per store are kept in a similar layout as per the current directory layout.
-   * @param regionDir snapshot directory for the parent region, <b>not</b> the standard region
-   *          directory. See {@link #getRegionSnapshotDirectory(SnapshotDescription, Path, String)}
-   * @param family name of the store to snapshot
-   * @return path to the snapshot home directory for the store/family
-   */
-  public static Path getStoreSnapshotDirectory(Path regionDir, String family) {
-    return Store.getStoreHomedir(regionDir, Bytes.toBytes(family));
-  }
-
-  /**
-   * Get the snapshot directory for each family to be added to the the snapshot
-   * @param snapshot description of the snapshot being take
-   * @param snapshotRegionDir directory in the snapshot where the region directory information
-   *          should be stored
-   * @param families families to be added (can be null)
-   * @return paths to the snapshot directory for each family, in the same order as the families
-   *         passed in
-   */
-  public static List<Path> getFamilySnapshotDirectories(SnapshotDescription snapshot,
-      Path snapshotRegionDir, FileStatus[] families) {
-    if (families == null || families.length == 0) return Collections.emptyList();
-
-    List<Path> familyDirs = new ArrayList<Path>(families.length);
-    for (FileStatus family : families) {
-      // build the reference directory name
-      familyDirs.add(getStoreSnapshotDirectory(snapshotRegionDir, family.getPath().getName()));
-    }
-    return familyDirs;
-  }
-
-  /**
-   * Create a snapshot timer for the master which notifies the monitor when an error occurs
-   * @param snapshot snapshot to monitor
-   * @param conf configuration to use when getting the max snapshot life
-   * @param monitor monitor to notify when the snapshot life expires
-   * @return the timer to use update to signal the start and end of the snapshot
-   */
-  @SuppressWarnings("rawtypes")
-  public static TimeoutExceptionInjector getMasterTimerAndBindToMonitor(SnapshotDescription snapshot,
-      Configuration conf, ForeignExceptionListener monitor) {
-    long maxTime = SnapshotDescriptionUtils.getMaxMasterTimeout(conf, snapshot.getType(),
-      SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME);
-    return new TimeoutExceptionInjector(monitor, maxTime);
-  }
-
-  /**
-   * Verify that all the expected logs got referenced
-   * @param fs filesystem where the logs live
-   * @param logsDir original logs directory
-   * @param serverNames names of the servers that involved in the snapshot
-   * @param snapshot description of the snapshot being taken
-   * @param snapshotLogDir directory for logs in the snapshot
-   * @throws IOException
-   */
-  public static void verifyAllLogsGotReferenced(FileSystem fs, Path logsDir,
-      Set<String> serverNames, SnapshotDescription snapshot, Path snapshotLogDir)
-      throws IOException {
-    assertTrue(snapshot, "Logs directory doesn't exist in snapshot", fs.exists(logsDir));
-    // for each of the server log dirs, make sure it matches the main directory
-    Multimap<String, String> snapshotLogs = getMapOfServersAndLogs(fs, snapshotLogDir, serverNames);
-    Multimap<String, String> realLogs = getMapOfServersAndLogs(fs, logsDir, serverNames);
-    if (realLogs != null) {
-      assertNotNull(snapshot, "No server logs added to snapshot", snapshotLogs);
-    } else if (realLogs == null) {
-      assertNull(snapshot, "Snapshotted server logs that don't exist", snapshotLogs);
-    }
-
-    // check the number of servers
-    Set<Entry<String, Collection<String>>> serverEntries = realLogs.asMap().entrySet();
-    Set<Entry<String, Collection<String>>> snapshotEntries = snapshotLogs.asMap().entrySet();
-    assertEquals(snapshot, "Not the same number of snapshot and original server logs directories",
-      serverEntries.size(), snapshotEntries.size());
-
-    // verify we snapshotted each of the log files
-    for (Entry<String, Collection<String>> serverLogs : serverEntries) {
-      // if the server is not the snapshot, skip checking its logs
-      if (!serverNames.contains(serverLogs.getKey())) continue;
-      Collection<String> snapshotServerLogs = snapshotLogs.get(serverLogs.getKey());
-      assertNotNull(snapshot, "Snapshots missing logs for server:" + serverLogs.getKey(),
-        snapshotServerLogs);
-
-      // check each of the log files
-      assertEquals(snapshot,
-        "Didn't reference all the log files for server:" + serverLogs.getKey(), serverLogs
-            .getValue().size(), snapshotServerLogs.size());
-      for (String log : serverLogs.getValue()) {
-        assertTrue(snapshot, "Snapshot logs didn't include " + log,
-          snapshotServerLogs.contains(log));
-      }
-    }
-  }
-
-  /**
-   * Verify one of a snapshot's region's recovered.edits, has been at the surface (file names,
-   * length), match the original directory.
-   * @param fs filesystem on which the snapshot had been taken
-   * @param rootDir full path to the root hbase directory
-   * @param regionInfo info for the region
-   * @param snapshot description of the snapshot that was taken
-   * @throws IOException if there is an unexpected error talking to the filesystem
-   */
-  public static void verifyRecoveredEdits(FileSystem fs, Path rootDir, HRegionInfo regionInfo,
-      SnapshotDescription snapshot) throws IOException {
-    Path regionDir = HRegion.getRegionDir(rootDir, regionInfo);
-    Path editsDir = HLog.getRegionDirRecoveredEditsDir(regionDir);
-    Path snapshotRegionDir = TakeSnapshotUtils.getRegionSnapshotDirectory(snapshot, rootDir,
-      regionInfo.getEncodedName());
-    Path snapshotEditsDir = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
-
-    FileStatus[] edits = FSUtils.listStatus(fs, editsDir);
-    FileStatus[] snapshotEdits = FSUtils.listStatus(fs, snapshotEditsDir);
-    if (edits == null) {
-      assertNull(snapshot, "Snapshot has edits but table doesn't", snapshotEdits);
-      return;
-    }
-
-    assertNotNull(snapshot, "Table has edits, but snapshot doesn't", snapshotEdits);
-
-    // check each of the files
-    assertEquals(snapshot, "Not same number of edits in snapshot as table", edits.length,
-      snapshotEdits.length);
-
-    // make sure we have a file with the same name as the original
-    // it would be really expensive to verify the content matches the original
-    for (FileStatus edit : edits) {
-      for (FileStatus sEdit : snapshotEdits) {
-        if (sEdit.getPath().equals(edit.getPath())) {
-          assertEquals(snapshot, "Snapshot file" + sEdit.getPath()
-              + " length not equal to the original: " + edit.getPath(), edit.getLen(),
-            sEdit.getLen());
-          break;
-        }
-      }
-      assertTrue(snapshot, "No edit in snapshot with name:" + edit.getPath(), false);
-    }
-  }
-
-  private static void assertNull(SnapshotDescription snapshot, String msg, Object isNull)
-      throws CorruptedSnapshotException {
-    if (isNull != null) {
-      throw new CorruptedSnapshotException(msg + ", Expected " + isNull + " to be null.", snapshot);
-    }
-  }
-
-  private static void assertNotNull(SnapshotDescription snapshot, String msg, Object notNull)
-      throws CorruptedSnapshotException {
-    if (notNull == null) {
-      throw new CorruptedSnapshotException(msg + ", Expected object to not be null, but was null.",
-          snapshot);
-    }
-  }
-
-  private static void assertTrue(SnapshotDescription snapshot, String msg, boolean isTrue)
-      throws CorruptedSnapshotException {
-    if (!isTrue) {
-      throw new CorruptedSnapshotException(msg + ", Expected true, but was false", snapshot);
-    }
-  }
-
-  /**
-   * Assert that the expect matches the gotten amount
-   * @param msg message to add the to exception
-   * @param expected
-   * @param gotten
-   * @throws CorruptedSnapshotException thrown if the two elements don't match
-   */
-  private static void assertEquals(SnapshotDescription snapshot, String msg, int expected,
-      int gotten) throws CorruptedSnapshotException {
-    if (expected != gotten) {
-      throw new CorruptedSnapshotException(msg + ". Expected:" + expected + ", got:" + gotten,
-          snapshot);
-    }
-  }
-
-  /**
-   * Assert that the expect matches the gotten amount
-   * @param msg message to add the to exception
-   * @param expected
-   * @param gotten
-   * @throws CorruptedSnapshotException thrown if the two elements don't match
-   */
-  private static void assertEquals(SnapshotDescription snapshot, String msg, long expected,
-      long gotten) throws CorruptedSnapshotException {
-    if (expected != gotten) {
-      throw new CorruptedSnapshotException(msg + ". Expected:" + expected + ", got:" + gotten,
-          snapshot);
-    }
-  }
-
-  /**
-   * @param logdir
-   * @param toInclude list of servers to include. If empty or null, returns all servers
-   * @return maps of servers to all their log files. If there is no log directory, returns
-   *         <tt>null</tt>
-   */
-  private static Multimap<String, String> getMapOfServersAndLogs(FileSystem fs, Path logdir,
-      Collection<String> toInclude) throws IOException {
-    // create a path filter based on the passed directories to include
-    PathFilter filter = toInclude == null || toInclude.size() == 0 ? null
-        : new MatchesDirectoryNames(toInclude);
-
-    // get all the expected directories
-    FileStatus[] serverLogDirs = FSUtils.listStatus(fs, logdir, filter);
-    if (serverLogDirs == null) return null;
-
-    // map those into a multimap of servername -> [log files]
-    Multimap<String, String> map = HashMultimap.create();
-    for (FileStatus server : serverLogDirs) {
-      FileStatus[] serverLogs = FSUtils.listStatus(fs, server.getPath(), null);
-      if (serverLogs == null) continue;
-      for (FileStatus log : serverLogs) {
-        map.put(server.getPath().getName(), log.getPath().getName());
-      }
-    }
-    return map;
-  }
-
-  /**
-   * Path filter that only accepts paths where that have a {@link Path#getName()} that is contained
-   * in the specified collection.
-   */
-  private static class MatchesDirectoryNames implements PathFilter {
-
-    Collection<String> paths;
-
-    public MatchesDirectoryNames(Collection<String> dirNames) {
-      this.paths = dirNames;
-    }
-
-    @Override
-    public boolean accept(Path path) {
-      return paths.contains(path.getName());
-    }
-  }
-
-  /**
-   * Get the log directory for a specific snapshot
-   * @param snapshotDir directory where the specific snapshot will be store
-   * @param serverName name of the parent regionserver for the log files
-   * @return path to the log home directory for the archive files.
-   */
-  public static Path getSnapshotHLogsDir(Path snapshotDir, String serverName) {
-    return new Path(snapshotDir, HLog.getHLogDirectoryName(serverName));
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java
deleted file mode 100644
index 95235c3..0000000
--- a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import java.io.IOException;
-import java.util.NavigableSet;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.regionserver.wal.HLog;
-
-/**
- * Copy over each of the files in a region's recovered.edits directory to the region's snapshot
- * directory.
- * <p>
- * This is a serial operation over each of the files in the recovered.edits directory and also
- * streams all the bytes to the client and then back to the filesystem, so the files being copied
- * should be <b>small</b> or it will (a) suck up a lot of bandwidth, and (b) take a long time.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-public class CopyRecoveredEditsTask extends SnapshotTask {
-
-  private static final Log LOG = LogFactory.getLog(CopyRecoveredEditsTask.class);
-  private final FileSystem fs;
-  private final Path regiondir;
-  private final Path outputDir;
-
-  /**
-   * @param snapshot Snapshot being taken
-   * @param monitor error monitor for the snapshot
-   * @param fs {@link FileSystem} where the snapshot is being taken
-   * @param regionDir directory for the region to examine for edits
-   * @param snapshotRegionDir directory for the region in the snapshot
-   */
-  public CopyRecoveredEditsTask(SnapshotDescription snapshot, ForeignExceptionDispatcher monitor,
-      FileSystem fs, Path regionDir, Path snapshotRegionDir) {
-    super(snapshot, monitor);
-    this.fs = fs;
-    this.regiondir = regionDir;
-    this.outputDir = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
-  }
-
-  @Override
-  public Void call() throws IOException {
-    NavigableSet<Path> files = HLog.getSplitEditFilesSorted(this.fs, regiondir);
-    if (files == null || files.size() == 0) return null;
-
-    // copy over each file.
-    // this is really inefficient (could be trivially parallelized), but is
-    // really simple to reason about.
-    for (Path source : files) {
-      // check to see if the file is zero length, in which case we can skip it
-      FileStatus stat = fs.getFileStatus(source);
-      if (stat.getLen() <= 0) continue;
-
-      // its not zero length, so copy over the file
-      Path out = new Path(outputDir, source.getName());
-      LOG.debug("Copying " + source + " to " + out);
-      FileUtil.copy(fs, source, fs, out, true, fs.getConf());
-
-      // check for errors to the running operation after each file
-      this.rethrowException();
-    }
-    return null;
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java
deleted file mode 100644
index d2e6637..0000000
--- a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
-import org.apache.hadoop.hbase.util.FSUtils;
-
-/**
- * Reference all the hfiles in a region for a snapshot.
- * <p>
- * Doesn't take into acccount if the hfiles are valid or not, just keeps track of what's in the
- * region's directory.
- */
-public class ReferenceRegionHFilesTask extends SnapshotTask {
-
-  public static final Log LOG = LogFactory.getLog(ReferenceRegionHFilesTask.class);
-  private final Path regiondir;
-  private final FileSystem fs;
-  private final PathFilter fileFilter;
-  private final Path snapshotDir;
-
-  /**
-   * Reference all the files in the given region directory
-   * @param snapshot snapshot for which to add references
-   * @param monitor to check/send error
-   * @param regionDir region directory to look for errors
-   * @param fs {@link FileSystem} where the snapshot/region live
-   * @param regionSnapshotDir directory in the snapshot to store region files
-   */
-  public ReferenceRegionHFilesTask(final SnapshotDescription snapshot,
-      ForeignExceptionDispatcher monitor, Path regionDir, final FileSystem fs, Path regionSnapshotDir) {
-    super(snapshot, monitor);
-    this.regiondir = regionDir;
-    this.fs = fs;
-
-    this.fileFilter = new PathFilter() {
-      @Override
-      public boolean accept(Path path) {
-        try {
-          return fs.isFile(path);
-        } catch (IOException e) {
-          LOG.error("Failed to reach fs to check file:" + path + ", marking as not file");
-          ReferenceRegionHFilesTask.this.snapshotFailure("Failed to reach fs to check file status",
-            e);
-          return false;
-        }
-      }
-    };
-    this.snapshotDir = regionSnapshotDir;
-  }
-
-  @Override
-  public Void call() throws IOException {
-    FileStatus[] families = FSUtils.listStatus(fs, regiondir, new FSUtils.FamilyDirFilter(fs));
-
-    // if no families, then we are done again
-    if (families == null || families.length == 0) {
-      LOG.info("No families under region directory:" + regiondir
-          + ", not attempting to add references.");
-      return null;
-    }
-
-    // snapshot directories to store the hfile reference
-    List<Path> snapshotFamilyDirs = TakeSnapshotUtils.getFamilySnapshotDirectories(snapshot,
-      snapshotDir, families);
-
-    LOG.debug("Add hfile references to snapshot directories:" + snapshotFamilyDirs);
-    for (int i = 0; i < families.length; i++) {
-      FileStatus family = families[i];
-      Path familyDir = family.getPath();
-      // get all the hfiles in the family
-      FileStatus[] hfiles = FSUtils.listStatus(fs, familyDir, fileFilter);
-
-      // if no hfiles, then we are done with this family
-      if (hfiles == null || hfiles.length == 0) {
-        LOG.debug("Not hfiles found for family: " + familyDir + ", skipping.");
-        continue;
-      }
-
-      // make the snapshot's family directory
-      Path snapshotFamilyDir = snapshotFamilyDirs.get(i);
-      fs.mkdirs(snapshotFamilyDir);
-
-      // create a reference for each hfile
-      for (FileStatus hfile : hfiles) {
-        // references are 0-length files, relying on file name.
-        Path referenceFile = new Path(snapshotFamilyDir, hfile.getPath().getName());
-        LOG.debug("Creating reference for:" + hfile.getPath() + " at " + referenceFile);
-        if (!fs.createNewFile(referenceFile)) {
-          throw new IOException("Failed to create reference file:" + referenceFile);
-        }
-      }
-    }
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Finished referencing hfiles, current region state:");
-      FSUtils.logFileSystemState(fs, regiondir, LOG);
-      LOG.debug("and the snapshot directory:");
-      FSUtils.logFileSystemState(fs, snapshotDir, LOG);
-    }
-    return null;
-  }
-}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java
deleted file mode 100644
index d1c45e3..0000000
--- a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java
+++ /dev/null
@@ -1,107 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.errorhandling.ForeignException;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.util.FSUtils;
-
-/**
- * Reference all the WAL files under a server's WAL directory
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-public class ReferenceServerWALsTask extends SnapshotTask {
-  private static final Log LOG = LogFactory.getLog(ReferenceServerWALsTask.class);
-  private final FileSystem fs;
-  private final Configuration conf;
-  private final String serverName;
-  private Path logDir;
-
-  /**
-   * @param snapshot snapshot being run
-   * @param failureListener listener to check for errors while running the operation and to
-   *          propagate errors found while running the task
-   * @param logDir log directory for the server. Name of the directory is taken as the name of the
-   *          server
-   * @param conf {@link Configuration} to extract filesystem information
-   * @param fs filesystem where the log files are stored and should be referenced
-   */
-  public ReferenceServerWALsTask(SnapshotDescription snapshot,
-      ForeignExceptionDispatcher failureListener, final Path logDir, final Configuration conf,
-      final FileSystem fs) {
-    super(snapshot, failureListener);
-    this.fs = fs;
-    this.conf = conf;
-    this.serverName = logDir.getName();
-    this.logDir = logDir;
-  }
-
-  /**
-   * Create reference files (empty files with the same path and file name as original).
-   * @throws IOException exception from hdfs or network problems
-   * @throws ForeignException exception from an external procedure
-   */
-  @Override
-  public Void call() throws IOException, ForeignException {
-    // TODO switch to using a single file to reference all required WAL files
-
-    // Iterate through each of the log files and add a reference to it.
-    // assumes that all the files under the server's logs directory is a log
-    FileStatus[] serverLogs = FSUtils.listStatus(fs, logDir, null);
-    if (serverLogs == null) LOG.info("No logs for server directory:" + logDir
-        + ", done referencing files.");
-
-    if (LOG.isDebugEnabled()) LOG.debug("Adding references for WAL files:"
-        + Arrays.toString(serverLogs));
-
-    for (FileStatus file : serverLogs) {
-      this.rethrowException();
-
-      // add the reference to the file. ex: hbase/.snapshots/.logs/<serverName>/<hlog>
-      Path rootDir = FSUtils.getRootDir(conf);
-      Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(this.snapshot, rootDir);
-      Path snapshotLogDir = TakeSnapshotUtils.getSnapshotHLogsDir(snapshotDir, serverName);
-      // actually store the reference on disk (small file)
-      Path ref = new Path(snapshotLogDir, file.getPath().getName());
-      if (!fs.createNewFile(ref)) {
-        if (!fs.exists(ref)) {
-          throw new IOException("Couldn't create reference for:" + file.getPath());
-        }
-      }
-      LOG.debug("Completed WAL referencing for: " + file.getPath() + " to " + ref);
-    }
-
-    LOG.debug("Successfully completed WAL referencing for ALL files");
-    return null;
-  }
-}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java
deleted file mode 100644
index 34e27d4..0000000
--- a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import java.util.concurrent.Callable;
-
-import org.apache.hadoop.hbase.errorhandling.ForeignException;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * General snapshot operation taken on a regionserver
- */
-public abstract class SnapshotTask implements ForeignExceptionSnare, Callable<Void>{
-
-  protected final SnapshotDescription snapshot;
-  protected final ForeignExceptionDispatcher errorMonitor;
-
-  /**
-   * @param snapshot Description of the snapshot we are going to operate on
-   * @param monitor listener interested in failures to the snapshot caused by this operation
-   */
-  public SnapshotTask(SnapshotDescription snapshot, ForeignExceptionDispatcher monitor) {
-    assert monitor != null : "ForeignExceptionDispatcher must not be null!";
-    assert snapshot != null : "SnapshotDescription must not be null!";
-    this.snapshot = snapshot;
-    this.errorMonitor = monitor;
-  }
-
-  public void snapshotFailure(String message, Exception e) {
-    ForeignException ee = new ForeignException(message, e);
-    errorMonitor.receive(ee);
-  }
-
-  @Override
-  public void rethrowException() throws ForeignException {
-    this.errorMonitor.rethrowException();
-  }
-
-  @Override
-  public boolean hasException() {
-    return this.errorMonitor.hasException();
-  }
-
-  @Override
-  public ForeignException getException() {
-    return this.errorMonitor.getException();
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java
deleted file mode 100644
index 744ebe4..0000000
--- a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.FSTableDescriptors;
-
-/**
- * Copy the table info into the snapshot directory
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-public class TableInfoCopyTask extends SnapshotTask {
-
-  public static final Log LOG = LogFactory.getLog(TableInfoCopyTask.class);
-  private final FileSystem fs;
-  private final Path rootDir;
-
-  /**
-   * Copy the table info for the given table into the snapshot
-   * @param monitor listen for errors while running the snapshot
-   * @param snapshot snapshot for which we are copying the table info
-   * @param fs {@link FileSystem} where the tableinfo is stored (and where the copy will be written)
-   * @param rootDir root of the {@link FileSystem} where the tableinfo is stored
-   */
-  public TableInfoCopyTask(ForeignExceptionDispatcher monitor,
-      SnapshotDescription snapshot, FileSystem fs, Path rootDir) {
-    super(snapshot, monitor);
-    this.rootDir = rootDir;
-    this.fs = fs;
-  }
-
-  @Override
-  public Void call() throws Exception {
-    LOG.debug("Running table info copy.");
-    this.rethrowException();
-    LOG.debug("Attempting to copy table info for snapshot:" + this.snapshot);
-    // get the HTable descriptor
-    HTableDescriptor orig = FSTableDescriptors.getTableDescriptor(fs, rootDir,
-      Bytes.toBytes(this.snapshot.getTable()));
-    this.rethrowException();
-    // write a copy of descriptor to the snapshot directory
-    Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
-    FSTableDescriptors.createTableDescriptorForTableDirectory(fs, snapshotDir, orig, false);
-    LOG.debug("Finished copying tableinfo.");
-    return null;
-  }
-}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/CopyRecoveredEditsTask.java b/src/main/java/org/apache/hadoop/hbase/snapshot/CopyRecoveredEditsTask.java
new file mode 100644
index 0000000..88a2e66
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/CopyRecoveredEditsTask.java
@@ -0,0 +1,90 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.IOException;
+import java.util.NavigableSet;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+
+/**
+ * Copy over each of the files in a region's recovered.edits directory to the region's snapshot
+ * directory.
+ * <p>
+ * This is a serial operation over each of the files in the recovered.edits directory and also
+ * streams all the bytes to the client and then back to the filesystem, so the files being copied
+ * should be <b>small</b> or it will (a) suck up a lot of bandwidth, and (b) take a long time.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class CopyRecoveredEditsTask extends SnapshotTask {
+
+  private static final Log LOG = LogFactory.getLog(CopyRecoveredEditsTask.class);
+  private final FileSystem fs;
+  private final Path regiondir;
+  private final Path outputDir;
+
+  /**
+   * @param snapshot Snapshot being taken
+   * @param monitor error monitor for the snapshot
+   * @param fs {@link FileSystem} where the snapshot is being taken
+   * @param regionDir directory for the region to examine for edits
+   * @param snapshotRegionDir directory for the region in the snapshot
+   */
+  public CopyRecoveredEditsTask(SnapshotDescription snapshot, ForeignExceptionDispatcher monitor,
+      FileSystem fs, Path regionDir, Path snapshotRegionDir) {
+    super(snapshot, monitor);
+    this.fs = fs;
+    this.regiondir = regionDir;
+    this.outputDir = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
+  }
+
+  @Override
+  public Void call() throws IOException {
+    NavigableSet<Path> files = HLog.getSplitEditFilesSorted(this.fs, regiondir);
+    if (files == null || files.size() == 0) return null;
+
+    // copy over each file.
+    // this is really inefficient (could be trivially parallelized), but is
+    // really simple to reason about.
+    for (Path source : files) {
+      // check to see if the file is zero length, in which case we can skip it
+      FileStatus stat = fs.getFileStatus(source);
+      if (stat.getLen() <= 0) continue;
+
+      // its not zero length, so copy over the file
+      Path out = new Path(outputDir, source.getName());
+      LOG.debug("Copying " + source + " to " + out);
+      FileUtil.copy(fs, source, fs, out, true, fs.getConf());
+
+      // check for errors to the running operation after each file
+      this.rethrowException();
+    }
+    return null;
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/CorruptedSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/CorruptedSnapshotException.java
new file mode 100644
index 0000000..2e16c1b
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/CorruptedSnapshotException.java
@@ -0,0 +1,56 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+
+/**
+ * Exception thrown when the found snapshot info from the filesystem is not valid
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class CorruptedSnapshotException extends HBaseSnapshotException {
+
+  /**
+   * @param message message describing the exception
+   * @param e cause
+   */
+  public CorruptedSnapshotException(String message, Exception e) {
+    super(message, e);
+  }
+
+  /**
+   * Snapshot was corrupt for some reason
+   * @param message full description of the failure
+   * @param snapshot snapshot that was expected
+   */
+  public CorruptedSnapshotException(String message, SnapshotDescription snapshot) {
+    super(message, snapshot);
+  }
+
+  /**
+   * @param message message describing the exception
+   */
+  public CorruptedSnapshotException(String message) {
+    super(message, (SnapshotDescription)null);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java b/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java
new file mode 100644
index 0000000..898c021
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java
@@ -0,0 +1,719 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileChecksum;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.io.HFileLink;
+import org.apache.hadoop.hbase.io.HLogLink;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.ExportSnapshotException;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+/**
+ * Export the specified snapshot to a given FileSystem.
+ *
+ * The .snapshot/name folder is copied to the destination cluster
+ * and then all the hfiles/hlogs are copied using a Map-Reduce Job in the .archive/ location.
+ * When everything is done, the second cluster can restore the snapshot.
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public final class ExportSnapshot extends Configured implements Tool {
+  private static final Log LOG = LogFactory.getLog(ExportSnapshot.class);
+
+  private static final String CONF_TMP_DIR = "hbase.tmp.dir";
+  private static final String CONF_FILES_USER = "snapshot.export.files.attributes.user";
+  private static final String CONF_FILES_GROUP = "snapshot.export.files.attributes.group";
+  private static final String CONF_FILES_MODE = "snapshot.export.files.attributes.mode";
+  private static final String CONF_CHECKSUM_VERIFY = "snapshot.export.checksum.verify";
+  private static final String CONF_OUTPUT_ROOT = "snapshot.export.output.root";
+  private static final String CONF_INPUT_ROOT = "snapshot.export.input.root";
+
+  private static final String INPUT_FOLDER_PREFIX = "export-files.";
+
+  // Export Map-Reduce Counters, to keep track of the progress
+  public enum Counter { MISSING_FILES, COPY_FAILED, BYTES_EXPECTED, BYTES_COPIED };
+
+  private static class ExportMapper extends Mapper<Text, NullWritable, NullWritable, NullWritable> {
+    final static int REPORT_SIZE = 1 * 1024 * 1024;
+    final static int BUFFER_SIZE = 64 * 1024;
+
+    private boolean verifyChecksum;
+    private String filesGroup;
+    private String filesUser;
+    private short filesMode;
+
+    private FileSystem outputFs;
+    private Path outputArchive;
+    private Path outputRoot;
+
+    private FileSystem inputFs;
+    private Path inputArchive;
+    private Path inputRoot;
+
+    @Override
+    public void setup(Context context) {
+      Configuration conf = context.getConfiguration();
+      verifyChecksum = conf.getBoolean(CONF_CHECKSUM_VERIFY, true);
+
+      filesGroup = conf.get(CONF_FILES_GROUP);
+      filesUser = conf.get(CONF_FILES_USER);
+      filesMode = (short)conf.getInt(CONF_FILES_MODE, 0);
+      outputRoot = new Path(conf.get(CONF_OUTPUT_ROOT));
+      inputRoot = new Path(conf.get(CONF_INPUT_ROOT));
+
+      inputArchive = new Path(inputRoot, HConstants.HFILE_ARCHIVE_DIRECTORY);
+      outputArchive = new Path(outputRoot, HConstants.HFILE_ARCHIVE_DIRECTORY);
+
+      try {
+        inputFs = FileSystem.get(inputRoot.toUri(), conf);
+      } catch (IOException e) {
+        throw new RuntimeException("Could not get the input FileSystem with root=" + inputRoot, e);
+      }
+
+      try {
+        outputFs = FileSystem.get(outputRoot.toUri(), conf);
+      } catch (IOException e) {
+        throw new RuntimeException("Could not get the output FileSystem with root="+ outputRoot, e);
+      }
+    }
+
+    @Override
+    public void cleanup(Context context) {
+      if (outputFs != null) {
+        try {
+          outputFs.close();
+        } catch (IOException e) {
+          LOG.error("Error closing output FileSystem", e);
+        }
+      }
+
+      if (inputFs != null) {
+        try {
+          inputFs.close();
+        } catch (IOException e) {
+          LOG.error("Error closing input FileSystem", e);
+        }
+      }
+    }
+
+    @Override
+    public void map(Text key, NullWritable value, Context context)
+        throws InterruptedException, IOException {
+      Path inputPath = new Path(key.toString());
+      Path outputPath = getOutputPath(inputPath);
+
+      LOG.info("copy file input=" + inputPath + " output=" + outputPath);
+      if (copyFile(context, inputPath, outputPath)) {
+        LOG.info("copy completed for input=" + inputPath + " output=" + outputPath);
+      }
+    }
+
+    /**
+     * Returns the location where the inputPath will be copied.
+     *  - hfiles are encoded as hfile links hfile-region-table
+     *  - logs are encoded as serverName/logName
+     */
+    private Path getOutputPath(final Path inputPath) throws IOException {
+      Path path;
+      if (HFileLink.isHFileLink(inputPath)) {
+        String family = inputPath.getParent().getName();
+        String table = HFileLink.getReferencedTableName(inputPath.getName());
+        String region = HFileLink.getReferencedRegionName(inputPath.getName());
+        String hfile = HFileLink.getReferencedHFileName(inputPath.getName());
+        path = new Path(table, new Path(region, new Path(family, hfile)));
+      } else if (isHLogLinkPath(inputPath)) {
+        String logName = inputPath.getName();
+        path = new Path(new Path(outputRoot, HConstants.HREGION_OLDLOGDIR_NAME), logName);
+      } else {
+        path = inputPath;
+      }
+      return new Path(outputArchive, path);
+    }
+
+    private boolean copyFile(final Context context, final Path inputPath, final Path outputPath)
+        throws IOException {
+      FSDataInputStream in = openSourceFile(inputPath);
+      if (in == null) {
+        context.getCounter(Counter.MISSING_FILES).increment(1);
+        return false;
+      }
+
+      try {
+        // Verify if the input file exists
+        FileStatus inputStat = getFileStatus(inputFs, inputPath);
+        if (inputStat == null) return false;
+
+        // Verify if the output file exists and is the same that we want to copy
+        FileStatus outputStat = getFileStatus(outputFs, outputPath);
+        if (outputStat != null && sameFile(inputStat, outputStat)) {
+          LOG.info("Skip copy " + inputPath + " to " + outputPath + ", same file.");
+          return true;
+        }
+
+        context.getCounter(Counter.BYTES_EXPECTED).increment(inputStat.getLen());
+
+        // Ensure that the output folder is there and copy the file
+        outputFs.mkdirs(outputPath.getParent());
+        FSDataOutputStream out = outputFs.create(outputPath, true);
+        try {
+          if (!copyData(context, inputPath, in, outputPath, out, inputStat.getLen()))
+            return false;
+        } finally {
+          out.close();
+        }
+
+        // Preserve attributes
+        return preserveAttributes(outputPath, inputStat);
+      } finally {
+        in.close();
+      }
+    }
+
+    /**
+     * Preserve the files attribute selected by the user copying them from the source file
+     */
+    private boolean preserveAttributes(final Path path, final FileStatus refStat) {
+      FileStatus stat;
+      try {
+        stat = outputFs.getFileStatus(path);
+      } catch (IOException e) {
+        LOG.warn("Unable to get the status for file=" + path);
+        return false;
+      }
+
+      try {
+        if (filesMode > 0 && stat.getPermission().toShort() != filesMode) {
+          outputFs.setPermission(path, new FsPermission(filesMode));
+        } else if (!stat.getPermission().equals(refStat.getPermission())) {
+          outputFs.setPermission(path, refStat.getPermission());
+        }
+      } catch (IOException e) {
+        LOG.error("Unable to set the permission for file=" + path, e);
+        return false;
+      }
+
+      try {
+        String user = (filesUser != null) ? filesUser : refStat.getOwner();
+        String group = (filesGroup != null) ? filesGroup : refStat.getGroup();
+        if (!(user.equals(stat.getOwner()) && group.equals(stat.getGroup()))) {
+          outputFs.setOwner(path, user, group);
+        }
+      } catch (IOException e) {
+        LOG.error("Unable to set the owner/group for file=" + path, e);
+        return false;
+      }
+
+      return true;
+    }
+
+    private boolean copyData(final Context context,
+        final Path inputPath, final FSDataInputStream in,
+        final Path outputPath, final FSDataOutputStream out,
+        final long inputFileSize) {
+      final String statusMessage = "copied %s/" + StringUtils.humanReadableInt(inputFileSize) +
+                                   " (%.3f%%) from " + inputPath + " to " + outputPath;
+
+      try {
+        byte[] buffer = new byte[BUFFER_SIZE];
+        long totalBytesWritten = 0;
+        int reportBytes = 0;
+        int bytesRead;
+
+        while ((bytesRead = in.read(buffer)) > 0) {
+          out.write(buffer, 0, bytesRead);
+          totalBytesWritten += bytesRead;
+          reportBytes += bytesRead;
+
+          if (reportBytes >= REPORT_SIZE) {
+            context.getCounter(Counter.BYTES_COPIED).increment(reportBytes);
+            context.setStatus(String.format(statusMessage,
+                              StringUtils.humanReadableInt(totalBytesWritten),
+                              reportBytes/(float)inputFileSize));
+            reportBytes = 0;
+          }
+        }
+
+        context.getCounter(Counter.BYTES_COPIED).increment(reportBytes);
+        context.setStatus(String.format(statusMessage,
+                          StringUtils.humanReadableInt(totalBytesWritten),
+                          reportBytes/(float)inputFileSize));
+
+        // Verify that the written size match
+        if (totalBytesWritten != inputFileSize) {
+          LOG.error("number of bytes copied not matching copied=" + totalBytesWritten +
+                    " expected=" + inputFileSize + " for file=" + inputPath);
+          context.getCounter(Counter.COPY_FAILED).increment(1);
+          return false;
+        }
+
+        return true;
+      } catch (IOException e) {
+        LOG.error("Error copying " + inputPath + " to " + outputPath, e);
+        context.getCounter(Counter.COPY_FAILED).increment(1);
+        return false;
+      }
+    }
+
+    private FSDataInputStream openSourceFile(final Path path) {
+      try {
+        if (HFileLink.isHFileLink(path)) {
+          return new HFileLink(inputRoot, inputArchive, path).open(inputFs);
+        } else if (isHLogLinkPath(path)) {
+          String serverName = path.getParent().getName();
+          String logName = path.getName();
+          return new HLogLink(inputRoot, serverName, logName).open(inputFs);
+        }
+        return inputFs.open(path);
+      } catch (IOException e) {
+        LOG.error("Unable to open source file=" + path, e);
+        return null;
+      }
+    }
+
+    private FileStatus getFileStatus(final FileSystem fs, final Path path) {
+      try {
+        if (HFileLink.isHFileLink(path)) {
+          Path refPath = HFileLink.getReferencedPath(fs, inputRoot, inputArchive, path);
+          return fs.getFileStatus(refPath);
+        } else if (isHLogLinkPath(path)) {
+          String serverName = path.getParent().getName();
+          String logName = path.getName();
+          return new HLogLink(inputRoot, serverName, logName).getFileStatus(fs);
+        }
+        return fs.getFileStatus(path);
+      } catch (IOException e) {
+        LOG.warn("Unable to get the status for file=" + path);
+        return null;
+      }
+    }
+
+    private FileChecksum getFileChecksum(final FileSystem fs, final Path path) {
+      try {
+        return fs.getFileChecksum(path);
+      } catch (IOException e) {
+        LOG.warn("Unable to get checksum for file=" + path, e);
+        return null;
+      }
+    }
+
+    /**
+     * Check if the two files are equal by looking at the file length,
+     * and at the checksum (if user has specified the verifyChecksum flag).
+     */
+    private boolean sameFile(final FileStatus inputStat, final FileStatus outputStat) {
+      // Not matching length
+      if (inputStat.getLen() != outputStat.getLen()) return false;
+
+      // Mark files as equals, since user asked for no checksum verification
+      if (!verifyChecksum) return true;
+
+      // If checksums are not available, files are not the same.
+      FileChecksum inChecksum = getFileChecksum(inputFs, inputStat.getPath());
+      if (inChecksum == null) return false;
+
+      FileChecksum outChecksum = getFileChecksum(outputFs, outputStat.getPath());
+      if (outChecksum == null) return false;
+
+      return inChecksum.equals(outChecksum);
+    }
+
+    /**
+     * HLog files are encoded as serverName/logName
+     * and since all the other files should be in /hbase/table/..path..
+     * we can rely on the depth, for now.
+     */
+    private static boolean isHLogLinkPath(final Path path) {
+      return path.depth() == 2;
+    }
+  }
+
+  /**
+   * Extract the list of files (HFiles/HLogs) to copy using Map-Reduce.
+   * @return list of files referenced by the snapshot (pair of path and size)
+   */
+  private List<Pair<Path, Long>> getSnapshotFiles(final FileSystem fs, final Path snapshotDir) throws IOException {
+    SnapshotDescription snapshotDesc = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
+
+    final List<Pair<Path, Long>> files = new ArrayList<Pair<Path, Long>>();
+    final String table = snapshotDesc.getTable();
+    final Configuration conf = getConf();
+
+    // Get snapshot files
+    SnapshotReferenceUtil.visitReferencedFiles(fs, snapshotDir,
+      new SnapshotReferenceUtil.FileVisitor() {
+        public void storeFile (final String region, final String family, final String hfile)
+            throws IOException {
+          Path path = new Path(family, HFileLink.createHFileLinkName(table, region, hfile));
+          long size = fs.getFileStatus(HFileLink.getReferencedPath(conf, fs, path)).getLen();
+          files.add(new Pair<Path, Long>(path, size));
+        }
+
+        public void recoveredEdits (final String region, final String logfile)
+            throws IOException {
+          // copied with the snapshot referenecs
+        }
+
+        public void logFile (final String server, final String logfile)
+            throws IOException {
+          long size = new HLogLink(conf, server, logfile).getFileStatus(fs).getLen();
+          files.add(new Pair<Path, Long>(new Path(server, logfile), size));
+        }
+    });
+
+    return files;
+  }
+
+  /**
+   * Given a list of file paths and sizes, create around ngroups in as balanced a way as possible.
+   * The groups created will have similar amounts of bytes.
+   * <p>
+   * The algorithm used is pretty straightforward; the file list is sorted by size,
+   * and then each group fetch the bigger file available, iterating through groups
+   * alternating the direction.
+   */
+  static List<List<Path>> getBalancedSplits(final List<Pair<Path, Long>> files, int ngroups) {
+    // Sort files by size, from small to big
+    Collections.sort(files, new Comparator<Pair<Path, Long>>() {
+      public int compare(Pair<Path, Long> a, Pair<Path, Long> b) {
+        long r = a.getSecond() - b.getSecond();
+        return (r < 0) ? -1 : ((r > 0) ? 1 : 0);
+      }
+    });
+
+    // create balanced groups
+    List<List<Path>> fileGroups = new LinkedList<List<Path>>();
+    long[] sizeGroups = new long[ngroups];
+    int hi = files.size() - 1;
+    int lo = 0;
+
+    List<Path> group;
+    int dir = 1;
+    int g = 0;
+
+    while (hi >= lo) {
+      if (g == fileGroups.size()) {
+        group = new LinkedList<Path>();
+        fileGroups.add(group);
+      } else {
+        group = fileGroups.get(g);
+      }
+
+      Pair<Path, Long> fileInfo = files.get(hi--);
+
+      // add the hi one
+      sizeGroups[g] += fileInfo.getSecond();
+      group.add(fileInfo.getFirst());
+
+      // change direction when at the end or the beginning
+      g += dir;
+      if (g == ngroups) {
+        dir = -1;
+        g = ngroups - 1;
+      } else if (g < 0) {
+        dir = 1;
+        g = 0;
+      }
+    }
+
+    if (LOG.isDebugEnabled()) {
+      for (int i = 0; i < sizeGroups.length; ++i) {
+        LOG.debug("export split=" + i + " size=" + StringUtils.humanReadableInt(sizeGroups[i]));
+      }
+    }
+
+    return fileGroups;
+  }
+
+  private static Path getInputFolderPath(final FileSystem fs, final Configuration conf)
+      throws IOException, InterruptedException {
+    String stagingName = "exportSnapshot-" + EnvironmentEdgeManager.currentTimeMillis();
+    Path stagingDir = new Path(conf.get(CONF_TMP_DIR), stagingName);
+    fs.mkdirs(stagingDir);
+    return new Path(stagingDir, INPUT_FOLDER_PREFIX +
+      String.valueOf(EnvironmentEdgeManager.currentTimeMillis()));
+  }
+
+  /**
+   * Create the input files, with the path to copy, for the MR job.
+   * Each input files contains n files, and each input file has a similar amount data to copy.
+   * The number of input files created are based on the number of mappers provided as argument
+   * and the number of the files to copy.
+   */
+  private static Path[] createInputFiles(final Configuration conf,
+      final List<Pair<Path, Long>> snapshotFiles, int mappers)
+      throws IOException, InterruptedException {
+    FileSystem fs = FileSystem.get(conf);
+    Path inputFolderPath = getInputFolderPath(fs, conf);
+    LOG.debug("Input folder location: " + inputFolderPath);
+
+    List<List<Path>> splits = getBalancedSplits(snapshotFiles, mappers);
+    Path[] inputFiles = new Path[splits.size()];
+
+    Text key = new Text();
+    for (int i = 0; i < inputFiles.length; i++) {
+      List<Path> files = splits.get(i);
+      inputFiles[i] = new Path(inputFolderPath, String.format("export-%d.seq", i));
+      SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, inputFiles[i],
+        Text.class, NullWritable.class);
+      LOG.debug("Input split: " + i);
+      try {
+        for (Path file: files) {
+          LOG.debug(file.toString());
+          key.set(file.toString());
+          writer.append(key, NullWritable.get());
+        }
+      } finally {
+        writer.close();
+      }
+    }
+
+    return inputFiles;
+  }
+
+  /**
+   * Run Map-Reduce Job to perform the files copy.
+   */
+  private boolean runCopyJob(final Path inputRoot, final Path outputRoot,
+      final List<Pair<Path, Long>> snapshotFiles, final boolean verifyChecksum,
+      final String filesUser, final String filesGroup, final int filesMode,
+      final int mappers) throws IOException, InterruptedException, ClassNotFoundException {
+    Configuration conf = getConf();
+    if (filesGroup != null) conf.set(CONF_FILES_GROUP, filesGroup);
+    if (filesUser != null) conf.set(CONF_FILES_USER, filesUser);
+    conf.setInt(CONF_FILES_MODE, filesMode);
+    conf.setBoolean(CONF_CHECKSUM_VERIFY, verifyChecksum);
+    conf.set(CONF_OUTPUT_ROOT, outputRoot.toString());
+    conf.set(CONF_INPUT_ROOT, inputRoot.toString());
+    conf.setInt("mapreduce.job.maps", mappers);
+
+    // job.setMapSpeculativeExecution(false)
+    conf.setBoolean("mapreduce.map.speculative", false);
+    conf.setBoolean("mapreduce.reduce.speculative", false);
+    conf.setBoolean("mapred.map.tasks.speculative.execution", false);
+    conf.setBoolean("mapred.reduce.tasks.speculative.execution", false);
+
+    Job job = new Job(conf);
+    job.setJobName("ExportSnapshot");
+    job.setJarByClass(ExportSnapshot.class);
+    job.setMapperClass(ExportMapper.class);
+    job.setInputFormatClass(SequenceFileInputFormat.class);
+    job.setOutputFormatClass(NullOutputFormat.class);
+    job.setNumReduceTasks(0);
+    for (Path path: createInputFiles(conf, snapshotFiles, mappers)) {
+      LOG.debug("Add Input Path=" + path);
+      SequenceFileInputFormat.addInputPath(job, path);
+    }
+
+    return job.waitForCompletion(true);
+  }
+
+  /**
+   * Execute the export snapshot by copying the snapshot metadata, hfiles and hlogs.
+   * @return 0 on success, and != 0 upon failure.
+   */
+  @Override
+  public int run(String[] args) throws Exception {
+    boolean verifyChecksum = true;
+    String snapshotName = null;
+    String filesGroup = null;
+    String filesUser = null;
+    Path outputRoot = null;
+    int filesMode = 0;
+    int mappers = getConf().getInt("mapreduce.job.maps", 1);
+
+    // Process command line args
+    for (int i = 0; i < args.length; i++) {
+      String cmd = args[i];
+      try {
+        if (cmd.equals("-snapshot")) {
+          snapshotName = args[++i];
+        } else if (cmd.equals("-copy-to")) {
+          outputRoot = new Path(args[++i]);
+        } else if (cmd.equals("-no-checksum-verify")) {
+          verifyChecksum = false;
+        } else if (cmd.equals("-mappers")) {
+          mappers = Integer.parseInt(args[++i]);
+        } else if (cmd.equals("-chuser")) {
+          filesUser = args[++i];
+        } else if (cmd.equals("-chgroup")) {
+          filesGroup = args[++i];
+        } else if (cmd.equals("-chmod")) {
+          filesMode = Integer.parseInt(args[++i], 8);
+        } else if (cmd.equals("-h") || cmd.equals("--help")) {
+          printUsageAndExit();
+        } else {
+          System.err.println("UNEXPECTED: " + cmd);
+          printUsageAndExit();
+        }
+      } catch (Exception e) {
+        printUsageAndExit();
+      }
+    }
+
+    // Check user options
+    if (snapshotName == null) {
+      System.err.println("Snapshot name not provided.");
+      printUsageAndExit();
+    }
+
+    if (outputRoot == null) {
+      System.err.println("Destination file-system not provided.");
+      printUsageAndExit();
+    }
+
+    Configuration conf = getConf();
+    Path inputRoot = FSUtils.getRootDir(conf);
+    FileSystem inputFs = FileSystem.get(conf);
+    FileSystem outputFs = FileSystem.get(outputRoot.toUri(), new Configuration());
+
+    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, inputRoot);
+    Path snapshotTmpDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshotName, outputRoot);
+    Path outputSnapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, outputRoot);
+
+    // Check if the snapshot already exists
+    if (outputFs.exists(outputSnapshotDir)) {
+      System.err.println("The snapshot '" + snapshotName +
+        "' already exists in the destination: " + outputSnapshotDir);
+      return 1;
+    }
+
+    // Check if the snapshot already in-progress
+    if (outputFs.exists(snapshotTmpDir)) {
+      System.err.println("A snapshot with the same name '" + snapshotName + "' is in-progress");
+      return 1;
+    }
+
+    // Step 0 - Extract snapshot files to copy
+    final List<Pair<Path, Long>> files = getSnapshotFiles(inputFs, snapshotDir);
+
+    // Step 1 - Copy fs1:/.snapshot/<snapshot> to  fs2:/.snapshot/.tmp/<snapshot>
+    // The snapshot references must be copied before the hfiles otherwise the cleaner
+    // will remove them because they are unreferenced.
+    try {
+      FileUtil.copy(inputFs, snapshotDir, outputFs, snapshotTmpDir, false, false, conf);
+    } catch (IOException e) {
+      System.err.println("Failed to copy the snapshot directory: from=" + snapshotDir +
+        " to=" + snapshotTmpDir);
+      e.printStackTrace(System.err);
+      return 1;
+    }
+
+    // Step 2 - Start MR Job to copy files
+    // The snapshot references must be copied before the files otherwise the files gets removed
+    // by the HFileArchiver, since they have no references.
+    try {
+      if (!runCopyJob(inputRoot, outputRoot, files, verifyChecksum,
+          filesUser, filesGroup, filesMode, mappers)) {
+        throw new ExportSnapshotException("Snapshot export failed!");
+      }
+
+      // Step 3 - Rename fs2:/.snapshot/.tmp/<snapshot> fs2:/.snapshot/<snapshot>
+      if (!outputFs.rename(snapshotTmpDir, outputSnapshotDir)) {
+        System.err.println("Snapshot export failed!");
+        System.err.println("Unable to rename snapshot directory from=" +
+                           snapshotTmpDir + " to=" + outputSnapshotDir);
+        return 1;
+      }
+
+      return 0;
+    } catch (Exception e) {
+      System.err.println("Snapshot export failed!");
+      e.printStackTrace(System.err);
+      outputFs.delete(outputSnapshotDir, true);
+      return 1;
+    }
+  }
+
+  // ExportSnapshot
+  private void printUsageAndExit() {
+    System.err.printf("Usage: bin/hbase %s [options]\n", getClass().getName());
+    System.err.println(" where [options] are:");
+    System.err.println("  -h|-help                Show this help and exit.");
+    System.err.println("  -snapshot NAME          Snapshot to restore.");
+    System.err.println("  -copy-to NAME           Remote destination hdfs://");
+    System.err.println("  -no-checksum-verify     Do not verify checksum.");
+    System.err.println("  -chuser USERNAME        Change the owner of the files to the specified one.");
+    System.err.println("  -chgroup GROUP          Change the group of the files to the specified one.");
+    System.err.println("  -chmod MODE             Change the permission of the files to the specified one.");
+    System.err.println("  -mappers                Number of mappers to use during the copy (mapreduce.job.maps).");
+    System.err.println();
+    System.err.println("Examples:");
+    System.err.println("  hbase " + getClass() + " \\");
+    System.err.println("    -snapshot MySnapshot -copy-to hdfs:///srv2:8082/hbase \\");
+    System.err.println("    -chuser MyUser -chgroup MyGroup -chmod 700 -mappers 16");
+    System.exit(1);
+  }
+
+  /**
+   * The guts of the {@link #main} method.
+   * Call this method to avoid the {@link #main(String[])} System.exit.
+   * @param args
+   * @return errCode
+   * @throws Exception
+   */
+  static int innerMain(final Configuration conf, final String [] args) throws Exception {
+    return ToolRunner.run(conf, new ExportSnapshot(), args);
+  }
+
+  public static void main(String[] args) throws Exception {
+     System.exit(innerMain(HBaseConfiguration.create(), args));
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshotException.java
new file mode 100644
index 0000000..76a83cd
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshotException.java
@@ -0,0 +1,43 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+/**
+ * Thrown when a snapshot could not be exported due to an error during the operation.
+ */
+@InterfaceAudience.Public
+@SuppressWarnings("serial")
+public class ExportSnapshotException extends HBaseSnapshotException {
+
+  /**
+   * @param msg message describing the exception
+   */
+  public ExportSnapshotException(String msg) {
+    super(msg);
+  }
+
+  /**
+   * @param message message describing the exception
+   * @param e cause
+   */
+  public ExportSnapshotException(String message, Exception e) {
+    super(message, e);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java
new file mode 100644
index 0000000..70a8842
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.HBaseIOException;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * General exception base class for when a snapshot fails
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class HBaseSnapshotException extends HBaseIOException {
+
+  private SnapshotDescription description;
+
+  /**
+   * Some exception happened for a snapshot and don't even know the snapshot that it was about
+   * @param msg Full description of the failure
+   */
+  public HBaseSnapshotException(String msg) {
+    super(msg);
+  }
+
+  /**
+   * Exception for the given snapshot that has no previous root cause
+   * @param msg reason why the snapshot failed
+   * @param desc description of the snapshot that is being failed
+   */
+  public HBaseSnapshotException(String msg, SnapshotDescription desc) {
+    super(msg);
+    this.description = desc;
+  }
+
+  /**
+   * Exception for the given snapshot due to another exception
+   * @param msg reason why the snapshot failed
+   * @param cause root cause of the failure
+   * @param desc description of the snapshot that is being failed
+   */
+  public HBaseSnapshotException(String msg, Throwable cause, SnapshotDescription desc) {
+    super(msg, cause);
+    this.description = desc;
+  }
+
+  /**
+   * Exception when the description of the snapshot cannot be determined, due to some root other
+   * root cause
+   * @param message description of what caused the failure
+   * @param e root cause
+   */
+  public HBaseSnapshotException(String message, Exception e) {
+    super(message, e);
+  }
+
+  public SnapshotDescription getSnapshotDescription() {
+    return this.description;
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceRegionHFilesTask.java b/src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceRegionHFilesTask.java
new file mode 100644
index 0000000..60d48d9
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceRegionHFilesTask.java
@@ -0,0 +1,127 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.util.FSUtils;
+
+/**
+ * Reference all the hfiles in a region for a snapshot.
+ * <p>
+ * Doesn't take into acccount if the hfiles are valid or not, just keeps track of what's in the
+ * region's directory.
+ */
+public class ReferenceRegionHFilesTask extends SnapshotTask {
+
+  public static final Log LOG = LogFactory.getLog(ReferenceRegionHFilesTask.class);
+  private final Path regiondir;
+  private final FileSystem fs;
+  private final PathFilter fileFilter;
+  private final Path snapshotDir;
+
+  /**
+   * Reference all the files in the given region directory
+   * @param snapshot snapshot for which to add references
+   * @param monitor to check/send error
+   * @param regionDir region directory to look for errors
+   * @param fs {@link FileSystem} where the snapshot/region live
+   * @param regionSnapshotDir directory in the snapshot to store region files
+   */
+  public ReferenceRegionHFilesTask(final SnapshotDescription snapshot,
+      ForeignExceptionDispatcher monitor, Path regionDir, final FileSystem fs, Path regionSnapshotDir) {
+    super(snapshot, monitor);
+    this.regiondir = regionDir;
+    this.fs = fs;
+
+    this.fileFilter = new PathFilter() {
+      @Override
+      public boolean accept(Path path) {
+        try {
+          return fs.isFile(path);
+        } catch (IOException e) {
+          LOG.error("Failed to reach fs to check file:" + path + ", marking as not file");
+          ReferenceRegionHFilesTask.this.snapshotFailure("Failed to reach fs to check file status",
+            e);
+          return false;
+        }
+      }
+    };
+    this.snapshotDir = regionSnapshotDir;
+  }
+
+  @Override
+  public Void call() throws IOException {
+    FileStatus[] families = FSUtils.listStatus(fs, regiondir, new FSUtils.FamilyDirFilter(fs));
+
+    // if no families, then we are done again
+    if (families == null || families.length == 0) {
+      LOG.info("No families under region directory:" + regiondir
+          + ", not attempting to add references.");
+      return null;
+    }
+
+    // snapshot directories to store the hfile reference
+    List<Path> snapshotFamilyDirs = TakeSnapshotUtils.getFamilySnapshotDirectories(snapshot,
+      snapshotDir, families);
+
+    LOG.debug("Add hfile references to snapshot directories:" + snapshotFamilyDirs);
+    for (int i = 0; i < families.length; i++) {
+      FileStatus family = families[i];
+      Path familyDir = family.getPath();
+      // get all the hfiles in the family
+      FileStatus[] hfiles = FSUtils.listStatus(fs, familyDir, fileFilter);
+
+      // if no hfiles, then we are done with this family
+      if (hfiles == null || hfiles.length == 0) {
+        LOG.debug("Not hfiles found for family: " + familyDir + ", skipping.");
+        continue;
+      }
+
+      // make the snapshot's family directory
+      Path snapshotFamilyDir = snapshotFamilyDirs.get(i);
+      fs.mkdirs(snapshotFamilyDir);
+
+      // create a reference for each hfile
+      for (FileStatus hfile : hfiles) {
+        // references are 0-length files, relying on file name.
+        Path referenceFile = new Path(snapshotFamilyDir, hfile.getPath().getName());
+        LOG.debug("Creating reference for:" + hfile.getPath() + " at " + referenceFile);
+        if (!fs.createNewFile(referenceFile)) {
+          throw new IOException("Failed to create reference file:" + referenceFile);
+        }
+      }
+    }
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Finished referencing hfiles, current region state:");
+      FSUtils.logFileSystemState(fs, regiondir, LOG);
+      LOG.debug("and the snapshot directory:");
+      FSUtils.logFileSystemState(fs, snapshotDir, LOG);
+    }
+    return null;
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceServerWALsTask.java b/src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceServerWALsTask.java
new file mode 100644
index 0000000..b0bdbfa
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceServerWALsTask.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.errorhandling.ForeignException;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.util.FSUtils;
+
+/**
+ * Reference all the WAL files under a server's WAL directory
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class ReferenceServerWALsTask extends SnapshotTask {
+  private static final Log LOG = LogFactory.getLog(ReferenceServerWALsTask.class);
+  private final FileSystem fs;
+  private final Configuration conf;
+  private final String serverName;
+  private Path logDir;
+
+  /**
+   * @param snapshot snapshot being run
+   * @param failureListener listener to check for errors while running the operation and to
+   *          propagate errors found while running the task
+   * @param logDir log directory for the server. Name of the directory is taken as the name of the
+   *          server
+   * @param conf {@link Configuration} to extract filesystem information
+   * @param fs filesystem where the log files are stored and should be referenced
+   */
+  public ReferenceServerWALsTask(SnapshotDescription snapshot,
+      ForeignExceptionDispatcher failureListener, final Path logDir, final Configuration conf,
+      final FileSystem fs) {
+    super(snapshot, failureListener);
+    this.fs = fs;
+    this.conf = conf;
+    this.serverName = logDir.getName();
+    this.logDir = logDir;
+  }
+
+  /**
+   * Create reference files (empty files with the same path and file name as original).
+   * @throws IOException exception from hdfs or network problems
+   * @throws ForeignException exception from an external procedure
+   */
+  @Override
+  public Void call() throws IOException, ForeignException {
+    // TODO switch to using a single file to reference all required WAL files
+
+    // Iterate through each of the log files and add a reference to it.
+    // assumes that all the files under the server's logs directory is a log
+    FileStatus[] serverLogs = FSUtils.listStatus(fs, logDir, null);
+    if (serverLogs == null) LOG.info("No logs for server directory:" + logDir
+        + ", done referencing files.");
+
+    if (LOG.isDebugEnabled()) LOG.debug("Adding references for WAL files:"
+        + Arrays.toString(serverLogs));
+
+    for (FileStatus file : serverLogs) {
+      this.rethrowException();
+
+      // add the reference to the file. ex: hbase/.snapshots/.logs/<serverName>/<hlog>
+      Path rootDir = FSUtils.getRootDir(conf);
+      Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(this.snapshot, rootDir);
+      Path snapshotLogDir = TakeSnapshotUtils.getSnapshotHLogsDir(snapshotDir, serverName);
+      // actually store the reference on disk (small file)
+      Path ref = new Path(snapshotLogDir, file.getPath().getName());
+      if (!fs.createNewFile(ref)) {
+        if (!fs.exists(ref)) {
+          throw new IOException("Couldn't create reference for:" + file.getPath());
+        }
+      }
+      LOG.debug("Completed WAL referencing for: " + file.getPath() + " to " + ref);
+    }
+
+    LOG.debug("Successfully completed WAL referencing for ALL files");
+    return null;
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotException.java
new file mode 100644
index 0000000..ff40783
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotException.java
@@ -0,0 +1,43 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * Thrown when a snapshot could not be restored due to a server-side error when restoring it.
+ */
+@SuppressWarnings("serial")
+public class RestoreSnapshotException extends HBaseSnapshotException {
+  public RestoreSnapshotException(String msg, SnapshotDescription desc) {
+    super(msg, desc);
+  }
+
+  public RestoreSnapshotException(String msg, Throwable cause, SnapshotDescription desc) {
+    super(msg, cause, desc);
+  }
+
+  public RestoreSnapshotException(String msg) {
+    super(msg);
+  }
+
+  public RestoreSnapshotException(String message, Exception e) {
+    super(message, e);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java b/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
new file mode 100644
index 0000000..4e60c1b
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
@@ -0,0 +1,424 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.backup.HFileArchiver;
+import org.apache.hadoop.hbase.catalog.CatalogTracker;
+import org.apache.hadoop.hbase.catalog.MetaEditor;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.io.HFileLink;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.FSVisitor;
+import org.apache.hadoop.hbase.util.ModifyRegionUtils;
+
+/**
+ * Helper to Restore/Clone a Snapshot
+ *
+ * <p>The helper assumes that a table is already created, and by calling restore()
+ * the content present in the snapshot will be restored as the new content of the table.
+ *
+ * <p>Clone from Snapshot: If the target table is empty, the restore operation
+ * is just a "clone operation", where the only operations are:
+ * <ul>
+ *  <li>for each region in the snapshot create a new region
+ *    (note that the region will have a different name, since the encoding contains the table name)
+ *  <li>for each file in the region create a new HFileLink to point to the original file.
+ *  <li>restore the logs, if any
+ * </ul>
+ *
+ * <p>Restore from Snapshot:
+ * <ul>
+ *  <li>for each region in the table verify which are available in the snapshot and which are not
+ *    <ul>
+ *    <li>if the region is not present in the snapshot, remove it.
+ *    <li>if the region is present in the snapshot
+ *      <ul>
+ *      <li>for each file in the table region verify which are available in the snapshot
+ *        <ul>
+ *          <li>if the hfile is not present in the snapshot, remove it
+ *          <li>if the hfile is present, keep it (nothing to do)
+ *        </ul>
+ *      <li>for each file in the snapshot region but not in the table
+ *        <ul>
+ *          <li>create a new HFileLink that point to the original file
+ *        </ul>
+ *      </ul>
+ *    </ul>
+ *  <li>for each region in the snapshot not present in the current table state
+ *    <ul>
+ *    <li>create a new region and for each file in the region create a new HFileLink
+ *      (This is the same as the clone operation)
+ *    </ul>
+ *  <li>restore the logs, if any
+ * </ul>
+ */
+@InterfaceAudience.Private
+public class RestoreSnapshotHelper {
+	private static final Log LOG = LogFactory.getLog(RestoreSnapshotHelper.class);
+
+  private final Map<byte[], byte[]> regionsMap =
+        new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+
+  private final ForeignExceptionDispatcher monitor;
+
+  private final SnapshotDescription snapshotDesc;
+  private final Path snapshotDir;
+
+  private final HTableDescriptor tableDesc;
+  private final Path tableDir;
+
+  private final CatalogTracker catalogTracker;
+  private final Configuration conf;
+  private final FileSystem fs;
+
+  public RestoreSnapshotHelper(final Configuration conf, final FileSystem fs,
+      final CatalogTracker catalogTracker,
+      final SnapshotDescription snapshotDescription, final Path snapshotDir,
+      final HTableDescriptor tableDescriptor, final Path tableDir,
+      final ForeignExceptionDispatcher monitor)
+  {
+    this.fs = fs;
+    this.conf = conf;
+    this.catalogTracker = catalogTracker;
+    this.snapshotDesc = snapshotDescription;
+    this.snapshotDir = snapshotDir;
+    this.tableDesc = tableDescriptor;
+    this.tableDir = tableDir;
+    this.monitor = monitor;
+  }
+
+  /**
+   * Restore table to a specified snapshot state.
+   */
+  public void restore() throws IOException {
+    long startTime = EnvironmentEdgeManager.currentTimeMillis();
+
+    LOG.debug("starting restore");
+    Set<String> snapshotRegionNames = SnapshotReferenceUtil.getSnapshotRegionNames(fs, snapshotDir);
+    if (snapshotRegionNames == null) {
+      LOG.warn("Nothing to restore. Snapshot " + snapshotDesc + " looks empty");
+      return;
+    }
+
+    // Identify which region are still available and which not.
+    // NOTE: we rely upon the region name as: "table name, start key, end key"
+    List<HRegionInfo> tableRegions = getTableRegions();
+    if (tableRegions != null) {
+      monitor.rethrowException();
+      List<HRegionInfo> regionsToRestore = new LinkedList<HRegionInfo>();
+      List<HRegionInfo> regionsToRemove = new LinkedList<HRegionInfo>();
+
+      for (HRegionInfo regionInfo: tableRegions) {
+        String regionName = regionInfo.getEncodedName();
+        if (snapshotRegionNames.contains(regionName)) {
+          LOG.info("region to restore: " + regionName);
+          snapshotRegionNames.remove(regionInfo);
+          regionsToRestore.add(regionInfo);
+        } else {
+          LOG.info("region to remove: " + regionName);
+          regionsToRemove.add(regionInfo);
+        }
+      }
+
+      // Restore regions using the snapshot data
+      monitor.rethrowException();
+      restoreRegions(regionsToRestore);
+
+      // Remove regions from the current table
+      monitor.rethrowException();
+      ModifyRegionUtils.deleteRegions(fs, catalogTracker, regionsToRemove);
+    }
+
+    // Regions to Add: present in the snapshot but not in the current table
+    if (snapshotRegionNames.size() > 0) {
+      List<HRegionInfo> regionsToAdd = new LinkedList<HRegionInfo>();
+
+      monitor.rethrowException();
+      for (String regionName: snapshotRegionNames) {
+        LOG.info("region to add: " + regionName);
+        Path regionDir = new Path(snapshotDir, regionName);
+        regionsToAdd.add(HRegion.loadDotRegionInfoFileContent(fs, regionDir));
+      }
+
+      // Create new regions cloning from the snapshot
+      monitor.rethrowException();
+      cloneRegions(regionsToAdd);
+    }
+
+    // Restore WALs
+    monitor.rethrowException();
+    restoreWALs();
+  }
+
+  /**
+   * Restore specified regions by restoring content to the snapshot state.
+   */
+  private void restoreRegions(final List<HRegionInfo> regions) throws IOException {
+    if (regions == null || regions.size() == 0) return;
+    for (HRegionInfo hri: regions) restoreRegion(hri);
+  }
+
+  /**
+   * Restore region by removing files not it in the snapshot
+   * and adding the missing ones from the snapshot.
+   */
+  private void restoreRegion(HRegionInfo regionInfo) throws IOException {
+    Path snapshotRegionDir = new Path(snapshotDir, regionInfo.getEncodedName());
+    Map<String, List<String>> snapshotFiles =
+                SnapshotReferenceUtil.getRegionHFileReferences(fs, snapshotRegionDir);
+
+    Path regionDir = new Path(tableDir, regionInfo.getEncodedName());
+    String tableName = tableDesc.getNameAsString();
+
+    for (Map.Entry<String, List<String>> familyEntry: snapshotFiles.entrySet()) {
+      byte[] family = Bytes.toBytes(familyEntry.getKey());
+      Path familyDir = new Path(regionDir, familyEntry.getKey());
+      Set<String> familyFiles = getTableRegionFamilyFiles(familyDir);
+
+      List<String> hfilesToAdd = new LinkedList<String>();
+      for (String hfileName: familyEntry.getValue()) {
+        if (familyFiles.contains(hfileName)) {
+          // HFile already present
+          familyFiles.remove(hfileName);
+        } else {
+          // HFile missing
+          hfilesToAdd.add(hfileName);
+        }
+      }
+
+      // Remove hfiles not present in the snapshot
+      for (String hfileName: familyFiles) {
+        Path hfile = new Path(familyDir, hfileName);
+        LOG.trace("Removing hfile=" + hfile + " from table=" + tableName);
+        HFileArchiver.archiveStoreFile(fs, regionInfo, conf, tableDir, family, hfile);
+      }
+
+      // Restore Missing files
+      for (String hfileName: hfilesToAdd) {
+        LOG.trace("Adding HFileLink " + hfileName + " to table=" + tableName);
+        restoreStoreFile(familyDir, regionInfo, hfileName);
+      }
+    }
+  }
+
+  /**
+   * @return The set of files in the specified family directory.
+   */
+  private Set<String> getTableRegionFamilyFiles(final Path familyDir) throws IOException {
+    Set<String> familyFiles = new HashSet<String>();
+
+    FileStatus[] hfiles = FSUtils.listStatus(fs, familyDir);
+    if (hfiles == null) return familyFiles;
+
+    for (FileStatus hfileRef: hfiles) {
+      String hfileName = hfileRef.getPath().getName();
+      familyFiles.add(hfileName);
+    }
+
+    return familyFiles;
+  }
+
+  /**
+   * Clone specified regions. For each region create a new region
+   * and create a HFileLink for each hfile.
+   */
+  private void cloneRegions(final List<HRegionInfo> regions) throws IOException {
+    if (regions == null || regions.size() == 0) return;
+
+    final Map<String, HRegionInfo> snapshotRegions =
+      new HashMap<String, HRegionInfo>(regions.size());
+
+    // clone region info (change embedded tableName with the new one)
+    HRegionInfo[] clonedRegionsInfo = new HRegionInfo[regions.size()];
+    for (int i = 0; i < clonedRegionsInfo.length; ++i) {
+      // clone the region info from the snapshot region info
+      HRegionInfo snapshotRegionInfo = regions.get(i);
+      clonedRegionsInfo[i] = cloneRegionInfo(snapshotRegionInfo);
+
+      // add the region name mapping between snapshot and cloned
+      String snapshotRegionName = snapshotRegionInfo.getEncodedName();
+      String clonedRegionName = clonedRegionsInfo[i].getEncodedName();
+      regionsMap.put(Bytes.toBytes(snapshotRegionName), Bytes.toBytes(clonedRegionName));
+      LOG.info("clone region=" + snapshotRegionName + " as " + clonedRegionName);
+
+      // Add mapping between cloned region name and snapshot region info
+      snapshotRegions.put(clonedRegionName, snapshotRegionInfo);
+    }
+
+    // create the regions on disk
+    List<HRegionInfo> clonedRegions = ModifyRegionUtils.createRegions(conf, FSUtils.getRootDir(conf),
+      tableDesc, clonedRegionsInfo, catalogTracker, new ModifyRegionUtils.RegionFillTask() {
+        public void fillRegion(final HRegion region) throws IOException {
+          cloneRegion(region, snapshotRegions.get(region.getRegionInfo().getEncodedName()));
+        }
+      });
+    if (regions != null && regions.size() > 0) {
+      // add regions to .META.
+      MetaEditor.addRegionsToMeta(catalogTracker, clonedRegions);
+    }
+  }
+
+  /**
+   * Clone region directory content from the snapshot info.
+   *
+   * Each region is encoded with the table name, so the cloned region will have
+   * a different region name.
+   *
+   * Instead of copying the hfiles a HFileLink is created.
+   *
+   * @param region {@link HRegion} cloned
+   * @param snapshotRegionInfo
+   */
+  private void cloneRegion(final HRegion region, final HRegionInfo snapshotRegionInfo)
+      throws IOException {
+    final Path snapshotRegionDir = new Path(snapshotDir, snapshotRegionInfo.getEncodedName());
+    final Path regionDir = new Path(tableDir, region.getRegionInfo().getEncodedName());
+    final String tableName = tableDesc.getNameAsString();
+    SnapshotReferenceUtil.visitRegionStoreFiles(fs, snapshotRegionDir,
+      new FSVisitor.StoreFileVisitor() {
+        public void storeFile (final String region, final String family, final String hfile)
+            throws IOException {
+          LOG.info("Adding HFileLink " + hfile + " to table=" + tableName);
+          Path familyDir = new Path(regionDir, family);
+          restoreStoreFile(familyDir, snapshotRegionInfo, hfile);
+        }
+    });
+  }
+
+  /**
+   * Create a new {@link HFileLink} to reference the store file.
+   *
+   * @param familyDir destination directory for the store file
+   * @param regionInfo destination region info for the table
+   * @param hfileName store file name (can be a Reference, HFileLink or simple HFile)
+   */
+  private void restoreStoreFile(final Path familyDir, final HRegionInfo regionInfo,
+      final String hfileName) throws IOException {
+    if (HFileLink.isHFileLink(hfileName)) {
+      HFileLink.createFromHFileLink(conf, fs, familyDir, hfileName);
+    } else {
+      HFileLink.create(conf, fs, familyDir, regionInfo, hfileName);
+    }
+  }
+
+  /**
+   * Create a new {@link HRegionInfo} from the snapshot region info.
+   * Keep the same startKey, endKey, regionId and split information but change
+   * the table name.
+   *
+   * @param snapshotRegionInfo Info for region to clone.
+   * @return the new HRegion instance
+   */
+  public HRegionInfo cloneRegionInfo(final HRegionInfo snapshotRegionInfo) {
+    return new HRegionInfo(tableDesc.getName(),
+                      snapshotRegionInfo.getStartKey(), snapshotRegionInfo.getEndKey(),
+                      snapshotRegionInfo.isSplit(), snapshotRegionInfo.getRegionId());
+  }
+
+  /**
+   * Restore snapshot WALs.
+   *
+   * Global Snapshot keep a reference to region servers logs present during the snapshot.
+   * (/hbase/.snapshot/snapshotName/.logs/hostName/logName)
+   *
+   * Since each log contains different tables data, logs must be split to
+   * extract the table that we are interested in.
+   */
+  private void restoreWALs() throws IOException {
+    final SnapshotLogSplitter logSplitter = new SnapshotLogSplitter(conf, fs, tableDir,
+                                Bytes.toBytes(snapshotDesc.getTable()), regionsMap);
+    try {
+      // Recover.Edits
+      SnapshotReferenceUtil.visitRecoveredEdits(fs, snapshotDir,
+          new FSVisitor.RecoveredEditsVisitor() {
+        public void recoveredEdits (final String region, final String logfile) throws IOException {
+          Path path = SnapshotReferenceUtil.getRecoveredEdits(snapshotDir, region, logfile);
+          logSplitter.splitRecoveredEdit(path);
+        }
+      });
+
+      // Region Server Logs
+      SnapshotReferenceUtil.visitLogFiles(fs, snapshotDir, new FSVisitor.LogFileVisitor() {
+        public void logFile (final String server, final String logfile) throws IOException {
+          logSplitter.splitLog(server, logfile);
+        }
+      });
+    } finally {
+      logSplitter.close();
+    }
+  }
+
+  /**
+   * @return the set of the regions contained in the table
+   */
+  private List<HRegionInfo> getTableRegions() throws IOException {
+    LOG.debug("get table regions: " + tableDir);
+    FileStatus[] regionDirs = FSUtils.listStatus(fs, tableDir, new FSUtils.RegionDirFilter(fs));
+    if (regionDirs == null) return null;
+
+    List<HRegionInfo> regions = new LinkedList<HRegionInfo>();
+    for (FileStatus regionDir: regionDirs) {
+      HRegionInfo hri = HRegion.loadDotRegionInfoFileContent(fs, regionDir.getPath());
+      regions.add(hri);
+    }
+    LOG.debug("found " + regions.size() + " regions for table=" + tableDesc.getNameAsString());
+    return regions;
+  }
+
+  /**
+   * Create a new table descriptor cloning the snapshot table schema.
+   *
+   * @param admin
+   * @param snapshotTableDescriptor
+   * @param tableName
+   * @return cloned table descriptor
+   * @throws IOException
+   */
+  public static HTableDescriptor cloneTableSchema(final HTableDescriptor snapshotTableDescriptor,
+      final byte[] tableName) throws IOException {
+    HTableDescriptor htd = new HTableDescriptor(tableName);
+    for (HColumnDescriptor hcd: snapshotTableDescriptor.getColumnFamilies()) {
+      htd.addFamily(hcd);
+    }
+    return htd;
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java
new file mode 100644
index 0000000..69dc3d0
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * Thrown when a snapshot could not be created due to a server-side error when taking the snapshot.
+ */
+@SuppressWarnings("serial")
+public class SnapshotCreationException extends HBaseSnapshotException {
+
+  /**
+   * Used internally by the RPC engine to pass the exception back to the client.
+   * @param msg error message to pass back
+   */
+  public SnapshotCreationException(String msg) {
+    super(msg);
+  }
+
+  /**
+   * Failure to create the specified snapshot
+   * @param msg reason why the snapshot couldn't be completed
+   * @param desc description of the snapshot attempted
+   */
+  public SnapshotCreationException(String msg, SnapshotDescription desc) {
+    super(msg, desc);
+  }
+
+  /**
+   * Failure to create the specified snapshot due to an external cause
+   * @param msg reason why the snapshot couldn't be completed
+   * @param cause root cause of the failure
+   * @param desc description of the snapshot attempted
+   */
+  public SnapshotCreationException(String msg, Throwable cause, SnapshotDescription desc) {
+    super(msg, cause, desc);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java
index 2ed0534..e42615b 100644
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java
@@ -31,8 +31,6 @@ import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.snapshot.exception.CorruptedSnapshotException;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.FSUtils;
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDoesNotExistException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDoesNotExistException.java
new file mode 100644
index 0000000..eb02ece
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDoesNotExistException.java
@@ -0,0 +1,45 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+
+/**
+ * Thrown when the server is looking for a snapshot but can't find the snapshot on the filesystem
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class SnapshotDoesNotExistException extends HBaseSnapshotException {
+  /**
+   * @param msg full description of the failure
+   */
+  public SnapshotDoesNotExistException(String msg) {
+    super(msg);
+  }
+
+  /**
+   * @param desc expected snapshot to find
+   */
+  public SnapshotDoesNotExistException(SnapshotDescription desc) {
+    super("Snapshot '" + desc.getName() +"' doesn't exist on the filesystem", desc);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotExistsException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotExistsException.java
new file mode 100644
index 0000000..2ce2d31
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotExistsException.java
@@ -0,0 +1,40 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * Thrown when a snapshot exists but should not
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class SnapshotExistsException extends HBaseSnapshotException {
+
+  /**
+   * Failure due to the snapshot already existing
+   * @param msg full description of the failure
+   * @param desc snapshot that was attempted
+   */
+  public SnapshotExistsException(String msg, SnapshotDescription desc) {
+    super(msg, desc);
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotLogSplitter.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotLogSplitter.java
new file mode 100644
index 0000000..788435c
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotLogSplitter.java
@@ -0,0 +1,196 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.io.HLogLink;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * If the snapshot has references to one or more log files,
+ * those must be split (each log contains multiple tables and regions)
+ * and must be placed in the region/recovered.edits folder.
+ * (recovered.edits files will be played on region startup)
+ *
+ * In case of Restore: the log can just be split in the recovered.edits folder.
+ * In case of Clone: each entry in the log must be modified to use the new region name.
+ * (region names are encoded with: tableName, startKey, regionIdTimeStamp)
+ *
+ * We can't use the normal split code, because the HLogKey contains the
+ * table name and the region name, and in case of "clone from snapshot"
+ * region name and table name will be different and must be replaced in
+ * the recovered.edits.
+ */
+@InterfaceAudience.Private
+class SnapshotLogSplitter implements Closeable {
+  static final Log LOG = LogFactory.getLog(SnapshotLogSplitter.class);
+
+  private final class LogWriter implements Closeable {
+    private HLog.Writer writer;
+    private Path logFile;
+    private long seqId;
+
+    public LogWriter(final Configuration conf, final FileSystem fs,
+        final Path logDir, long seqId) throws IOException {
+      logFile = new Path(logDir, logFileName(seqId, true));
+      this.writer = HLog.createWriter(fs, logFile, conf);
+      this.seqId = seqId;
+    }
+
+    public void close() throws IOException {
+      writer.close();
+
+      Path finalFile = new Path(logFile.getParent(), logFileName(seqId, false));
+      LOG.debug("LogWriter tmpLogFile=" + logFile + " -> logFile=" + finalFile);
+      fs.rename(logFile, finalFile);
+    }
+
+    public void append(final HLog.Entry entry) throws IOException {
+      writer.append(entry);
+      if (seqId < entry.getKey().getLogSeqNum()) {
+        seqId = entry.getKey().getLogSeqNum();
+      }
+    }
+
+    private String logFileName(long seqId, boolean temp) {
+      String fileName = String.format("%019d", seqId);
+      if (temp) fileName += HLog.RECOVERED_LOG_TMPFILE_SUFFIX;
+      return fileName;
+    }
+  }
+
+  private final Map<byte[], LogWriter> regionLogWriters =
+      new TreeMap<byte[], LogWriter>(Bytes.BYTES_COMPARATOR);
+
+  private final Map<byte[], byte[]> regionsMap;
+  private final Configuration conf;
+  private final byte[] snapshotTableName;
+  private final byte[] tableName;
+  private final Path tableDir;
+  private final FileSystem fs;
+
+  /**
+   * @params tableName snapshot table name
+   * @params regionsMap maps original region names to the new ones.
+   */
+  public SnapshotLogSplitter(final Configuration conf, final FileSystem fs,
+      final Path tableDir, final byte[] snapshotTableName,
+      final Map<byte[], byte[]> regionsMap) {
+    this.regionsMap = regionsMap;
+    this.snapshotTableName = snapshotTableName;
+    this.tableName = Bytes.toBytes(tableDir.getName());
+    this.tableDir = tableDir;
+    this.conf = conf;
+    this.fs = fs;
+  }
+
+  public void close() throws IOException {
+    for (LogWriter writer: regionLogWriters.values()) {
+      writer.close();
+    }
+  }
+
+  public void splitLog(final String serverName, final String logfile) throws IOException {
+    LOG.debug("Restore log=" + logfile + " server=" + serverName +
+              " for snapshotTable=" + Bytes.toString(snapshotTableName) +
+              " to table=" + Bytes.toString(tableName));
+    splitLog(new HLogLink(conf, serverName, logfile).getAvailablePath(fs));
+  }
+
+  public void splitRecoveredEdit(final Path editPath) throws IOException {
+    LOG.debug("Restore recover.edits=" + editPath +
+              " for snapshotTable=" + Bytes.toString(snapshotTableName) +
+              " to table=" + Bytes.toString(tableName));
+    splitLog(editPath);
+  }
+
+  /**
+   * Split the snapshot HLog reference into regions recovered.edits.
+   *
+   * The HLogKey contains the table name and the region name,
+   * and they must be changed to the restored table names.
+   *
+   * @param logPath Snapshot HLog reference path
+   */
+  public void splitLog(final Path logPath) throws IOException {
+    HLog.Reader log = HLog.getReader(fs, logPath, conf);
+    try {
+      HLog.Entry entry;
+      LogWriter writer = null;
+      byte[] regionName = null;
+      byte[] newRegionName = null;
+      while ((entry = log.next()) != null) {
+        HLogKey key = entry.getKey();
+
+        // We're interested only in the snapshot table that we're restoring
+        if (!Bytes.equals(key.getTablename(), snapshotTableName)) continue;
+
+        // Writer for region.
+        if (!Bytes.equals(regionName, key.getEncodedRegionName())) {
+          regionName = key.getEncodedRegionName().clone();
+
+          // Get the new region name in case of clone, or use the original one
+          newRegionName = regionsMap.get(regionName);
+          if (newRegionName == null) newRegionName = regionName;
+
+          writer = getOrCreateWriter(newRegionName, key.getLogSeqNum());
+          LOG.debug("+ regionName=" + Bytes.toString(regionName));
+        }
+
+        // Append Entry
+        key = new HLogKey(newRegionName, tableName,
+                          key.getLogSeqNum(), key.getWriteTime(), key.getClusterId());
+        writer.append(new HLog.Entry(key, entry.getEdit()));
+      }
+    } catch (IOException e) {
+      LOG.warn("Something wrong during the log split", e);
+    } finally {
+      log.close();
+    }
+  }
+
+  /**
+   * Create a LogWriter for specified region if not already created.
+   */
+  private LogWriter getOrCreateWriter(final byte[] regionName, long seqId) throws IOException {
+    LogWriter writer = regionLogWriters.get(regionName);
+    if (writer == null) {
+      Path regionDir = HRegion.getRegionDir(tableDir, Bytes.toString(regionName));
+      Path dir = HLog.getRegionDirRecoveredEditsDir(regionDir);
+      fs.mkdirs(dir);
+
+      writer = new LogWriter(conf, fs, dir, seqId);
+      regionLogWriters.put(regionName, writer);
+    }
+    return(writer);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotTask.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotTask.java
new file mode 100644
index 0000000..31a79a1
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotTask.java
@@ -0,0 +1,65 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import java.util.concurrent.Callable;
+
+import org.apache.hadoop.hbase.errorhandling.ForeignException;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * General snapshot operation taken on a regionserver
+ */
+public abstract class SnapshotTask implements ForeignExceptionSnare, Callable<Void>{
+
+  protected final SnapshotDescription snapshot;
+  protected final ForeignExceptionDispatcher errorMonitor;
+
+  /**
+   * @param snapshot Description of the snapshot we are going to operate on
+   * @param monitor listener interested in failures to the snapshot caused by this operation
+   */
+  public SnapshotTask(SnapshotDescription snapshot, ForeignExceptionDispatcher monitor) {
+    assert monitor != null : "ForeignExceptionDispatcher must not be null!";
+    assert snapshot != null : "SnapshotDescription must not be null!";
+    this.snapshot = snapshot;
+    this.errorMonitor = monitor;
+  }
+
+  public void snapshotFailure(String message, Exception e) {
+    ForeignException ee = new ForeignException(message, e);
+    errorMonitor.receive(ee);
+  }
+
+  @Override
+  public void rethrowException() throws ForeignException {
+    this.errorMonitor.rethrowException();
+  }
+
+  @Override
+  public boolean hasException() {
+    return this.errorMonitor.hasException();
+  }
+
+  @Override
+  public ForeignException getException() {
+    return this.errorMonitor.getException();
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/TableInfoCopyTask.java b/src/main/java/org/apache/hadoop/hbase/snapshot/TableInfoCopyTask.java
new file mode 100644
index 0000000..56452e0
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/TableInfoCopyTask.java
@@ -0,0 +1,72 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
+
+/**
+ * Copy the table info into the snapshot directory
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class TableInfoCopyTask extends SnapshotTask {
+
+  public static final Log LOG = LogFactory.getLog(TableInfoCopyTask.class);
+  private final FileSystem fs;
+  private final Path rootDir;
+
+  /**
+   * Copy the table info for the given table into the snapshot
+   * @param monitor listen for errors while running the snapshot
+   * @param snapshot snapshot for which we are copying the table info
+   * @param fs {@link FileSystem} where the tableinfo is stored (and where the copy will be written)
+   * @param rootDir root of the {@link FileSystem} where the tableinfo is stored
+   */
+  public TableInfoCopyTask(ForeignExceptionDispatcher monitor,
+      SnapshotDescription snapshot, FileSystem fs, Path rootDir) {
+    super(snapshot, monitor);
+    this.rootDir = rootDir;
+    this.fs = fs;
+  }
+
+  @Override
+  public Void call() throws Exception {
+    LOG.debug("Running table info copy.");
+    this.rethrowException();
+    LOG.debug("Attempting to copy table info for snapshot:" + this.snapshot);
+    // get the HTable descriptor
+    HTableDescriptor orig = FSTableDescriptors.getTableDescriptor(fs, rootDir,
+      Bytes.toBytes(this.snapshot.getTable()));
+    this.rethrowException();
+    // write a copy of descriptor to the snapshot directory
+    Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
+    FSTableDescriptors.createTableDescriptorForTableDirectory(fs, snapshotDir, orig, false);
+    LOG.debug("Finished copying tableinfo.");
+    return null;
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/TablePartiallyOpenException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/TablePartiallyOpenException.java
new file mode 100644
index 0000000..19b5fdb
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/TablePartiallyOpenException.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Thrown if a table should be online/offline but is partial open
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class TablePartiallyOpenException extends IOException {
+  private static final long serialVersionUID = 3571982660065058361L;
+
+  public TablePartiallyOpenException() {
+    super();
+  }
+
+  /**
+   * @param s message
+   */
+  public TablePartiallyOpenException(String s) {
+    super(s);
+  }
+
+  /**
+   * @param tableName Name of table that is partial open
+   */
+  public TablePartiallyOpenException(byte[] tableName) {
+    this(Bytes.toString(tableName));
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/TakeSnapshotUtils.java b/src/main/java/org/apache/hadoop/hbase/snapshot/TakeSnapshotUtils.java
new file mode 100644
index 0000000..de5a32f
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/TakeSnapshotUtils.java
@@ -0,0 +1,324 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener;
+import org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Store;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+
+import com.google.common.collect.HashMultimap;
+import com.google.common.collect.Multimap;
+
+/**
+ * Utilities for useful when taking a snapshot
+ */
+public class TakeSnapshotUtils {
+
+  private static final Log LOG = LogFactory.getLog(TakeSnapshotUtils.class);
+
+  private TakeSnapshotUtils() {
+    // private constructor for util class
+  }
+
+  /**
+   * Get the per-region snapshot description location.
+   * <p>
+   * Under the per-snapshot directory, specific files per-region are kept in a similar layout as per
+   * the current directory layout.
+   * @param desc description of the snapshot
+   * @param rootDir root directory for the hbase installation
+   * @param regionName encoded name of the region (see {@link HRegionInfo#encodeRegionName(byte[])})
+   * @return path to the per-region directory for the snapshot
+   */
+  public static Path getRegionSnapshotDirectory(SnapshotDescription desc, Path rootDir,
+      String regionName) {
+    Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(desc, rootDir);
+    return HRegion.getRegionDir(snapshotDir, regionName);
+  }
+
+  /**
+   * Get the home directory for store-level snapshot files.
+   * <p>
+   * Specific files per store are kept in a similar layout as per the current directory layout.
+   * @param regionDir snapshot directory for the parent region, <b>not</b> the standard region
+   *          directory. See {@link #getRegionSnapshotDirectory(SnapshotDescription, Path, String)}
+   * @param family name of the store to snapshot
+   * @return path to the snapshot home directory for the store/family
+   */
+  public static Path getStoreSnapshotDirectory(Path regionDir, String family) {
+    return Store.getStoreHomedir(regionDir, Bytes.toBytes(family));
+  }
+
+  /**
+   * Get the snapshot directory for each family to be added to the the snapshot
+   * @param snapshot description of the snapshot being take
+   * @param snapshotRegionDir directory in the snapshot where the region directory information
+   *          should be stored
+   * @param families families to be added (can be null)
+   * @return paths to the snapshot directory for each family, in the same order as the families
+   *         passed in
+   */
+  public static List<Path> getFamilySnapshotDirectories(SnapshotDescription snapshot,
+      Path snapshotRegionDir, FileStatus[] families) {
+    if (families == null || families.length == 0) return Collections.emptyList();
+
+    List<Path> familyDirs = new ArrayList<Path>(families.length);
+    for (FileStatus family : families) {
+      // build the reference directory name
+      familyDirs.add(getStoreSnapshotDirectory(snapshotRegionDir, family.getPath().getName()));
+    }
+    return familyDirs;
+  }
+
+  /**
+   * Create a snapshot timer for the master which notifies the monitor when an error occurs
+   * @param snapshot snapshot to monitor
+   * @param conf configuration to use when getting the max snapshot life
+   * @param monitor monitor to notify when the snapshot life expires
+   * @return the timer to use update to signal the start and end of the snapshot
+   */
+  @SuppressWarnings("rawtypes")
+  public static TimeoutExceptionInjector getMasterTimerAndBindToMonitor(SnapshotDescription snapshot,
+      Configuration conf, ForeignExceptionListener monitor) {
+    long maxTime = SnapshotDescriptionUtils.getMaxMasterTimeout(conf, snapshot.getType(),
+      SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME);
+    return new TimeoutExceptionInjector(monitor, maxTime);
+  }
+
+  /**
+   * Verify that all the expected logs got referenced
+   * @param fs filesystem where the logs live
+   * @param logsDir original logs directory
+   * @param serverNames names of the servers that involved in the snapshot
+   * @param snapshot description of the snapshot being taken
+   * @param snapshotLogDir directory for logs in the snapshot
+   * @throws IOException
+   */
+  public static void verifyAllLogsGotReferenced(FileSystem fs, Path logsDir,
+      Set<String> serverNames, SnapshotDescription snapshot, Path snapshotLogDir)
+      throws IOException {
+    assertTrue(snapshot, "Logs directory doesn't exist in snapshot", fs.exists(logsDir));
+    // for each of the server log dirs, make sure it matches the main directory
+    Multimap<String, String> snapshotLogs = getMapOfServersAndLogs(fs, snapshotLogDir, serverNames);
+    Multimap<String, String> realLogs = getMapOfServersAndLogs(fs, logsDir, serverNames);
+    if (realLogs != null) {
+      assertNotNull(snapshot, "No server logs added to snapshot", snapshotLogs);
+    } else if (realLogs == null) {
+      assertNull(snapshot, "Snapshotted server logs that don't exist", snapshotLogs);
+    }
+
+    // check the number of servers
+    Set<Entry<String, Collection<String>>> serverEntries = realLogs.asMap().entrySet();
+    Set<Entry<String, Collection<String>>> snapshotEntries = snapshotLogs.asMap().entrySet();
+    assertEquals(snapshot, "Not the same number of snapshot and original server logs directories",
+      serverEntries.size(), snapshotEntries.size());
+
+    // verify we snapshotted each of the log files
+    for (Entry<String, Collection<String>> serverLogs : serverEntries) {
+      // if the server is not the snapshot, skip checking its logs
+      if (!serverNames.contains(serverLogs.getKey())) continue;
+      Collection<String> snapshotServerLogs = snapshotLogs.get(serverLogs.getKey());
+      assertNotNull(snapshot, "Snapshots missing logs for server:" + serverLogs.getKey(),
+        snapshotServerLogs);
+
+      // check each of the log files
+      assertEquals(snapshot,
+        "Didn't reference all the log files for server:" + serverLogs.getKey(), serverLogs
+            .getValue().size(), snapshotServerLogs.size());
+      for (String log : serverLogs.getValue()) {
+        assertTrue(snapshot, "Snapshot logs didn't include " + log,
+          snapshotServerLogs.contains(log));
+      }
+    }
+  }
+
+  /**
+   * Verify one of a snapshot's region's recovered.edits, has been at the surface (file names,
+   * length), match the original directory.
+   * @param fs filesystem on which the snapshot had been taken
+   * @param rootDir full path to the root hbase directory
+   * @param regionInfo info for the region
+   * @param snapshot description of the snapshot that was taken
+   * @throws IOException if there is an unexpected error talking to the filesystem
+   */
+  public static void verifyRecoveredEdits(FileSystem fs, Path rootDir, HRegionInfo regionInfo,
+      SnapshotDescription snapshot) throws IOException {
+    Path regionDir = HRegion.getRegionDir(rootDir, regionInfo);
+    Path editsDir = HLog.getRegionDirRecoveredEditsDir(regionDir);
+    Path snapshotRegionDir = TakeSnapshotUtils.getRegionSnapshotDirectory(snapshot, rootDir,
+      regionInfo.getEncodedName());
+    Path snapshotEditsDir = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
+
+    FileStatus[] edits = FSUtils.listStatus(fs, editsDir);
+    FileStatus[] snapshotEdits = FSUtils.listStatus(fs, snapshotEditsDir);
+    if (edits == null) {
+      assertNull(snapshot, "Snapshot has edits but table doesn't", snapshotEdits);
+      return;
+    }
+
+    assertNotNull(snapshot, "Table has edits, but snapshot doesn't", snapshotEdits);
+
+    // check each of the files
+    assertEquals(snapshot, "Not same number of edits in snapshot as table", edits.length,
+      snapshotEdits.length);
+
+    // make sure we have a file with the same name as the original
+    // it would be really expensive to verify the content matches the original
+    for (FileStatus edit : edits) {
+      for (FileStatus sEdit : snapshotEdits) {
+        if (sEdit.getPath().equals(edit.getPath())) {
+          assertEquals(snapshot, "Snapshot file" + sEdit.getPath()
+              + " length not equal to the original: " + edit.getPath(), edit.getLen(),
+            sEdit.getLen());
+          break;
+        }
+      }
+      assertTrue(snapshot, "No edit in snapshot with name:" + edit.getPath(), false);
+    }
+  }
+
+  private static void assertNull(SnapshotDescription snapshot, String msg, Object isNull)
+      throws CorruptedSnapshotException {
+    if (isNull != null) {
+      throw new CorruptedSnapshotException(msg + ", Expected " + isNull + " to be null.", snapshot);
+    }
+  }
+
+  private static void assertNotNull(SnapshotDescription snapshot, String msg, Object notNull)
+      throws CorruptedSnapshotException {
+    if (notNull == null) {
+      throw new CorruptedSnapshotException(msg + ", Expected object to not be null, but was null.",
+          snapshot);
+    }
+  }
+
+  private static void assertTrue(SnapshotDescription snapshot, String msg, boolean isTrue)
+      throws CorruptedSnapshotException {
+    if (!isTrue) {
+      throw new CorruptedSnapshotException(msg + ", Expected true, but was false", snapshot);
+    }
+  }
+
+  /**
+   * Assert that the expect matches the gotten amount
+   * @param msg message to add the to exception
+   * @param expected
+   * @param gotten
+   * @throws CorruptedSnapshotException thrown if the two elements don't match
+   */
+  private static void assertEquals(SnapshotDescription snapshot, String msg, int expected,
+      int gotten) throws CorruptedSnapshotException {
+    if (expected != gotten) {
+      throw new CorruptedSnapshotException(msg + ". Expected:" + expected + ", got:" + gotten,
+          snapshot);
+    }
+  }
+
+  /**
+   * Assert that the expect matches the gotten amount
+   * @param msg message to add the to exception
+   * @param expected
+   * @param gotten
+   * @throws CorruptedSnapshotException thrown if the two elements don't match
+   */
+  private static void assertEquals(SnapshotDescription snapshot, String msg, long expected,
+      long gotten) throws CorruptedSnapshotException {
+    if (expected != gotten) {
+      throw new CorruptedSnapshotException(msg + ". Expected:" + expected + ", got:" + gotten,
+          snapshot);
+    }
+  }
+
+  /**
+   * @param logdir
+   * @param toInclude list of servers to include. If empty or null, returns all servers
+   * @return maps of servers to all their log files. If there is no log directory, returns
+   *         <tt>null</tt>
+   */
+  private static Multimap<String, String> getMapOfServersAndLogs(FileSystem fs, Path logdir,
+      Collection<String> toInclude) throws IOException {
+    // create a path filter based on the passed directories to include
+    PathFilter filter = toInclude == null || toInclude.size() == 0 ? null
+        : new MatchesDirectoryNames(toInclude);
+
+    // get all the expected directories
+    FileStatus[] serverLogDirs = FSUtils.listStatus(fs, logdir, filter);
+    if (serverLogDirs == null) return null;
+
+    // map those into a multimap of servername -> [log files]
+    Multimap<String, String> map = HashMultimap.create();
+    for (FileStatus server : serverLogDirs) {
+      FileStatus[] serverLogs = FSUtils.listStatus(fs, server.getPath(), null);
+      if (serverLogs == null) continue;
+      for (FileStatus log : serverLogs) {
+        map.put(server.getPath().getName(), log.getPath().getName());
+      }
+    }
+    return map;
+  }
+
+  /**
+   * Path filter that only accepts paths where that have a {@link Path#getName()} that is contained
+   * in the specified collection.
+   */
+  private static class MatchesDirectoryNames implements PathFilter {
+
+    Collection<String> paths;
+
+    public MatchesDirectoryNames(Collection<String> dirNames) {
+      this.paths = dirNames;
+    }
+
+    @Override
+    public boolean accept(Path path) {
+      return paths.contains(path.getName());
+    }
+  }
+
+  /**
+   * Get the log directory for a specific snapshot
+   * @param snapshotDir directory where the specific snapshot will be store
+   * @param serverName name of the parent regionserver for the log files
+   * @return path to the log home directory for the archive files.
+   */
+  public static Path getSnapshotHLogsDir(Path snapshotDir, String serverName) {
+    return new Path(snapshotDir, HLog.getHLogDirectoryName(serverName));
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/UnexpectedSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/UnexpectedSnapshotException.java
new file mode 100644
index 0000000..856c1cb
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/UnexpectedSnapshotException.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * General exception when an unexpected error occurs while running a snapshot.
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class UnexpectedSnapshotException extends HBaseSnapshotException {
+
+  /**
+   * General exception for some cause
+   * @param msg reason why the snapshot couldn't be completed
+   * @param cause root cause of the failure
+   * @param snapshot description of the snapshot attempted
+   */
+  public UnexpectedSnapshotException(String msg, Exception cause, SnapshotDescription snapshot) {
+    super(msg, cause, snapshot);
+  }
+
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java
new file mode 100644
index 0000000..a6b381f
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+/**
+ * Exception thrown when we get a request for a snapshot we don't recognize.
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class UnknownSnapshotException extends HBaseSnapshotException {
+
+  /**
+   * @param msg full information about the failure
+   */
+  public UnknownSnapshotException(String msg) {
+    super(msg);
+  }
+
+  public UnknownSnapshotException(String msg, Exception  e) {
+    super(msg, e);
+  }
+
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java
deleted file mode 100644
index ce670ed..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-
-/**
- * Exception thrown when the found snapshot info from the filesystem is not valid
- */
-@SuppressWarnings("serial")
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public class CorruptedSnapshotException extends HBaseSnapshotException {
-
-  /**
-   * @param message message describing the exception
-   * @param e cause
-   */
-  public CorruptedSnapshotException(String message, Exception e) {
-    super(message, e);
-  }
-
-  /**
-   * Snapshot was corrupt for some reason
-   * @param message full description of the failure
-   * @param snapshot snapshot that was expected
-   */
-  public CorruptedSnapshotException(String message, SnapshotDescription snapshot) {
-    super(message, snapshot);
-  }
-
-  /**
-   * @param message message describing the exception
-   */
-  public CorruptedSnapshotException(String message) {
-    super(message, (SnapshotDescription)null);
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/ExportSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/ExportSnapshotException.java
deleted file mode 100644
index 6e943d5..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/ExportSnapshotException.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-
-/**
- * Thrown when a snapshot could not be exported due to an error during the operation.
- */
-@InterfaceAudience.Public
-@SuppressWarnings("serial")
-public class ExportSnapshotException extends HBaseSnapshotException {
-
-  /**
-   * @param msg message describing the exception
-   */
-  public ExportSnapshotException(String msg) {
-    super(msg);
-  }
-
-  /**
-   * @param message message describing the exception
-   * @param e cause
-   */
-  public ExportSnapshotException(String message, Exception e) {
-    super(message, e);
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java
deleted file mode 100644
index b597ce7..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.hbase.HBaseIOException;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * General exception base class for when a snapshot fails
- */
-@SuppressWarnings("serial")
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public class HBaseSnapshotException extends HBaseIOException {
-
-  private SnapshotDescription description;
-
-  /**
-   * Some exception happened for a snapshot and don't even know the snapshot that it was about
-   * @param msg Full description of the failure
-   */
-  public HBaseSnapshotException(String msg) {
-    super(msg);
-  }
-
-  /**
-   * Exception for the given snapshot that has no previous root cause
-   * @param msg reason why the snapshot failed
-   * @param desc description of the snapshot that is being failed
-   */
-  public HBaseSnapshotException(String msg, SnapshotDescription desc) {
-    super(msg);
-    this.description = desc;
-  }
-
-  /**
-   * Exception for the given snapshot due to another exception
-   * @param msg reason why the snapshot failed
-   * @param cause root cause of the failure
-   * @param desc description of the snapshot that is being failed
-   */
-  public HBaseSnapshotException(String msg, Throwable cause, SnapshotDescription desc) {
-    super(msg, cause);
-    this.description = desc;
-  }
-
-  /**
-   * Exception when the description of the snapshot cannot be determined, due to some root other
-   * root cause
-   * @param message description of what caused the failure
-   * @param e root cause
-   */
-  public HBaseSnapshotException(String message, Exception e) {
-    super(message, e);
-  }
-
-  public SnapshotDescription getSnapshotDescription() {
-    return this.description;
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/RestoreSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/RestoreSnapshotException.java
deleted file mode 100644
index 08c74c9..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/RestoreSnapshotException.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * Thrown when a snapshot could not be restored due to a server-side error when restoring it.
- */
-@SuppressWarnings("serial")
-public class RestoreSnapshotException extends HBaseSnapshotException {
-  public RestoreSnapshotException(String msg, SnapshotDescription desc) {
-    super(msg, desc);
-  }
-
-  public RestoreSnapshotException(String msg, Throwable cause, SnapshotDescription desc) {
-    super(msg, cause, desc);
-  }
-
-  public RestoreSnapshotException(String msg) {
-    super(msg);
-  }
-
-  public RestoreSnapshotException(String message, Exception e) {
-    super(message, e);
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java
deleted file mode 100644
index 9f25a0c..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * Thrown when a snapshot could not be created due to a server-side error when taking the snapshot.
- */
-@SuppressWarnings("serial")
-public class SnapshotCreationException extends HBaseSnapshotException {
-
-  /**
-   * Used internally by the RPC engine to pass the exception back to the client.
-   * @param msg error message to pass back
-   */
-  public SnapshotCreationException(String msg) {
-    super(msg);
-  }
-
-  /**
-   * Failure to create the specified snapshot
-   * @param msg reason why the snapshot couldn't be completed
-   * @param desc description of the snapshot attempted
-   */
-  public SnapshotCreationException(String msg, SnapshotDescription desc) {
-    super(msg, desc);
-  }
-
-  /**
-   * Failure to create the specified snapshot due to an external cause
-   * @param msg reason why the snapshot couldn't be completed
-   * @param cause root cause of the failure
-   * @param desc description of the snapshot attempted
-   */
-  public SnapshotCreationException(String msg, Throwable cause, SnapshotDescription desc) {
-    super(msg, cause, desc);
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java
deleted file mode 100644
index 90e3d6a..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-
-/**
- * Thrown when the server is looking for a snapshot but can't find the snapshot on the filesystem
- */
-@SuppressWarnings("serial")
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public class SnapshotDoesNotExistException extends HBaseSnapshotException {
-  /**
-   * @param msg full description of the failure
-   */
-  public SnapshotDoesNotExistException(String msg) {
-    super(msg);
-  }
-
-  /**
-   * @param desc expected snapshot to find
-   */
-  public SnapshotDoesNotExistException(SnapshotDescription desc) {
-    super("Snapshot '" + desc.getName() +"' doesn't exist on the filesystem", desc);
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java
deleted file mode 100644
index 1fb7496..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * Thrown when a snapshot exists but should not
- */
-@SuppressWarnings("serial")
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public class SnapshotExistsException extends HBaseSnapshotException {
-
-  /**
-   * Failure due to the snapshot already existing
-   * @param msg full description of the failure
-   * @param desc snapshot that was attempted
-   */
-  public SnapshotExistsException(String msg, SnapshotDescription desc) {
-    super(msg, desc);
-  }
-}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java
deleted file mode 100644
index 580d67e..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import java.io.IOException;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * Thrown if a table should be online/offline but is partial open
- */
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public class TablePartiallyOpenException extends IOException {
-  private static final long serialVersionUID = 3571982660065058361L;
-
-  public TablePartiallyOpenException() {
-    super();
-  }
-
-  /**
-   * @param s message
-   */
-  public TablePartiallyOpenException(String s) {
-    super(s);
-  }
-
-  /**
-   * @param tableName Name of table that is partial open
-   */
-  public TablePartiallyOpenException(byte[] tableName) {
-    this(Bytes.toString(tableName));
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java
deleted file mode 100644
index f729920..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * General exception when an unexpected error occurs while running a snapshot.
- */
-@SuppressWarnings("serial")
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public class UnexpectedSnapshotException extends HBaseSnapshotException {
-
-  /**
-   * General exception for some cause
-   * @param msg reason why the snapshot couldn't be completed
-   * @param cause root cause of the failure
-   * @param snapshot description of the snapshot attempted
-   */
-  public UnexpectedSnapshotException(String msg, Exception cause, SnapshotDescription snapshot) {
-    super(msg, cause, snapshot);
-  }
-
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java
deleted file mode 100644
index 7136cd1..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.exception;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Exception thrown when we get a request for a snapshot we don't recognize.
- */
-@SuppressWarnings("serial")
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public class UnknownSnapshotException extends HBaseSnapshotException {
-
-  /**
-   * @param msg full information about the failure
-   */
-  public UnknownSnapshotException(String msg) {
-    super(msg);
-  }
-
-  public UnknownSnapshotException(String msg, Exception  e) {
-    super(msg, e);
-  }
-
-}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/restore/RestoreSnapshotHelper.java b/src/main/java/org/apache/hadoop/hbase/snapshot/restore/RestoreSnapshotHelper.java
deleted file mode 100644
index d341415..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/restore/RestoreSnapshotHelper.java
+++ /dev/null
@@ -1,425 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase.snapshot.restore;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeMap;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.backup.HFileArchiver;
-import org.apache.hadoop.hbase.catalog.CatalogTracker;
-import org.apache.hadoop.hbase.catalog.MetaEditor;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.io.HFileLink;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.hadoop.hbase.util.FSVisitor;
-import org.apache.hadoop.hbase.util.ModifyRegionUtils;
-
-/**
- * Helper to Restore/Clone a Snapshot
- *
- * <p>The helper assumes that a table is already created, and by calling restore()
- * the content present in the snapshot will be restored as the new content of the table.
- *
- * <p>Clone from Snapshot: If the target table is empty, the restore operation
- * is just a "clone operation", where the only operations are:
- * <ul>
- *  <li>for each region in the snapshot create a new region
- *    (note that the region will have a different name, since the encoding contains the table name)
- *  <li>for each file in the region create a new HFileLink to point to the original file.
- *  <li>restore the logs, if any
- * </ul>
- *
- * <p>Restore from Snapshot:
- * <ul>
- *  <li>for each region in the table verify which are available in the snapshot and which are not
- *    <ul>
- *    <li>if the region is not present in the snapshot, remove it.
- *    <li>if the region is present in the snapshot
- *      <ul>
- *      <li>for each file in the table region verify which are available in the snapshot
- *        <ul>
- *          <li>if the hfile is not present in the snapshot, remove it
- *          <li>if the hfile is present, keep it (nothing to do)
- *        </ul>
- *      <li>for each file in the snapshot region but not in the table
- *        <ul>
- *          <li>create a new HFileLink that point to the original file
- *        </ul>
- *      </ul>
- *    </ul>
- *  <li>for each region in the snapshot not present in the current table state
- *    <ul>
- *    <li>create a new region and for each file in the region create a new HFileLink
- *      (This is the same as the clone operation)
- *    </ul>
- *  <li>restore the logs, if any
- * </ul>
- */
-@InterfaceAudience.Private
-public class RestoreSnapshotHelper {
-	private static final Log LOG = LogFactory.getLog(RestoreSnapshotHelper.class);
-
-  private final Map<byte[], byte[]> regionsMap =
-        new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
-
-  private final ForeignExceptionDispatcher monitor;
-
-  private final SnapshotDescription snapshotDesc;
-  private final Path snapshotDir;
-
-  private final HTableDescriptor tableDesc;
-  private final Path tableDir;
-
-  private final CatalogTracker catalogTracker;
-  private final Configuration conf;
-  private final FileSystem fs;
-
-  public RestoreSnapshotHelper(final Configuration conf, final FileSystem fs,
-      final CatalogTracker catalogTracker,
-      final SnapshotDescription snapshotDescription, final Path snapshotDir,
-      final HTableDescriptor tableDescriptor, final Path tableDir,
-      final ForeignExceptionDispatcher monitor)
-  {
-    this.fs = fs;
-    this.conf = conf;
-    this.catalogTracker = catalogTracker;
-    this.snapshotDesc = snapshotDescription;
-    this.snapshotDir = snapshotDir;
-    this.tableDesc = tableDescriptor;
-    this.tableDir = tableDir;
-    this.monitor = monitor;
-  }
-
-  /**
-   * Restore table to a specified snapshot state.
-   */
-  public void restore() throws IOException {
-    long startTime = EnvironmentEdgeManager.currentTimeMillis();
-
-    LOG.debug("starting restore");
-    Set<String> snapshotRegionNames = SnapshotReferenceUtil.getSnapshotRegionNames(fs, snapshotDir);
-    if (snapshotRegionNames == null) {
-      LOG.warn("Nothing to restore. Snapshot " + snapshotDesc + " looks empty");
-      return;
-    }
-
-    // Identify which region are still available and which not.
-    // NOTE: we rely upon the region name as: "table name, start key, end key"
-    List<HRegionInfo> tableRegions = getTableRegions();
-    if (tableRegions != null) {
-      monitor.rethrowException();
-      List<HRegionInfo> regionsToRestore = new LinkedList<HRegionInfo>();
-      List<HRegionInfo> regionsToRemove = new LinkedList<HRegionInfo>();
-
-      for (HRegionInfo regionInfo: tableRegions) {
-        String regionName = regionInfo.getEncodedName();
-        if (snapshotRegionNames.contains(regionName)) {
-          LOG.info("region to restore: " + regionName);
-          snapshotRegionNames.remove(regionInfo);
-          regionsToRestore.add(regionInfo);
-        } else {
-          LOG.info("region to remove: " + regionName);
-          regionsToRemove.add(regionInfo);
-        }
-      }
-
-      // Restore regions using the snapshot data
-      monitor.rethrowException();
-      restoreRegions(regionsToRestore);
-
-      // Remove regions from the current table
-      monitor.rethrowException();
-      ModifyRegionUtils.deleteRegions(fs, catalogTracker, regionsToRemove);
-    }
-
-    // Regions to Add: present in the snapshot but not in the current table
-    if (snapshotRegionNames.size() > 0) {
-      List<HRegionInfo> regionsToAdd = new LinkedList<HRegionInfo>();
-
-      monitor.rethrowException();
-      for (String regionName: snapshotRegionNames) {
-        LOG.info("region to add: " + regionName);
-        Path regionDir = new Path(snapshotDir, regionName);
-        regionsToAdd.add(HRegion.loadDotRegionInfoFileContent(fs, regionDir));
-      }
-
-      // Create new regions cloning from the snapshot
-      monitor.rethrowException();
-      cloneRegions(regionsToAdd);
-    }
-
-    // Restore WALs
-    monitor.rethrowException();
-    restoreWALs();
-  }
-
-  /**
-   * Restore specified regions by restoring content to the snapshot state.
-   */
-  private void restoreRegions(final List<HRegionInfo> regions) throws IOException {
-    if (regions == null || regions.size() == 0) return;
-    for (HRegionInfo hri: regions) restoreRegion(hri);
-  }
-
-  /**
-   * Restore region by removing files not it in the snapshot
-   * and adding the missing ones from the snapshot.
-   */
-  private void restoreRegion(HRegionInfo regionInfo) throws IOException {
-    Path snapshotRegionDir = new Path(snapshotDir, regionInfo.getEncodedName());
-    Map<String, List<String>> snapshotFiles =
-                SnapshotReferenceUtil.getRegionHFileReferences(fs, snapshotRegionDir);
-
-    Path regionDir = new Path(tableDir, regionInfo.getEncodedName());
-    String tableName = tableDesc.getNameAsString();
-
-    for (Map.Entry<String, List<String>> familyEntry: snapshotFiles.entrySet()) {
-      byte[] family = Bytes.toBytes(familyEntry.getKey());
-      Path familyDir = new Path(regionDir, familyEntry.getKey());
-      Set<String> familyFiles = getTableRegionFamilyFiles(familyDir);
-
-      List<String> hfilesToAdd = new LinkedList<String>();
-      for (String hfileName: familyEntry.getValue()) {
-        if (familyFiles.contains(hfileName)) {
-          // HFile already present
-          familyFiles.remove(hfileName);
-        } else {
-          // HFile missing
-          hfilesToAdd.add(hfileName);
-        }
-      }
-
-      // Remove hfiles not present in the snapshot
-      for (String hfileName: familyFiles) {
-        Path hfile = new Path(familyDir, hfileName);
-        LOG.trace("Removing hfile=" + hfile + " from table=" + tableName);
-        HFileArchiver.archiveStoreFile(fs, regionInfo, conf, tableDir, family, hfile);
-      }
-
-      // Restore Missing files
-      for (String hfileName: hfilesToAdd) {
-        LOG.trace("Adding HFileLink " + hfileName + " to table=" + tableName);
-        restoreStoreFile(familyDir, regionInfo, hfileName);
-      }
-    }
-  }
-
-  /**
-   * @return The set of files in the specified family directory.
-   */
-  private Set<String> getTableRegionFamilyFiles(final Path familyDir) throws IOException {
-    Set<String> familyFiles = new HashSet<String>();
-
-    FileStatus[] hfiles = FSUtils.listStatus(fs, familyDir);
-    if (hfiles == null) return familyFiles;
-
-    for (FileStatus hfileRef: hfiles) {
-      String hfileName = hfileRef.getPath().getName();
-      familyFiles.add(hfileName);
-    }
-
-    return familyFiles;
-  }
-
-  /**
-   * Clone specified regions. For each region create a new region
-   * and create a HFileLink for each hfile.
-   */
-  private void cloneRegions(final List<HRegionInfo> regions) throws IOException {
-    if (regions == null || regions.size() == 0) return;
-
-    final Map<String, HRegionInfo> snapshotRegions =
-      new HashMap<String, HRegionInfo>(regions.size());
-
-    // clone region info (change embedded tableName with the new one)
-    HRegionInfo[] clonedRegionsInfo = new HRegionInfo[regions.size()];
-    for (int i = 0; i < clonedRegionsInfo.length; ++i) {
-      // clone the region info from the snapshot region info
-      HRegionInfo snapshotRegionInfo = regions.get(i);
-      clonedRegionsInfo[i] = cloneRegionInfo(snapshotRegionInfo);
-
-      // add the region name mapping between snapshot and cloned
-      String snapshotRegionName = snapshotRegionInfo.getEncodedName();
-      String clonedRegionName = clonedRegionsInfo[i].getEncodedName();
-      regionsMap.put(Bytes.toBytes(snapshotRegionName), Bytes.toBytes(clonedRegionName));
-      LOG.info("clone region=" + snapshotRegionName + " as " + clonedRegionName);
-
-      // Add mapping between cloned region name and snapshot region info
-      snapshotRegions.put(clonedRegionName, snapshotRegionInfo);
-    }
-
-    // create the regions on disk
-    List<HRegionInfo> clonedRegions = ModifyRegionUtils.createRegions(conf, FSUtils.getRootDir(conf),
-      tableDesc, clonedRegionsInfo, catalogTracker, new ModifyRegionUtils.RegionFillTask() {
-        public void fillRegion(final HRegion region) throws IOException {
-          cloneRegion(region, snapshotRegions.get(region.getRegionInfo().getEncodedName()));
-        }
-      });
-    if (regions != null && regions.size() > 0) {
-      // add regions to .META.
-      MetaEditor.addRegionsToMeta(catalogTracker, clonedRegions);
-    }
-  }
-
-  /**
-   * Clone region directory content from the snapshot info.
-   *
-   * Each region is encoded with the table name, so the cloned region will have
-   * a different region name.
-   *
-   * Instead of copying the hfiles a HFileLink is created.
-   *
-   * @param region {@link HRegion} cloned
-   * @param snapshotRegionInfo
-   */
-  private void cloneRegion(final HRegion region, final HRegionInfo snapshotRegionInfo)
-      throws IOException {
-    final Path snapshotRegionDir = new Path(snapshotDir, snapshotRegionInfo.getEncodedName());
-    final Path regionDir = new Path(tableDir, region.getRegionInfo().getEncodedName());
-    final String tableName = tableDesc.getNameAsString();
-    SnapshotReferenceUtil.visitRegionStoreFiles(fs, snapshotRegionDir,
-      new FSVisitor.StoreFileVisitor() {
-        public void storeFile (final String region, final String family, final String hfile)
-            throws IOException {
-          LOG.info("Adding HFileLink " + hfile + " to table=" + tableName);
-          Path familyDir = new Path(regionDir, family);
-          restoreStoreFile(familyDir, snapshotRegionInfo, hfile);
-        }
-    });
-  }
-
-  /**
-   * Create a new {@link HFileLink} to reference the store file.
-   *
-   * @param familyDir destination directory for the store file
-   * @param regionInfo destination region info for the table
-   * @param hfileName store file name (can be a Reference, HFileLink or simple HFile)
-   */
-  private void restoreStoreFile(final Path familyDir, final HRegionInfo regionInfo,
-      final String hfileName) throws IOException {
-    if (HFileLink.isHFileLink(hfileName)) {
-      HFileLink.createFromHFileLink(conf, fs, familyDir, hfileName);
-    } else {
-      HFileLink.create(conf, fs, familyDir, regionInfo, hfileName);
-    }
-  }
-
-  /**
-   * Create a new {@link HRegionInfo} from the snapshot region info.
-   * Keep the same startKey, endKey, regionId and split information but change
-   * the table name.
-   *
-   * @param snapshotRegionInfo Info for region to clone.
-   * @return the new HRegion instance
-   */
-  public HRegionInfo cloneRegionInfo(final HRegionInfo snapshotRegionInfo) {
-    return new HRegionInfo(tableDesc.getName(),
-                      snapshotRegionInfo.getStartKey(), snapshotRegionInfo.getEndKey(),
-                      snapshotRegionInfo.isSplit(), snapshotRegionInfo.getRegionId());
-  }
-
-  /**
-   * Restore snapshot WALs.
-   *
-   * Global Snapshot keep a reference to region servers logs present during the snapshot.
-   * (/hbase/.snapshot/snapshotName/.logs/hostName/logName)
-   *
-   * Since each log contains different tables data, logs must be split to
-   * extract the table that we are interested in.
-   */
-  private void restoreWALs() throws IOException {
-    final SnapshotLogSplitter logSplitter = new SnapshotLogSplitter(conf, fs, tableDir,
-                                Bytes.toBytes(snapshotDesc.getTable()), regionsMap);
-    try {
-      // Recover.Edits
-      SnapshotReferenceUtil.visitRecoveredEdits(fs, snapshotDir,
-          new FSVisitor.RecoveredEditsVisitor() {
-        public void recoveredEdits (final String region, final String logfile) throws IOException {
-          Path path = SnapshotReferenceUtil.getRecoveredEdits(snapshotDir, region, logfile);
-          logSplitter.splitRecoveredEdit(path);
-        }
-      });
-
-      // Region Server Logs
-      SnapshotReferenceUtil.visitLogFiles(fs, snapshotDir, new FSVisitor.LogFileVisitor() {
-        public void logFile (final String server, final String logfile) throws IOException {
-          logSplitter.splitLog(server, logfile);
-        }
-      });
-    } finally {
-      logSplitter.close();
-    }
-  }
-
-  /**
-   * @return the set of the regions contained in the table
-   */
-  private List<HRegionInfo> getTableRegions() throws IOException {
-    LOG.debug("get table regions: " + tableDir);
-    FileStatus[] regionDirs = FSUtils.listStatus(fs, tableDir, new FSUtils.RegionDirFilter(fs));
-    if (regionDirs == null) return null;
-
-    List<HRegionInfo> regions = new LinkedList<HRegionInfo>();
-    for (FileStatus regionDir: regionDirs) {
-      HRegionInfo hri = HRegion.loadDotRegionInfoFileContent(fs, regionDir.getPath());
-      regions.add(hri);
-    }
-    LOG.debug("found " + regions.size() + " regions for table=" + tableDesc.getNameAsString());
-    return regions;
-  }
-
-  /**
-   * Create a new table descriptor cloning the snapshot table schema.
-   *
-   * @param admin
-   * @param snapshotTableDescriptor
-   * @param tableName
-   * @return cloned table descriptor
-   * @throws IOException
-   */
-  public static HTableDescriptor cloneTableSchema(final HTableDescriptor snapshotTableDescriptor,
-      final byte[] tableName) throws IOException {
-    HTableDescriptor htd = new HTableDescriptor(tableName);
-    for (HColumnDescriptor hcd: snapshotTableDescriptor.getColumnFamilies()) {
-      htd.addFamily(hcd);
-    }
-    return htd;
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/restore/SnapshotLogSplitter.java b/src/main/java/org/apache/hadoop/hbase/snapshot/restore/SnapshotLogSplitter.java
deleted file mode 100644
index d9b5792..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/restore/SnapshotLogSplitter.java
+++ /dev/null
@@ -1,200 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase.snapshot.restore;
-
-import java.io.Closeable;
-import java.io.EOFException;
-import java.io.IOException;
-import java.util.TreeMap;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.io.HLogLink;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.wal.HLog;
-import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
-import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * If the snapshot has references to one or more log files,
- * those must be split (each log contains multiple tables and regions)
- * and must be placed in the region/recovered.edits folder.
- * (recovered.edits files will be played on region startup)
- *
- * In case of Restore: the log can just be split in the recovered.edits folder.
- * In case of Clone: each entry in the log must be modified to use the new region name.
- * (region names are encoded with: tableName, startKey, regionIdTimeStamp)
- *
- * We can't use the normal split code, because the HLog contains the
- * table name and the region name, and in case of "clone from snapshot"
- * region name and table name will be different and must be replaced in
- * the recovered.edits.
- */
-@InterfaceAudience.Private
-class SnapshotLogSplitter implements Closeable {
-  static final Log LOG = LogFactory.getLog(SnapshotLogSplitter.class);
-
-  private final class LogWriter implements Closeable {
-    private HLog.Writer writer;
-    private Path logFile;
-    private long seqId;
-
-    public LogWriter(final Configuration conf, final FileSystem fs,
-        final Path logDir, long seqId) throws IOException {
-      logFile = new Path(logDir, logFileName(seqId, true));
-      this.writer = HLog.createWriter(fs, logFile, conf);
-      this.seqId = seqId;
-    }
-
-    public void close() throws IOException {
-      writer.close();
-
-      Path finalFile = new Path(logFile.getParent(), logFileName(seqId, false));
-      LOG.debug("LogWriter tmpLogFile=" + logFile + " -> logFile=" + finalFile);
-      fs.rename(logFile, finalFile);
-    }
-
-    public void append(final HLog.Entry entry) throws IOException {
-      writer.append(entry);
-      if (seqId < entry.getKey().getLogSeqNum()) {
-        seqId = entry.getKey().getLogSeqNum();
-      }
-    }
-
-    private String logFileName(long seqId, boolean temp) {
-      String fileName = String.format("%019d", seqId);
-      if (temp) fileName += HLog.RECOVERED_LOG_TMPFILE_SUFFIX;
-      return fileName;
-    }
-  }
-
-  private final Map<byte[], LogWriter> regionLogWriters =
-      new TreeMap<byte[], LogWriter>(Bytes.BYTES_COMPARATOR);
-
-  private final Map<byte[], byte[]> regionsMap;
-  private final Configuration conf;
-  private final byte[] snapshotTableName;
-  private final byte[] tableName;
-  private final Path tableDir;
-  private final FileSystem fs;
-
-  /**
-   * @params tableName snapshot table name
-   * @params regionsMap maps original region names to the new ones.
-   */
-  public SnapshotLogSplitter(final Configuration conf, final FileSystem fs,
-      final Path tableDir, final byte[] snapshotTableName,
-      final Map<byte[], byte[]> regionsMap) {
-    this.regionsMap = regionsMap;
-    this.snapshotTableName = snapshotTableName;
-    this.tableName = Bytes.toBytes(tableDir.getName());
-    this.tableDir = tableDir;
-    this.conf = conf;
-    this.fs = fs;
-  }
-
-  public void close() throws IOException {
-    for (LogWriter writer: regionLogWriters.values()) {
-      writer.close();
-    }
-  }
-
-  public void splitLog(final String serverName, final String logfile) throws IOException {
-    LOG.debug("Restore log=" + logfile + " server=" + serverName +
-              " for snapshotTable=" + Bytes.toString(snapshotTableName) +
-              " to table=" + Bytes.toString(tableName));
-    splitLog(new HLogLink(conf, serverName, logfile).getAvailablePath(fs));
-  }
-
-  public void splitRecoveredEdit(final Path editPath) throws IOException {
-    LOG.debug("Restore recover.edits=" + editPath +
-              " for snapshotTable=" + Bytes.toString(snapshotTableName) +
-              " to table=" + Bytes.toString(tableName));
-    splitLog(editPath);
-  }
-
-  /**
-   * Split the snapshot HLog reference into regions recovered.edits.
-   *
-   * The HLog contains the table name and the region name,
-   * and they must be changed to the restored table names.
-   *
-   * @param logPath Snapshot HLog reference path
-   */
-  public void splitLog(final Path logPath) throws IOException {
-    HLog.Reader log = HLog.getReader(fs, logPath, conf);
-    try {
-      HLog.Entry entry;
-      LogWriter writer = null;
-      byte[] regionName = null;
-      byte[] newRegionName = null;
-      while ((entry = log.next()) != null) {
-        HLogKey key = entry.getKey();
-
-        // We're interested only in the snapshot table that we're restoring
-        if (!Bytes.equals(key.getTablename(), snapshotTableName)) continue;
-
-        // Writer for region.
-        if (!Bytes.equals(regionName, key.getEncodedRegionName())) {
-          regionName = key.getEncodedRegionName().clone();
-
-          // Get the new region name in case of clone, or use the original one
-          newRegionName = regionsMap.get(regionName);
-          if (newRegionName == null) newRegionName = regionName;
-
-          writer = getOrCreateWriter(newRegionName, key.getLogSeqNum());
-          LOG.debug("+ regionName=" + Bytes.toString(regionName));
-        }
-
-        // Append Entry
-        key = new HLogKey(newRegionName, tableName,
-                          key.getLogSeqNum(), key.getWriteTime(), key.getClusterId());
-        writer.append(new HLog.Entry(key, entry.getEdit()));
-      }
-    } catch (IOException e) {
-      LOG.warn("Something wrong during the log split", e);
-    } finally {
-      log.close();
-    }
-  }
-
-  /**
-   * Create a LogWriter for specified region if not already created.
-   */
-  private LogWriter getOrCreateWriter(final byte[] regionName, long seqId) throws IOException {
-    LogWriter writer = regionLogWriters.get(regionName);
-    if (writer == null) {
-      Path regionDir = HRegion.getRegionDir(tableDir, Bytes.toString(regionName));
-      Path dir = HLog.getRegionDirRecoveredEditsDir(regionDir);
-      fs.mkdirs(dir);
-
-      writer = new LogWriter(conf, fs, dir, seqId);
-      regionLogWriters.put(regionName, writer);
-    }
-    return(writer);
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/tool/ExportSnapshot.java b/src/main/java/org/apache/hadoop/hbase/snapshot/tool/ExportSnapshot.java
deleted file mode 100644
index 1037a6d..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/tool/ExportSnapshot.java
+++ /dev/null
@@ -1,702 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase.snapshot.tool;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.LinkedList;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileChecksum;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.io.HFileLink;
-import org.apache.hadoop.hbase.io.HLogLink;
-import org.apache.hadoop.hbase.io.Reference;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
-import org.apache.hadoop.hbase.snapshot.exception.ExportSnapshotException;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.hadoop.hbase.util.Pair;
-
-/**
- * Export the specified snapshot to a given FileSystem.
- *
- * The .snapshot/name folder is copied to the destination cluster
- * and then all the hfiles/hlogs are copied using a Map-Reduce Job in the .archive/ location.
- * When everything is done, the second cluster can restore the snapshot.
- */
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public final class ExportSnapshot extends Configured implements Tool {
-  private static final Log LOG = LogFactory.getLog(ExportSnapshot.class);
-
-  private static final String CONF_TMP_DIR = "hbase.tmp.dir";
-  private static final String CONF_FILES_USER = "snapshot.export.files.attributes.user";
-  private static final String CONF_FILES_GROUP = "snapshot.export.files.attributes.group";
-  private static final String CONF_FILES_MODE = "snapshot.export.files.attributes.mode";
-  private static final String CONF_CHECKSUM_VERIFY = "snapshot.export.checksum.verify";
-  private static final String CONF_OUTPUT_ROOT = "snapshot.export.output.root";
-  private static final String CONF_INPUT_ROOT = "snapshot.export.input.root";
-
-  private static final String INPUT_FOLDER_PREFIX = "export-files.";
-
-  // Export Map-Reduce Counters, to keep track of the progress
-  public enum Counter { MISSING_FILES, COPY_FAILED, BYTES_EXPECTED, BYTES_COPIED };
-
-  private static class ExportMapper extends Mapper<Text, NullWritable, NullWritable, NullWritable> {
-    final static int REPORT_SIZE = 1 * 1024 * 1024;
-    final static int BUFFER_SIZE = 64 * 1024;
-
-    private boolean verifyChecksum;
-    private String filesGroup;
-    private String filesUser;
-    private short filesMode;
-
-    private FileSystem outputFs;
-    private Path outputArchive;
-    private Path outputRoot;
-
-    private FileSystem inputFs;
-    private Path inputArchive;
-    private Path inputRoot;
-
-    @Override
-    public void setup(Context context) {
-      Configuration conf = context.getConfiguration();
-      verifyChecksum = conf.getBoolean(CONF_CHECKSUM_VERIFY, true);
-
-      filesGroup = conf.get(CONF_FILES_GROUP);
-      filesUser = conf.get(CONF_FILES_USER);
-      filesMode = (short)conf.getInt(CONF_FILES_MODE, 0);
-      outputRoot = new Path(conf.get(CONF_OUTPUT_ROOT));
-      inputRoot = new Path(conf.get(CONF_INPUT_ROOT));
-
-      inputArchive = new Path(inputRoot, HConstants.HFILE_ARCHIVE_DIRECTORY);
-      outputArchive = new Path(outputRoot, HConstants.HFILE_ARCHIVE_DIRECTORY);
-
-      try {
-        inputFs = FileSystem.get(inputRoot.toUri(), conf);
-      } catch (IOException e) {
-        throw new RuntimeException("Could not get the input FileSystem with root=" + inputRoot, e);
-      }
-
-      try {
-        outputFs = FileSystem.get(outputRoot.toUri(), conf);
-      } catch (IOException e) {
-        throw new RuntimeException("Could not get the output FileSystem with root="+ outputRoot, e);
-      }
-    }
-
-    @Override
-    public void map(Text key, NullWritable value, Context context)
-        throws InterruptedException, IOException {
-      Path inputPath = new Path(key.toString());
-      Path outputPath = getOutputPath(inputPath);
-
-      LOG.info("copy file input=" + inputPath + " output=" + outputPath);
-      if (copyFile(context, inputPath, outputPath)) {
-        LOG.info("copy completed for input=" + inputPath + " output=" + outputPath);
-      }
-    }
-
-    /**
-     * Returns the location where the inputPath will be copied.
-     *  - hfiles are encoded as hfile links hfile-region-table
-     *  - logs are encoded as serverName/logName
-     */
-    private Path getOutputPath(final Path inputPath) throws IOException {
-      Path path;
-      if (HFileLink.isHFileLink(inputPath)) {
-        String family = inputPath.getParent().getName();
-        String table = HFileLink.getReferencedTableName(inputPath.getName());
-        String region = HFileLink.getReferencedRegionName(inputPath.getName());
-        String hfile = HFileLink.getReferencedHFileName(inputPath.getName());
-        path = new Path(table, new Path(region, new Path(family, hfile)));
-      } else if (isHLogLinkPath(inputPath)) {
-        String logName = inputPath.getName();
-        path = new Path(new Path(outputRoot, HConstants.HREGION_OLDLOGDIR_NAME), logName);
-      } else {
-        path = inputPath;
-      }
-      return new Path(outputArchive, path);
-    }
-
-    private boolean copyFile(final Context context, final Path inputPath, final Path outputPath)
-        throws IOException {
-      FSDataInputStream in = openSourceFile(inputPath);
-      if (in == null) {
-        context.getCounter(Counter.MISSING_FILES).increment(1);
-        return false;
-      }
-
-      try {
-        // Verify if the input file exists
-        FileStatus inputStat = getFileStatus(inputFs, inputPath);
-        if (inputStat == null) return false;
-
-        // Verify if the output file exists and is the same that we want to copy
-        FileStatus outputStat = getFileStatus(outputFs, outputPath);
-        if (outputStat != null && sameFile(inputStat, outputStat)) {
-          LOG.info("Skip copy " + inputPath + " to " + outputPath + ", same file.");
-          return true;
-        }
-
-        context.getCounter(Counter.BYTES_EXPECTED).increment(inputStat.getLen());
-
-        // Ensure that the output folder is there and copy the file
-        outputFs.mkdirs(outputPath.getParent());
-        FSDataOutputStream out = outputFs.create(outputPath, true);
-        try {
-          if (!copyData(context, inputPath, in, outputPath, out, inputStat.getLen()))
-            return false;
-        } finally {
-          out.close();
-        }
-
-        // Preserve attributes
-        return preserveAttributes(outputPath, inputStat);
-      } finally {
-        in.close();
-      }
-    }
-
-    /**
-     * Preserve the files attribute selected by the user copying them from the source file
-     */
-    private boolean preserveAttributes(final Path path, final FileStatus refStat) {
-      FileStatus stat;
-      try {
-        stat = outputFs.getFileStatus(path);
-      } catch (IOException e) {
-        LOG.warn("Unable to get the status for file=" + path);
-        return false;
-      }
-
-      try {
-        if (filesMode > 0 && stat.getPermission().toShort() != filesMode) {
-          outputFs.setPermission(path, new FsPermission(filesMode));
-        } else if (!stat.getPermission().equals(refStat.getPermission())) {
-          outputFs.setPermission(path, refStat.getPermission());
-        }
-      } catch (IOException e) {
-        LOG.error("Unable to set the permission for file=" + path, e);
-        return false;
-      }
-
-      try {
-        String user = (filesUser != null) ? filesUser : refStat.getOwner();
-        String group = (filesGroup != null) ? filesGroup : refStat.getGroup();
-        if (!(user.equals(stat.getOwner()) && group.equals(stat.getGroup()))) {
-          outputFs.setOwner(path, user, group);
-        }
-      } catch (Exception e) {
-        LOG.error("Unable to set the owner/group for file=" + path, e);
-        return false;
-      }
-
-      return true;
-    }
-
-    private boolean copyData(final Context context,
-        final Path inputPath, final FSDataInputStream in,
-        final Path outputPath, final FSDataOutputStream out,
-        final long inputFileSize) {
-      final String statusMessage = "copied %s/" + StringUtils.humanReadableInt(inputFileSize) +
-                                   " (%.3f%%) from " + inputPath + " to " + outputPath;
-
-      try {
-        byte[] buffer = new byte[BUFFER_SIZE];
-        long totalBytesWritten = 0;
-        int reportBytes = 0;
-        int bytesRead;
-
-        while ((bytesRead = in.read(buffer)) > 0) {
-          out.write(buffer, 0, bytesRead);
-          totalBytesWritten += bytesRead;
-          reportBytes += bytesRead;
-
-          if (reportBytes >= REPORT_SIZE) {
-            context.getCounter(Counter.BYTES_COPIED).increment(reportBytes);
-            context.setStatus(String.format(statusMessage,
-                              StringUtils.humanReadableInt(totalBytesWritten),
-                              reportBytes/(float)inputFileSize));
-            reportBytes = 0;
-          }
-        }
-
-        context.getCounter(Counter.BYTES_COPIED).increment(reportBytes);
-        context.setStatus(String.format(statusMessage,
-                          StringUtils.humanReadableInt(totalBytesWritten),
-                          reportBytes/(float)inputFileSize));
-
-        // Verify that the written size match
-        if (totalBytesWritten != inputFileSize) {
-          LOG.error("number of bytes copied not matching copied=" + totalBytesWritten +
-                    " expected=" + inputFileSize + " for file=" + inputPath);
-          context.getCounter(Counter.COPY_FAILED).increment(1);
-          return false;
-        }
-
-        return true;
-      } catch (IOException e) {
-        LOG.error("Error copying " + inputPath + " to " + outputPath, e);
-        context.getCounter(Counter.COPY_FAILED).increment(1);
-        return false;
-      }
-    }
-
-    private FSDataInputStream openSourceFile(final Path path) {
-      try {
-        if (HFileLink.isHFileLink(path)) {
-          return new HFileLink(inputRoot, inputArchive, path).open(inputFs);
-        } else if (isHLogLinkPath(path)) {
-          String serverName = path.getParent().getName();
-          String logName = path.getName();
-          return new HLogLink(inputRoot, serverName, logName).open(inputFs);
-        }
-        return inputFs.open(path);
-      } catch (IOException e) {
-        LOG.error("Unable to open source file=" + path, e);
-        return null;
-      }
-    }
-
-    private FileStatus getFileStatus(final FileSystem fs, final Path path) {
-      try {
-        if (HFileLink.isHFileLink(path)) {
-          Path refPath = HFileLink.getReferencedPath(fs, inputRoot, inputArchive, path);
-          return fs.getFileStatus(refPath);
-        } else if (isHLogLinkPath(path)) {
-          String serverName = path.getParent().getName();
-          String logName = path.getName();
-          return new HLogLink(inputRoot, serverName, logName).getFileStatus(fs);
-        }
-        return fs.getFileStatus(path);
-      } catch (IOException e) {
-        LOG.warn("Unable to get the status for file=" + path);
-        return null;
-      }
-    }
-
-    private FileChecksum getFileChecksum(final FileSystem fs, final Path path) {
-      try {
-        return fs.getFileChecksum(path);
-      } catch (IOException e) {
-        LOG.warn("Unable to get checksum for file=" + path, e);
-        return null;
-      }
-    }
-
-    /**
-     * Check if the two files are equal by looking at the file length,
-     * and at the checksum (if user has specified the verifyChecksum flag).
-     */
-    private boolean sameFile(final FileStatus inputStat, final FileStatus outputStat) {
-      // Not matching length
-      if (inputStat.getLen() != outputStat.getLen()) return false;
-
-      // Mark files as equals, since user asked for no checksum verification
-      if (!verifyChecksum) return true;
-
-      // If checksums are not available, files are not the same.
-      FileChecksum inChecksum = getFileChecksum(inputFs, inputStat.getPath());
-      if (inChecksum == null) return false;
-
-      FileChecksum outChecksum = getFileChecksum(outputFs, outputStat.getPath());
-      if (outChecksum == null) return false;
-
-      return inChecksum.equals(outChecksum);
-    }
-
-    /**
-     * HLog files are encoded as serverName/logName
-     * and since all the other files should be in /hbase/table/..path..
-     * we can rely on the depth, for now.
-     */
-    private static boolean isHLogLinkPath(final Path path) {
-      return path.depth() == 2;
-    }
-  }
-
-  /**
-   * Extract the list of files (HFiles/HLogs) to copy using Map-Reduce.
-   * @return list of files referenced by the snapshot (pair of path and size)
-   */
-  private List<Pair<Path, Long>> getSnapshotFiles(final FileSystem fs, final Path snapshotDir) throws IOException {
-    SnapshotDescription snapshotDesc = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
-
-    final List<Pair<Path, Long>> files = new ArrayList<Pair<Path, Long>>();
-    final String table = snapshotDesc.getTable();
-    final Configuration conf = getConf();
-
-    // Get snapshot files
-    SnapshotReferenceUtil.visitReferencedFiles(fs, snapshotDir,
-      new SnapshotReferenceUtil.FileVisitor() {
-        public void storeFile (final String region, final String family, final String hfile)
-            throws IOException {
-          Path path = new Path(family, HFileLink.createHFileLinkName(table, region, hfile));
-          long size = fs.getFileStatus(HFileLink.getReferencedPath(conf, fs, path)).getLen();
-          files.add(new Pair<Path, Long>(path, size));
-        }
-
-        public void recoveredEdits (final String region, final String logfile)
-            throws IOException {
-          // copied with the snapshot referenecs
-        }
-
-        public void logFile (final String server, final String logfile)
-            throws IOException {
-          long size = new HLogLink(conf, server, logfile).getFileStatus(fs).getLen();
-          files.add(new Pair<Path, Long>(new Path(server, logfile), size));
-        }
-    });
-
-    return files;
-  }
-
-  /**
-   * Given a list of file paths and sizes, create around ngroups in as balanced a way as possible.
-   * The groups created will have similar amounts of bytes.
-   * <p>
-   * The algorithm used is pretty straightforward; the file list is sorted by size,
-   * and then each group fetch the bigger file available, iterating through groups
-   * alternating the direction.
-   */
-  static List<List<Path>> getBalancedSplits(final List<Pair<Path, Long>> files, int ngroups) {
-    // Sort files by size, from small to big
-    Collections.sort(files, new Comparator<Pair<Path, Long>>() {
-      public int compare(Pair<Path, Long> a, Pair<Path, Long> b) {
-        long r = a.getSecond() - b.getSecond();
-        return (r < 0) ? -1 : ((r > 0) ? 1 : 0);
-      }
-    });
-
-    // create balanced groups
-    List<List<Path>> fileGroups = new LinkedList<List<Path>>();
-    long[] sizeGroups = new long[ngroups];
-    int hi = files.size() - 1;
-    int lo = 0;
-
-    List<Path> group;
-    int dir = 1;
-    int g = 0;
-
-    while (hi >= lo) {
-      if (g == fileGroups.size()) {
-        group = new LinkedList<Path>();
-        fileGroups.add(group);
-      } else {
-        group = fileGroups.get(g);
-      }
-
-      Pair<Path, Long> fileInfo = files.get(hi--);
-
-      // add the hi one
-      sizeGroups[g] += fileInfo.getSecond();
-      group.add(fileInfo.getFirst());
-
-      // change direction when at the end or the beginning
-      g += dir;
-      if (g == ngroups) {
-        dir = -1;
-        g = ngroups - 1;
-      } else if (g < 0) {
-        dir = 1;
-        g = 0;
-      }
-    }
-
-    if (LOG.isDebugEnabled()) {
-      for (int i = 0; i < sizeGroups.length; ++i) {
-        LOG.debug("export split=" + i + " size=" + StringUtils.humanReadableInt(sizeGroups[i]));
-      }
-    }
-
-    return fileGroups;
-  }
-
-  private static Path getInputFolderPath(final FileSystem fs, final Configuration conf)
-      throws IOException, InterruptedException {
-    String stagingName = "exportSnapshot-" + EnvironmentEdgeManager.currentTimeMillis();
-    Path stagingDir = new Path(conf.get(CONF_TMP_DIR), stagingName);
-    fs.mkdirs(stagingDir);
-    return new Path(stagingDir, INPUT_FOLDER_PREFIX +
-      String.valueOf(EnvironmentEdgeManager.currentTimeMillis()));
-  }
-
-  /**
-   * Create the input files, with the path to copy, for the MR job.
-   * Each input files contains n files, and each input file has a similar amount data to copy.
-   * The number of input files created are based on the number of mappers provided as argument
-   * and the number of the files to copy.
-   */
-  private static Path[] createInputFiles(final Configuration conf,
-      final List<Pair<Path, Long>> snapshotFiles, int mappers)
-      throws IOException, InterruptedException {
-    FileSystem fs = FileSystem.get(conf);
-    Path inputFolderPath = getInputFolderPath(fs, conf);
-    LOG.debug("Input folder location: " + inputFolderPath);
-
-    List<List<Path>> splits = getBalancedSplits(snapshotFiles, mappers);
-    Path[] inputFiles = new Path[splits.size()];
-
-    Text key = new Text();
-    for (int i = 0; i < inputFiles.length; i++) {
-      List<Path> files = splits.get(i);
-      inputFiles[i] = new Path(inputFolderPath, String.format("export-%d.seq", i));
-      SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, inputFiles[i],
-        Text.class, NullWritable.class);
-      LOG.debug("Input split: " + i);
-      try {
-        for (Path file: files) {
-          LOG.debug(file.toString());
-          key.set(file.toString());
-          writer.append(key, NullWritable.get());
-        }
-      } finally {
-        writer.close();
-      }
-    }
-
-    return inputFiles;
-  }
-
-  /**
-   * Run Map-Reduce Job to perform the files copy.
-   */
-  private boolean runCopyJob(final Path inputRoot, final Path outputRoot,
-      final List<Pair<Path, Long>> snapshotFiles, final boolean verifyChecksum,
-      final String filesUser, final String filesGroup, final int filesMode,
-      final int mappers) throws IOException, InterruptedException, ClassNotFoundException {
-    Configuration conf = getConf();
-    if (filesGroup != null) conf.set(CONF_FILES_GROUP, filesGroup);
-    if (filesUser != null) conf.set(CONF_FILES_USER, filesUser);
-    conf.setInt(CONF_FILES_MODE, filesMode);
-    conf.setBoolean(CONF_CHECKSUM_VERIFY, verifyChecksum);
-    conf.set(CONF_OUTPUT_ROOT, outputRoot.toString());
-    conf.set(CONF_INPUT_ROOT, inputRoot.toString());
-    conf.setInt("mapreduce.job.maps", mappers);
-
-    // job.setMapSpeculativeExecution(false)
-    conf.setBoolean("mapreduce.map.speculative", false);
-    conf.setBoolean("mapreduce.reduce.speculative", false);
-    conf.setBoolean("mapred.map.tasks.speculative.execution", false);
-    conf.setBoolean("mapred.reduce.tasks.speculative.execution", false);
-
-    Job job = new Job(conf);
-    job.setJobName("ExportSnapshot");
-    job.setJarByClass(ExportSnapshot.class);
-    job.setMapperClass(ExportMapper.class);
-    job.setInputFormatClass(SequenceFileInputFormat.class);
-    job.setOutputFormatClass(NullOutputFormat.class);
-    job.setNumReduceTasks(0);
-    for (Path path: createInputFiles(conf, snapshotFiles, mappers)) {
-      LOG.debug("Add Input Path=" + path);
-      SequenceFileInputFormat.addInputPath(job, path);
-    }
-
-    return job.waitForCompletion(true);
-  }
-
-  /**
-   * Execute the export snapshot by copying the snapshot metadata, hfiles and hlogs.
-   * @return 0 on success, and != 0 upon failure.
-   */
-  @Override
-  public int run(String[] args) throws Exception {
-    boolean verifyChecksum = true;
-    String snapshotName = null;
-    String filesGroup = null;
-    String filesUser = null;
-    Path outputRoot = null;
-    int filesMode = 0;
-    int mappers = getConf().getInt("mapreduce.job.maps", 1);
-
-    // Process command line args
-    for (int i = 0; i < args.length; i++) {
-      String cmd = args[i];
-      try {
-        if (cmd.equals("-snapshot")) {
-          snapshotName = args[++i];
-        } else if (cmd.equals("-copy-to")) {
-          outputRoot = new Path(args[++i]);
-        } else if (cmd.equals("-no-checksum-verify")) {
-          verifyChecksum = false;
-        } else if (cmd.equals("-mappers")) {
-          mappers = Integer.parseInt(args[++i]);
-        } else if (cmd.equals("-chuser")) {
-          filesUser = args[++i];
-        } else if (cmd.equals("-chgroup")) {
-          filesGroup = args[++i];
-        } else if (cmd.equals("-chmod")) {
-          filesMode = Integer.parseInt(args[++i], 8);
-        } else if (cmd.equals("-h") || cmd.equals("--help")) {
-          printUsageAndExit();
-        } else {
-          System.err.println("UNEXPECTED: " + cmd);
-          printUsageAndExit();
-        }
-      } catch (Exception e) {
-        printUsageAndExit();
-      }
-    }
-
-    // Check user options
-    if (snapshotName == null) {
-      System.err.println("Snapshot name not provided.");
-      printUsageAndExit();
-    }
-
-    if (outputRoot == null) {
-      System.err.println("Destination file-system not provided.");
-      printUsageAndExit();
-    }
-
-    Configuration conf = getConf();
-    Path inputRoot = FSUtils.getRootDir(conf);
-    FileSystem inputFs = FileSystem.get(conf);
-    FileSystem outputFs = FileSystem.get(outputRoot.toUri(), new Configuration());
-
-    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, inputRoot);
-    Path snapshotTmpDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshotName, outputRoot);
-    Path outputSnapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, outputRoot);
-
-    // Check if the snapshot already exists
-    if (outputFs.exists(outputSnapshotDir)) {
-      System.err.println("The snapshot '" + snapshotName +
-        "' already exists in the destination: " + outputSnapshotDir);
-      return 1;
-    }
-
-    // Check if the snapshot already in-progress
-    if (outputFs.exists(snapshotTmpDir)) {
-      System.err.println("A snapshot with the same name '" + snapshotName + "' is in-progress");
-      return 1;
-    }
-
-    // Step 0 - Extract snapshot files to copy
-    final List<Pair<Path, Long>> files = getSnapshotFiles(inputFs, snapshotDir);
-
-    // Step 1 - Copy fs1:/.snapshot/<snapshot> to  fs2:/.snapshot/.tmp/<snapshot>
-    // The snapshot references must be copied before the hfiles otherwise the cleaner
-    // will remove them because they are unreferenced.
-    try {
-      FileUtil.copy(inputFs, snapshotDir, outputFs, snapshotTmpDir, false, false, conf);
-    } catch (IOException e) {
-      System.err.println("Failed to copy the snapshot directory: from=" + snapshotDir +
-        " to=" + snapshotTmpDir);
-      e.printStackTrace(System.err);
-      return 1;
-    }
-
-    // Step 2 - Start MR Job to copy files
-    // The snapshot references must be copied before the files otherwise the files gets removed
-    // by the HFileArchiver, since they have no references.
-    try {
-      if (!runCopyJob(inputRoot, outputRoot, files, verifyChecksum,
-          filesUser, filesGroup, filesMode, mappers)) {
-        throw new ExportSnapshotException("Snapshot export failed!");
-      }
-
-      // Step 3 - Rename fs2:/.snapshot/.tmp/<snapshot> fs2:/.snapshot/<snapshot>
-      if (!outputFs.rename(snapshotTmpDir, outputSnapshotDir)) {
-        System.err.println("Snapshot export failed!");
-        System.err.println("Unable to rename snapshot directory from=" +
-                           snapshotTmpDir + " to=" + outputSnapshotDir);
-        return 1;
-      }
-
-      return 0;
-    } catch (Exception e) {
-      System.err.println("Snapshot export failed!");
-      e.printStackTrace(System.err);
-      outputFs.delete(outputSnapshotDir, true);
-      return 1;
-    }
-  }
-
-  // ExportSnapshot
-  private void printUsageAndExit() {
-    System.err.printf("Usage: bin/hbase %s [options]\n", getClass().getName());
-    System.err.println(" where [options] are:");
-    System.err.println("  -h|-help                Show this help and exit.");
-    System.err.println("  -snapshot NAME          Snapshot to restore.");
-    System.err.println("  -copy-to NAME           Remote destination hdfs://");
-    System.err.println("  -no-checksum-verify     Do not verify checksum.");
-    System.err.println("  -chuser USERNAME        Change the owner of the files to the specified one.");
-    System.err.println("  -chgroup GROUP          Change the group of the files to the specified one.");
-    System.err.println("  -chmod MODE             Change the permission of the files to the specified one.");
-    System.err.println("  -mappers                Number of mappers to use during the copy (mapreduce.job.maps).");
-    System.err.println();
-    System.err.println("Examples:");
-    System.err.println("  hbase " + getClass() + " \\");
-    System.err.println("    -snapshot MySnapshot -copy-to hdfs:///srv2:8082/hbase \\");
-    System.err.println("    -chuser MyUser -chgroup MyGroup -chmod 700 -mappers 16");
-    System.exit(1);
-  }
-
-  /**
-   * The guts of the {@link #main} method.
-   * Call this method to avoid the {@link #main(String[])} System.exit.
-   * @param args
-   * @return errCode
-   * @throws Exception
-   */
-  static int innerMain(final Configuration conf, final String [] args) throws Exception {
-    return ToolRunner.run(conf, new ExportSnapshot(), args);
-  }
-
-  public static void main(String[] args) throws Exception {
-     System.exit(innerMain(HBaseConfiguration.create(), args));
-  }
-}
diff --git a/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java b/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
index df2c751..1eceb56 100644
--- a/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
+++ b/src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
@@ -40,8 +40,8 @@ import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotDoesNotExistException;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.SnapshotDoesNotExistException;
 import org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
diff --git a/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java b/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
index 93e4b62..85f9de9 100644
--- a/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
+++ b/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
@@ -36,8 +36,8 @@ import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy;
 import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.SnapshotCreationException;
 import org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
diff --git a/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java b/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
index e77d8c1..409da4f 100644
--- a/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
+++ b/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
@@ -47,7 +47,7 @@ import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
 import org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils;
-import org.apache.hadoop.hbase.snapshot.exception.UnknownSnapshotException;
+import org.apache.hadoop.hbase.snapshot.UnknownSnapshotException;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.HFileArchiveUtil;
diff --git a/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java b/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java
index 6497387..a6e627c 100644
--- a/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java
+++ b/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java
@@ -32,9 +32,9 @@ import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
 import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
+import org.apache.hadoop.hbase.snapshot.TakeSnapshotUtils;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.junit.After;
 import org.junit.AfterClass;
diff --git a/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotManager.java b/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotManager.java
new file mode 100644
index 0000000..5d16fa3
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotManager.java
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.snapshot;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.executor.ExecutorService;
+import org.apache.hadoop.hbase.master.MasterFileSystem;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.zookeeper.KeeperException;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+/**
+ * Test basic snapshot manager functionality
+ */
+@Category(SmallTests.class)
+public class TestSnapshotManager {
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  MasterServices services = Mockito.mock(MasterServices.class);
+  ExecutorService pool = Mockito.mock(ExecutorService.class);
+  MasterFileSystem mfs = Mockito.mock(MasterFileSystem.class);
+  FileSystem fs;
+  {
+    try {
+      fs = UTIL.getTestFileSystem();
+    } catch (IOException e) {
+      throw new RuntimeException("Couldn't get test filesystem", e);
+    }
+  }
+
+  private SnapshotManager getNewManager() throws KeeperException, IOException {
+    Mockito.reset(services);
+    Mockito.when(services.getConfiguration()).thenReturn(UTIL.getConfiguration());
+    Mockito.when(services.getMasterFileSystem()).thenReturn(mfs);
+    Mockito.when(mfs.getFileSystem()).thenReturn(fs);
+    Mockito.when(mfs.getRootDir()).thenReturn(UTIL.getDataTestDir());
+    return new SnapshotManager(services);
+  }
+
+  @Test
+  public void testInProcess() throws KeeperException, IOException {
+    SnapshotManager manager = getNewManager();
+    TakeSnapshotHandler handler = Mockito.mock(TakeSnapshotHandler.class);
+    assertFalse("Manager is in process when there is no current handler", manager.isTakingSnapshot());
+    manager.setSnapshotHandlerForTesting(handler);
+    Mockito.when(handler.isFinished()).thenReturn(false);
+    assertTrue("Manager isn't in process when handler is running", manager.isTakingSnapshot());
+    Mockito.when(handler.isFinished()).thenReturn(true);
+    assertFalse("Manager is process when handler isn't running", manager.isTakingSnapshot());
+  }
+}
\ No newline at end of file
diff --git a/src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java b/src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java
deleted file mode 100644
index 96e725c..0000000
--- a/src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.master.snapshot.manage;
-
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-import java.io.IOException;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.Server;
-import org.apache.hadoop.hbase.SmallTests;
-import org.apache.hadoop.hbase.TableDescriptors;
-import org.apache.hadoop.hbase.executor.ExecutorService;
-import org.apache.hadoop.hbase.master.MasterFileSystem;
-import org.apache.hadoop.hbase.master.MasterServices;
-import org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.zookeeper.KeeperException;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-import org.mockito.Mockito;
-
-/**
- * Test basic snapshot manager functionality
- */
-@Category(SmallTests.class)
-public class TestSnapshotManager {
-  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
-
-  MasterServices services = Mockito.mock(MasterServices.class);
-  ExecutorService pool = Mockito.mock(ExecutorService.class);
-  MasterFileSystem mfs = Mockito.mock(MasterFileSystem.class);
-  FileSystem fs;
-  {
-    try {
-      fs = UTIL.getTestFileSystem();
-    } catch (IOException e) {
-      throw new RuntimeException("Couldn't get test filesystem", e);
-    }
-  }
-
-  private SnapshotManager getNewManager() throws KeeperException, IOException {
-    Mockito.reset(services);
-    Mockito.when(services.getConfiguration()).thenReturn(UTIL.getConfiguration());
-    Mockito.when(services.getMasterFileSystem()).thenReturn(mfs);
-    Mockito.when(mfs.getFileSystem()).thenReturn(fs);
-    Mockito.when(mfs.getRootDir()).thenReturn(UTIL.getDataTestDir());
-    return new SnapshotManager(services);
-  }
-
-  @Test
-  public void testInProcess() throws KeeperException, IOException {
-    SnapshotManager manager = getNewManager();
-    TakeSnapshotHandler handler = Mockito.mock(TakeSnapshotHandler.class);
-    assertFalse("Manager is in process when there is no current handler", manager.isTakingSnapshot());
-    manager.setSnapshotHandlerForTesting(handler);
-    Mockito.when(handler.isFinished()).thenReturn(false);
-    assertTrue("Manager isn't in process when handler is running", manager.isTakingSnapshot());
-    Mockito.when(handler.isFinished()).thenReturn(true);
-    assertFalse("Manager is process when handler isn't running", manager.isTakingSnapshot());
-  }
-}
\ No newline at end of file
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java
deleted file mode 100644
index 5aae56d..0000000
--- a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java
+++ /dev/null
@@ -1,127 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.SmallTests;
-import org.apache.hadoop.hbase.errorhandling.ForeignException;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.regionserver.wal.HLog;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-import org.mockito.Mockito;
-
-/**
- * Test that we correctly copy the recovered edits from a directory
- */
-@Category(SmallTests.class)
-public class TestCopyRecoveredEditsTask {
-
-  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
-
-  @Test
-  public void testCopyFiles() throws Exception {
-
-    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
-    ForeignExceptionDispatcher monitor = Mockito.mock(ForeignExceptionDispatcher.class);
-    FileSystem fs = UTIL.getTestFileSystem();
-    Path root = UTIL.getDataTestDir();
-    String regionName = "regionA";
-    Path regionDir = new Path(root, regionName);
-    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, root);
-
-    try {
-      // doesn't really matter where the region's snapshot directory is, but this is pretty close
-      Path snapshotRegionDir = new Path(workingDir, regionName);
-      fs.mkdirs(snapshotRegionDir);
-
-      // put some stuff in the recovered.edits directory
-      Path edits = HLog.getRegionDirRecoveredEditsDir(regionDir);
-      fs.mkdirs(edits);
-      // make a file with some data
-      Path file1 = new Path(edits, "0000000000000002352");
-      FSDataOutputStream out = fs.create(file1);
-      byte[] data = new byte[] { 1, 2, 3, 4 };
-      out.write(data);
-      out.close();
-      // make an empty file
-      Path empty = new Path(edits, "empty");
-      fs.createNewFile(empty);
-
-      CopyRecoveredEditsTask task = new CopyRecoveredEditsTask(snapshot, monitor, fs, regionDir,
-          snapshotRegionDir);
-      CopyRecoveredEditsTask taskSpy = Mockito.spy(task);
-      taskSpy.call();
-
-      Path snapshotEdits = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
-      FileStatus[] snapshotEditFiles = FSUtils.listStatus(fs, snapshotEdits);
-      assertEquals("Got wrong number of files in the snapshot edits", 1, snapshotEditFiles.length);
-      FileStatus file = snapshotEditFiles[0];
-      assertEquals("Didn't copy expected file", file1.getName(), file.getPath().getName());
-
-      Mockito.verify(monitor, Mockito.never()).receive(Mockito.any(ForeignException.class));
-      Mockito.verify(taskSpy, Mockito.never()).snapshotFailure(Mockito.anyString(),
-           Mockito.any(Exception.class));
-
-    } finally {
-      // cleanup the working directory
-      FSUtils.delete(fs, regionDir, true);
-      FSUtils.delete(fs, workingDir, true);
-    }
-  }
-
-  /**
-   * Check that we don't get an exception if there is no recovered edits directory to copy
-   * @throws Exception on failure
-   */
-  @Test
-  public void testNoEditsDir() throws Exception {
-    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
-    ForeignExceptionDispatcher monitor = Mockito.mock(ForeignExceptionDispatcher.class);
-    FileSystem fs = UTIL.getTestFileSystem();
-    Path root = UTIL.getDataTestDir();
-    String regionName = "regionA";
-    Path regionDir = new Path(root, regionName);
-    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, root);
-    try {
-      // doesn't really matter where the region's snapshot directory is, but this is pretty close
-      Path snapshotRegionDir = new Path(workingDir, regionName);
-      fs.mkdirs(snapshotRegionDir);
-      Path regionEdits = HLog.getRegionDirRecoveredEditsDir(regionDir);
-      assertFalse("Edits dir exists already - it shouldn't", fs.exists(regionEdits));
-
-      CopyRecoveredEditsTask task = new CopyRecoveredEditsTask(snapshot, monitor, fs, regionDir,
-          snapshotRegionDir);
-      task.call();
-    } finally {
-      // cleanup the working directory
-      FSUtils.delete(fs, regionDir, true);
-      FSUtils.delete(fs, workingDir, true);
-    }
-  }
-}
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java
deleted file mode 100644
index 42d0c98..0000000
--- a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import static org.junit.Assert.assertTrue;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.SmallTests;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-import org.mockito.Mockito;
-
-@Category(SmallTests.class)
-public class TestReferenceRegionHFilesTask {
-  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
-
-  @Test
-  public void testRun() throws IOException {
-    FileSystem fs = UTIL.getTestFileSystem();
-    // setup the region internals
-    Path testdir = UTIL.getDataTestDir();
-    Path regionDir = new Path(testdir, "region");
-    Path family1 = new Path(regionDir, "fam1");
-    // make an empty family
-    Path family2 = new Path(regionDir, "fam2");
-    fs.mkdirs(family2);
-
-    // add some files to family 1
-    Path file1 = new Path(family1, "05f99689ae254693836613d1884c6b63");
-    fs.createNewFile(file1);
-    Path file2 = new Path(family1, "7ac9898bf41d445aa0003e3d699d5d26");
-    fs.createNewFile(file2);
-
-    // create the snapshot directory
-    Path snapshotRegionDir = new Path(testdir, HConstants.SNAPSHOT_DIR_NAME);
-    fs.mkdirs(snapshotRegionDir);
-
-    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("name")
-        .setTable("table").build();
-    ForeignExceptionDispatcher monitor = Mockito.mock(ForeignExceptionDispatcher.class);
-    ReferenceRegionHFilesTask task = new ReferenceRegionHFilesTask(snapshot, monitor, regionDir,
-        fs, snapshotRegionDir);
-    ReferenceRegionHFilesTask taskSpy = Mockito.spy(task);
-    task.call();
-
-    // make sure we never get an error
-    Mockito.verify(taskSpy, Mockito.never()).snapshotFailure(Mockito.anyString(),
-        Mockito.any(Exception.class));
-
-    // verify that all the hfiles get referenced
-    List<String> hfiles = new ArrayList<String>(2);
-    FileStatus[] regions = FSUtils.listStatus(fs, snapshotRegionDir);
-    for (FileStatus region : regions) {
-      FileStatus[] fams = FSUtils.listStatus(fs, region.getPath());
-      for (FileStatus fam : fams) {
-        FileStatus[] files = FSUtils.listStatus(fs, fam.getPath());
-        for (FileStatus file : files) {
-          hfiles.add(file.getPath().getName());
-        }
-      }
-    }
-    assertTrue("Didn't reference :" + file1, hfiles.contains(file1.getName()));
-    assertTrue("Didn't reference :" + file1, hfiles.contains(file2.getName()));
-  }
-}
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java
deleted file mode 100644
index 48afde2..0000000
--- a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import static org.mockito.Matchers.any;
-import static org.mockito.Matchers.anyString;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.verify;
-
-import org.apache.hadoop.hbase.SmallTests;
-import org.apache.hadoop.hbase.errorhandling.ForeignException;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-import org.mockito.Mockito;
-
-@Category(SmallTests.class)
-public class TestSnapshotTask {
-
-  /**
-   * Check that errors from running the task get propagated back to the error listener.
-   */
-  @Test
-  public void testErrorPropagation() throws Exception {
-    ForeignExceptionDispatcher error = mock(ForeignExceptionDispatcher.class);
-    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot")
-        .setTable("table").build();
-    final Exception thrown = new Exception("Failed!");
-    SnapshotTask fail = new SnapshotTask(snapshot, error) {
-      @Override
-      public Void call() {
-        snapshotFailure("Injected failure", thrown);
-        return null;
-      }
-    };
-    fail.call();
-
-    verify(error, Mockito.times(1)).receive(any(ForeignException.class));
-  }
-
-}
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java
deleted file mode 100644
index 8d6df8f..0000000
--- a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.server.snapshot.task;
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.SmallTests;
-import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-import org.mockito.Mockito;
-
-/**
- * Test that the WAL reference task works as expected
- */
-@Category(SmallTests.class)
-public class TestWALReferenceTask {
-
-  private static final Log LOG = LogFactory.getLog(TestWALReferenceTask.class);
-  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
-
-  @Test
-  public void testRun() throws IOException {
-    Configuration conf = UTIL.getConfiguration();
-    FileSystem fs = UTIL.getTestFileSystem();
-    // setup the log dir
-    Path testDir = UTIL.getDataTestDir();
-    Set<String> servers = new HashSet<String>();
-    Path logDir = new Path(testDir, ".logs");
-    Path server1Dir = new Path(logDir, "Server1");
-    servers.add(server1Dir.getName());
-    Path server2Dir = new Path(logDir, "me.hbase.com,56073,1348618509968");
-    servers.add(server2Dir.getName());
-    // logs under server 1
-    Path log1_1 = new Path(server1Dir, "me.hbase.com%2C56073%2C1348618509968.1348618520536");
-    Path log1_2 = new Path(server1Dir, "me.hbase.com%2C56073%2C1348618509968.1234567890123");
-    // logs under server 2
-    Path log2_1 = new Path(server2Dir, "me.hbase.com%2C56074%2C1348618509998.1348618515589");
-    Path log2_2 = new Path(server2Dir, "me.hbase.com%2C56073%2C1348618509968.1234567890123");
-
-    // create all the log files
-    fs.createNewFile(log1_1);
-    fs.createNewFile(log1_2);
-    fs.createNewFile(log2_1);
-    fs.createNewFile(log2_2);
-
-    FSUtils.logFileSystemState(fs, testDir, LOG);
-    FSUtils.setRootDir(conf, testDir);
-    SnapshotDescription snapshot = SnapshotDescription.newBuilder()
-        .setName("testWALReferenceSnapshot").build();
-    ForeignExceptionDispatcher listener = Mockito.mock(ForeignExceptionDispatcher.class);
-
-    // reference all the files in the first server directory
-    ReferenceServerWALsTask task = new ReferenceServerWALsTask(snapshot, listener, server1Dir,
-        conf, fs);
-    task.call();
-
-    // reference all the files in the first server directory
-    task = new ReferenceServerWALsTask(snapshot, listener, server2Dir, conf, fs);
-    task.call();
-
-    // verify that we got everything
-    FSUtils.logFileSystemState(fs, testDir, LOG);
-    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, testDir);
-    Path snapshotLogDir = new Path(workingDir, HConstants.HREGION_LOGDIR_NAME);
-
-    // make sure we reference the all the wal files
-    TakeSnapshotUtils.verifyAllLogsGotReferenced(fs, logDir, servers, snapshot, snapshotLogDir);
-
-    // make sure we never got an error
-    Mockito.verify(listener, Mockito.atLeastOnce()).rethrowException();
-    Mockito.verifyNoMoreInteractions(listener);
-  }
-}
\ No newline at end of file
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java b/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
index 72d1cb1..b04fb55 100644
--- a/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
@@ -41,9 +41,9 @@ import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.snapshot.HBaseSnapshotException;
 import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
-import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.snapshot.TakeSnapshotUtils;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestCopyRecoveredEditsTask.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestCopyRecoveredEditsTask.java
new file mode 100644
index 0000000..b68c3b9
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestCopyRecoveredEditsTask.java
@@ -0,0 +1,126 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.errorhandling.ForeignException;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+/**
+ * Test that we correctly copy the recovered edits from a directory
+ */
+@Category(SmallTests.class)
+public class TestCopyRecoveredEditsTask {
+
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  @Test
+  public void testCopyFiles() throws Exception {
+
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
+    ForeignExceptionDispatcher monitor = Mockito.mock(ForeignExceptionDispatcher.class);
+    FileSystem fs = UTIL.getTestFileSystem();
+    Path root = UTIL.getDataTestDir();
+    String regionName = "regionA";
+    Path regionDir = new Path(root, regionName);
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, root);
+
+    try {
+      // doesn't really matter where the region's snapshot directory is, but this is pretty close
+      Path snapshotRegionDir = new Path(workingDir, regionName);
+      fs.mkdirs(snapshotRegionDir);
+
+      // put some stuff in the recovered.edits directory
+      Path edits = HLog.getRegionDirRecoveredEditsDir(regionDir);
+      fs.mkdirs(edits);
+      // make a file with some data
+      Path file1 = new Path(edits, "0000000000000002352");
+      FSDataOutputStream out = fs.create(file1);
+      byte[] data = new byte[] { 1, 2, 3, 4 };
+      out.write(data);
+      out.close();
+      // make an empty file
+      Path empty = new Path(edits, "empty");
+      fs.createNewFile(empty);
+
+      CopyRecoveredEditsTask task = new CopyRecoveredEditsTask(snapshot, monitor, fs, regionDir,
+          snapshotRegionDir);
+      CopyRecoveredEditsTask taskSpy = Mockito.spy(task);
+      taskSpy.call();
+
+      Path snapshotEdits = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
+      FileStatus[] snapshotEditFiles = FSUtils.listStatus(fs, snapshotEdits);
+      assertEquals("Got wrong number of files in the snapshot edits", 1, snapshotEditFiles.length);
+      FileStatus file = snapshotEditFiles[0];
+      assertEquals("Didn't copy expected file", file1.getName(), file.getPath().getName());
+
+      Mockito.verify(monitor, Mockito.never()).receive(Mockito.any(ForeignException.class));
+      Mockito.verify(taskSpy, Mockito.never()).snapshotFailure(Mockito.anyString(),
+           Mockito.any(Exception.class));
+    } finally {
+      // cleanup the working directory
+      FSUtils.delete(fs, regionDir, true);
+      FSUtils.delete(fs, workingDir, true);
+    }
+  }
+
+  /**
+   * Check that we don't get an exception if there is no recovered edits directory to copy
+   * @throws Exception on failure
+   */
+  @Test
+  public void testNoEditsDir() throws Exception {
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
+    ForeignExceptionDispatcher monitor = Mockito.mock(ForeignExceptionDispatcher.class);
+    FileSystem fs = UTIL.getTestFileSystem();
+    Path root = UTIL.getDataTestDir();
+    String regionName = "regionA";
+    Path regionDir = new Path(root, regionName);
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, root);
+    try {
+      // doesn't really matter where the region's snapshot directory is, but this is pretty close
+      Path snapshotRegionDir = new Path(workingDir, regionName);
+      fs.mkdirs(snapshotRegionDir);
+      Path regionEdits = HLog.getRegionDirRecoveredEditsDir(regionDir);
+      assertFalse("Edits dir exists already - it shouldn't", fs.exists(regionEdits));
+
+      CopyRecoveredEditsTask task = new CopyRecoveredEditsTask(snapshot, monitor, fs, regionDir,
+          snapshotRegionDir);
+      task.call();
+    } finally {
+      // cleanup the working directory
+      FSUtils.delete(fs, regionDir, true);
+      FSUtils.delete(fs, workingDir, true);
+    }
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
new file mode 100644
index 0000000..bebdf68
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
@@ -0,0 +1,255 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.snapshot;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.snapshot.ExportSnapshot;
+import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
+import org.apache.hadoop.mapreduce.Job;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+/**
+ * Test Export Snapshot Tool
+ */
+@Category(MediumTests.class)
+public class TestExportSnapshot {
+  private final Log LOG = LogFactory.getLog(getClass());
+
+  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+
+  private final static byte[] FAMILY = Bytes.toBytes("cf");
+
+  private byte[] snapshotName;
+  private byte[] tableName;
+  private HBaseAdmin admin;
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    TEST_UTIL.getConfiguration().setInt("hbase.regionserver.msginterval", 100);
+    TEST_UTIL.getConfiguration().setInt("hbase.client.pause", 250);
+    TEST_UTIL.getConfiguration().setInt("hbase.client.retries.number", 6);
+    TEST_UTIL.getConfiguration().setBoolean("hbase.master.enabletable.roundrobin", true);
+    TEST_UTIL.startMiniCluster(3);
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  /**
+   * Create a table and take a snapshot of the table used by the export test.
+   */
+  @Before
+  public void setUp() throws Exception {
+    this.admin = TEST_UTIL.getHBaseAdmin();
+
+    long tid = System.currentTimeMillis();
+    tableName = Bytes.toBytes("testtb-" + tid);
+    snapshotName = Bytes.toBytes("snaptb0-" + tid);
+
+    // create Table
+    HTableDescriptor htd = new HTableDescriptor(tableName);
+    htd.addFamily(new HColumnDescriptor(FAMILY));
+    admin.createTable(htd, null);
+    HTable table = new HTable(TEST_UTIL.getConfiguration(), tableName);
+    TEST_UTIL.loadTable(table, FAMILY);
+
+    // take a snapshot
+    admin.disableTable(tableName);
+    admin.snapshot(snapshotName, tableName);
+    admin.enableTable(tableName);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    this.admin.close();
+  }
+
+  /**
+   * Verfy the result of getBalanceSplits() method.
+   * The result are groups of files, used as input list for the "export" mappers.
+   * All the groups should have similar amount of data.
+   *
+   * The input list is a pair of file path and length.
+   * The getBalanceSplits() function sort it by length,
+   * and assign to each group a file, going back and forth through the groups.
+   */
+  @Test
+  public void testBalanceSplit() throws Exception {
+    // Create a list of files
+    List<Pair<Path, Long>> files = new ArrayList<Pair<Path, Long>>();
+    for (long i = 0; i <= 20; i++) {
+      files.add(new Pair<Path, Long>(new Path("file-" + i), i));
+    }
+
+    // Create 5 groups (total size 210)
+    //    group 0: 20, 11, 10,  1 (total size: 42)
+    //    group 1: 19, 12,  9,  2 (total size: 42)
+    //    group 2: 18, 13,  8,  3 (total size: 42)
+    //    group 3: 17, 12,  7,  4 (total size: 42)
+    //    group 4: 16, 11,  6,  5 (total size: 42)
+    List<List<Path>> splits = ExportSnapshot.getBalancedSplits(files, 5);
+    assertEquals(5, splits.size());
+    assertEquals(Arrays.asList(new Path("file-20"), new Path("file-11"),
+      new Path("file-10"), new Path("file-1"), new Path("file-0")), splits.get(0));
+    assertEquals(Arrays.asList(new Path("file-19"), new Path("file-12"),
+      new Path("file-9"), new Path("file-2")), splits.get(1));
+    assertEquals(Arrays.asList(new Path("file-18"), new Path("file-13"),
+      new Path("file-8"), new Path("file-3")), splits.get(2));
+    assertEquals(Arrays.asList(new Path("file-17"), new Path("file-14"),
+      new Path("file-7"), new Path("file-4")), splits.get(3));
+    assertEquals(Arrays.asList(new Path("file-16"), new Path("file-15"),
+      new Path("file-6"), new Path("file-5")), splits.get(4));
+  }
+
+  /**
+   * Verify if exported snapshot and copied files matches the original one.
+   */
+  @Test
+  public void testExportFileSystemState() throws Exception {
+    Path copyDir = TEST_UTIL.getDataTestDir("export-" + System.currentTimeMillis());
+    URI hdfsUri = FileSystem.get(TEST_UTIL.getConfiguration()).getUri();
+    FileSystem fs = FileSystem.get(copyDir.toUri(), new Configuration());
+    copyDir = copyDir.makeQualified(fs);
+
+    // Export Snapshot
+    int res = ExportSnapshot.innerMain(TEST_UTIL.getConfiguration(), new String[] {
+      "-snapshot", Bytes.toString(snapshotName),
+      "-copy-to", copyDir.toString()
+    });
+    assertEquals(0, res);
+
+    // Verify File-System state
+    FileStatus[] rootFiles = fs.listStatus(copyDir);
+    assertEquals(2, rootFiles.length);
+    for (FileStatus fileStatus: rootFiles) {
+      String name = fileStatus.getPath().getName();
+      assertTrue(fileStatus.isDir());
+      assertTrue(name.equals(".snapshot") || name.equals(".archive"));
+    }
+
+    // compare the snapshot metadata and verify the hfiles
+    final FileSystem hdfs = FileSystem.get(hdfsUri, TEST_UTIL.getConfiguration());
+    final Path snapshotDir = new Path(".snapshot", Bytes.toString(snapshotName));
+    verifySnapshot(hdfs, new Path(TEST_UTIL.getDefaultRootDirPath(), snapshotDir),
+        fs, new Path(copyDir, snapshotDir));
+    verifyArchive(fs, copyDir, Bytes.toString(snapshotName));
+
+    // Remove the exported dir
+    fs.delete(copyDir, true);
+  }
+
+  /*
+   * verify if the snapshot folder on file-system 1 match the one on file-system 2
+   */
+  private void verifySnapshot(final FileSystem fs1, final Path root1,
+      final FileSystem fs2, final Path root2) throws IOException {
+    Set<String> s = new HashSet<String>();
+    assertEquals(listFiles(fs1, root1, root1), listFiles(fs2, root2, root2));
+  }
+
+  /*
+   * Verify if the files exists
+   */
+  private void verifyArchive(final FileSystem fs, final Path rootDir, final String snapshotName)
+      throws IOException {
+    final Path exportedSnapshot = new Path(rootDir, new Path(".snapshot", snapshotName));
+    final Path exportedArchive = new Path(rootDir, ".archive");
+    LOG.debug(listFiles(fs, exportedArchive, exportedArchive));
+    SnapshotReferenceUtil.visitReferencedFiles(fs, exportedSnapshot,
+        new SnapshotReferenceUtil.FileVisitor() {
+        public void storeFile (final String region, final String family, final String hfile)
+            throws IOException {
+          verifyNonEmptyFile(new Path(exportedArchive,
+            new Path(Bytes.toString(tableName), new Path(region, new Path(family, hfile)))));
+        }
+
+        public void recoveredEdits (final String region, final String logfile)
+            throws IOException {
+          verifyNonEmptyFile(new Path(exportedSnapshot,
+            new Path(Bytes.toString(tableName), new Path(region, logfile))));
+        }
+
+        public void logFile (final String server, final String logfile)
+            throws IOException {
+          verifyNonEmptyFile(new Path(exportedSnapshot, new Path(server, logfile)));
+        }
+
+        private void verifyNonEmptyFile(final Path path) throws IOException {
+          LOG.debug(path);
+          assertTrue(fs.exists(path));
+          assertTrue(fs.getFileStatus(path).getLen() > 0);
+        }
+    });
+  }
+
+  private Set<String> listFiles(final FileSystem fs, final Path root, final Path dir)
+      throws IOException {
+    Set<String> files = new HashSet<String>();
+    int rootPrefix = root.toString().length();
+    FileStatus[] list = FSUtils.listStatus(fs, dir);
+    if (list != null) {
+      for (FileStatus fstat: list) {
+        LOG.debug(fstat.getPath());
+        if (fstat.isDir()) {
+          files.addAll(listFiles(fs, root, fstat.getPath()));
+        } else {
+          files.add(fstat.getPath().toString().substring(rootPrefix));
+        }
+      }
+    }
+    return files;
+  }
+}
+
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestReferenceRegionHFilesTask.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestReferenceRegionHFilesTask.java
new file mode 100644
index 0000000..73c2aba
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestReferenceRegionHFilesTask.java
@@ -0,0 +1,92 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.ReferenceRegionHFilesTask;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+@Category(SmallTests.class)
+public class TestReferenceRegionHFilesTask {
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  @Test
+  public void testRun() throws IOException {
+    FileSystem fs = UTIL.getTestFileSystem();
+    // setup the region internals
+    Path testdir = UTIL.getDataTestDir();
+    Path regionDir = new Path(testdir, "region");
+    Path family1 = new Path(regionDir, "fam1");
+    // make an empty family
+    Path family2 = new Path(regionDir, "fam2");
+    fs.mkdirs(family2);
+
+    // add some files to family 1
+    Path file1 = new Path(family1, "05f99689ae254693836613d1884c6b63");
+    fs.createNewFile(file1);
+    Path file2 = new Path(family1, "7ac9898bf41d445aa0003e3d699d5d26");
+    fs.createNewFile(file2);
+
+    // create the snapshot directory
+    Path snapshotRegionDir = new Path(testdir, HConstants.SNAPSHOT_DIR_NAME);
+    fs.mkdirs(snapshotRegionDir);
+
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("name")
+        .setTable("table").build();
+    ForeignExceptionDispatcher monitor = Mockito.mock(ForeignExceptionDispatcher.class);
+    ReferenceRegionHFilesTask task = new ReferenceRegionHFilesTask(snapshot, monitor, regionDir,
+        fs, snapshotRegionDir);
+    ReferenceRegionHFilesTask taskSpy = Mockito.spy(task);
+    task.call();
+
+    // make sure we never get an error
+    Mockito.verify(taskSpy, Mockito.never()).snapshotFailure(Mockito.anyString(),
+        Mockito.any(Exception.class));
+
+    // verify that all the hfiles get referenced
+    List<String> hfiles = new ArrayList<String>(2);
+    FileStatus[] regions = FSUtils.listStatus(fs, snapshotRegionDir);
+    for (FileStatus region : regions) {
+      FileStatus[] fams = FSUtils.listStatus(fs, region.getPath());
+      for (FileStatus fam : fams) {
+        FileStatus[] files = FSUtils.listStatus(fs, fam.getPath());
+        for (FileStatus file : files) {
+          hfiles.add(file.getPath().getName());
+        }
+      }
+    }
+    assertTrue("Didn't reference :" + file1, hfiles.contains(file1.getName()));
+    assertTrue("Didn't reference :" + file1, hfiles.contains(file2.getName()));
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotLogSplitter.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotLogSplitter.java
new file mode 100644
index 0000000..66b941a
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotLogSplitter.java
@@ -0,0 +1,176 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import static org.junit.Assert.assertArrayEquals;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
+import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.junit.*;
+import org.junit.experimental.categories.Category;
+
+/**
+ * Test snapshot log splitter
+ */
+@Category(SmallTests.class)
+public class TestSnapshotLogSplitter {
+  final Log LOG = LogFactory.getLog(getClass());
+
+  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+
+  private byte[] TEST_QUALIFIER = Bytes.toBytes("q");
+  private byte[] TEST_FAMILY = Bytes.toBytes("f");
+
+  private Configuration conf;
+  private FileSystem fs;
+  private Path logFile;
+
+  @Before
+  public void setup() throws Exception {
+    conf = TEST_UTIL.getConfiguration();
+    fs = FileSystem.get(conf);
+    logFile = new Path(TEST_UTIL.getDataTestDir(), "test.log");
+    writeTestLog(logFile);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    fs.delete(logFile, false);
+  }
+
+  @Test
+  public void testSplitLogs() throws IOException {
+    Map<byte[], byte[]> regionsMap = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+    splitTestLogs(getTableName(5), regionsMap);
+  }
+
+  @Test
+  public void testSplitLogsOnDifferentTable() throws IOException {
+    byte[] tableName = getTableName(1);
+    Map<byte[], byte[]> regionsMap = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+    for (int j = 0; j < 10; ++j) {
+      byte[] regionName = getRegionName(tableName, j);
+      byte[] newRegionName = getNewRegionName(tableName, j);
+      regionsMap.put(regionName, newRegionName);
+    }
+    splitTestLogs(tableName, regionsMap);
+  }
+
+  /*
+   * Split and verify test logs for the specified table
+   */
+  private void splitTestLogs(final byte[] tableName, final Map<byte[], byte[]> regionsMap)
+      throws IOException {
+    Path tableDir = new Path(TEST_UTIL.getDataTestDir(), Bytes.toString(tableName));
+    SnapshotLogSplitter logSplitter = new SnapshotLogSplitter(conf, fs, tableDir,
+      tableName, regionsMap);
+    try {
+      logSplitter.splitLog(logFile);
+    } finally {
+      logSplitter.close();
+    }
+    verifyRecoverEdits(tableDir, tableName, regionsMap);
+  }
+
+  /*
+   * Verify that every logs in the table directory has just the specified table and regions.
+   */
+  private void verifyRecoverEdits(final Path tableDir, final byte[] tableName,
+      final Map<byte[], byte[]> regionsMap) throws IOException {
+    for (FileStatus regionStatus: FSUtils.listStatus(fs, tableDir)) {
+      assertTrue(regionStatus.getPath().getName().startsWith(Bytes.toString(tableName)));
+      Path regionEdits = HLog.getRegionDirRecoveredEditsDir(regionStatus.getPath());
+      byte[] regionName = Bytes.toBytes(regionStatus.getPath().getName());
+      assertFalse(regionsMap.containsKey(regionName));
+      for (FileStatus logStatus: FSUtils.listStatus(fs, regionEdits)) {
+        HLog.Reader reader = HLog.getReader(fs, logStatus.getPath(), conf);
+        try {
+          HLog.Entry entry;
+          while ((entry = reader.next()) != null) {
+            HLogKey key = entry.getKey();
+            assertArrayEquals(tableName, key.getTablename());
+            assertArrayEquals(regionName, key.getEncodedRegionName());
+          }
+        } finally {
+          reader.close();
+        }
+      }
+    }
+  }
+
+  /*
+   * Write some entries in the log file.
+   * 7 different tables with name "testtb-%d"
+   * 10 region per table with name "tableName-region-%d"
+   * 50 entry with row key "row-%d"
+   */
+  private void writeTestLog(final Path logFile) throws IOException {
+    fs.mkdirs(logFile.getParent());
+    HLog.Writer writer = HLog.createWriter(fs, logFile, conf);
+    try {
+      for (int i = 0; i < 7; ++i) {
+        byte[] tableName = getTableName(i);
+        for (int j = 0; j < 10; ++j) {
+          byte[] regionName = getRegionName(tableName, j);
+          for (int k = 0; k < 50; ++k) {
+            byte[] rowkey = Bytes.toBytes("row-" + k);
+            HLogKey key = new HLogKey(regionName, tableName, (long)k,
+              System.currentTimeMillis(), HConstants.DEFAULT_CLUSTER_ID);
+            WALEdit edit = new WALEdit();
+            edit.add(new KeyValue(rowkey, TEST_FAMILY, TEST_QUALIFIER, rowkey));
+            writer.append(new HLog.Entry(key, edit));
+          }
+        }
+      }
+    } finally {
+      writer.close();
+    }
+  }
+
+  private byte[] getTableName(int tableId) {
+    return Bytes.toBytes("testtb-" + tableId);
+  }
+
+  private byte[] getRegionName(final byte[] tableName, int regionId) {
+    return Bytes.toBytes(Bytes.toString(tableName) + "-region-" + regionId);
+  }
+
+  private byte[] getNewRegionName(final byte[] tableName, int regionId) {
+    return Bytes.toBytes(Bytes.toString(tableName) + "-new-region-" + regionId);
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotTask.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotTask.java
new file mode 100644
index 0000000..36b7050
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotTask.java
@@ -0,0 +1,58 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.anyString;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.verify;
+
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.errorhandling.ForeignException;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.SnapshotTask;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+@Category(SmallTests.class)
+public class TestSnapshotTask {
+
+  /**
+   * Check that errors from running the task get propagated back to the error listener.
+   */
+  @Test
+  public void testErrorPropagation() throws Exception {
+    ForeignExceptionDispatcher error = mock(ForeignExceptionDispatcher.class);
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot")
+        .setTable("table").build();
+    final Exception thrown = new Exception("Failed!");
+    SnapshotTask fail = new SnapshotTask(snapshot, error) {
+      @Override
+      public Void call() {
+        snapshotFailure("Injected failure", thrown);
+        return null;
+      }
+    };
+    fail.call();
+
+    verify(error, Mockito.times(1)).receive(any(ForeignException.class));
+  }
+
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestWALReferenceTask.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestWALReferenceTask.java
new file mode 100644
index 0000000..a813ba7
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestWALReferenceTask.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.ReferenceServerWALsTask;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+/**
+ * Test that the WAL reference task works as expected
+ */
+@Category(SmallTests.class)
+public class TestWALReferenceTask {
+
+  private static final Log LOG = LogFactory.getLog(TestWALReferenceTask.class);
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  @Test
+  public void testRun() throws IOException {
+    Configuration conf = UTIL.getConfiguration();
+    FileSystem fs = UTIL.getTestFileSystem();
+    // setup the log dir
+    Path testDir = UTIL.getDataTestDir();
+    Set<String> servers = new HashSet<String>();
+    Path logDir = new Path(testDir, ".logs");
+    Path server1Dir = new Path(logDir, "Server1");
+    servers.add(server1Dir.getName());
+    Path server2Dir = new Path(logDir, "me.hbase.com,56073,1348618509968");
+    servers.add(server2Dir.getName());
+    // logs under server 1
+    Path log1_1 = new Path(server1Dir, "me.hbase.com%2C56073%2C1348618509968.1348618520536");
+    Path log1_2 = new Path(server1Dir, "me.hbase.com%2C56073%2C1348618509968.1234567890123");
+    // logs under server 2
+    Path log2_1 = new Path(server2Dir, "me.hbase.com%2C56074%2C1348618509998.1348618515589");
+    Path log2_2 = new Path(server2Dir, "me.hbase.com%2C56073%2C1348618509968.1234567890123");
+
+    // create all the log files
+    fs.createNewFile(log1_1);
+    fs.createNewFile(log1_2);
+    fs.createNewFile(log2_1);
+    fs.createNewFile(log2_2);
+
+    FSUtils.logFileSystemState(fs, testDir, LOG);
+    FSUtils.setRootDir(conf, testDir);
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder()
+        .setName("testWALReferenceSnapshot").build();
+    ForeignExceptionDispatcher listener = Mockito.mock(ForeignExceptionDispatcher.class);
+
+    // reference all the files in the first server directory
+    ReferenceServerWALsTask task = new ReferenceServerWALsTask(snapshot, listener, server1Dir,
+        conf, fs);
+    task.call();
+
+    // reference all the files in the first server directory
+    task = new ReferenceServerWALsTask(snapshot, listener, server2Dir, conf, fs);
+    task.call();
+
+    // verify that we got everything
+    FSUtils.logFileSystemState(fs, testDir, LOG);
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, testDir);
+    Path snapshotLogDir = new Path(workingDir, HConstants.HREGION_LOGDIR_NAME);
+
+    // make sure we reference the all the wal files
+    TakeSnapshotUtils.verifyAllLogsGotReferenced(fs, logDir, servers, snapshot, snapshotLogDir);
+
+    // make sure we never got an error
+    Mockito.verify(listener, Mockito.atLeastOnce()).rethrowException();
+    Mockito.verifyNoMoreInteractions(listener);
+  }
+}
\ No newline at end of file
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/restore/TestSnapshotLogSplitter.java b/src/test/java/org/apache/hadoop/hbase/snapshot/restore/TestSnapshotLogSplitter.java
deleted file mode 100644
index 6c71ca2..0000000
--- a/src/test/java/org/apache/hadoop/hbase/snapshot/restore/TestSnapshotLogSplitter.java
+++ /dev/null
@@ -1,176 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot.restore;
-
-import static org.junit.Assert.assertArrayEquals;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.SmallTests;
-import org.apache.hadoop.hbase.regionserver.wal.HLog;
-import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
-import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.junit.*;
-import org.junit.experimental.categories.Category;
-
-/**
- * Test snapshot log splitter
- */
-@Category(SmallTests.class)
-public class TestSnapshotLogSplitter {
-  final Log LOG = LogFactory.getLog(getClass());
-
-  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
-
-  private byte[] TEST_QUALIFIER = Bytes.toBytes("q");
-  private byte[] TEST_FAMILY = Bytes.toBytes("f");
-
-  private Configuration conf;
-  private FileSystem fs;
-  private Path logFile;
-
-  @Before
-  public void setup() throws Exception {
-    conf = TEST_UTIL.getConfiguration();
-    fs = FileSystem.get(conf);
-    logFile = new Path(TEST_UTIL.getDataTestDir(), "test.log");
-    writeTestLog(logFile);
-  }
-
-  @After
-  public void tearDown() throws Exception {
-    fs.delete(logFile, false);
-  }
-
-  @Test
-  public void testSplitLogs() throws IOException {
-    Map<byte[], byte[]> regionsMap = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
-    splitTestLogs(getTableName(5), regionsMap);
-  }
-
-  @Test
-  public void testSplitLogsOnDifferentTable() throws IOException {
-    byte[] tableName = getTableName(1);
-    Map<byte[], byte[]> regionsMap = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
-    for (int j = 0; j < 10; ++j) {
-      byte[] regionName = getRegionName(tableName, j);
-      byte[] newRegionName = getNewRegionName(tableName, j);
-      regionsMap.put(regionName, newRegionName);
-    }
-    splitTestLogs(tableName, regionsMap);
-  }
-
-  /*
-   * Split and verify test logs for the specified table
-   */
-  private void splitTestLogs(final byte[] tableName, final Map<byte[], byte[]> regionsMap)
-      throws IOException {
-    Path tableDir = new Path(TEST_UTIL.getDataTestDir(), Bytes.toString(tableName));
-    SnapshotLogSplitter logSplitter = new SnapshotLogSplitter(conf, fs, tableDir,
-      tableName, regionsMap);
-    try {
-      logSplitter.splitLog(logFile);
-    } finally {
-      logSplitter.close();
-    }
-    verifyRecoverEdits(tableDir, tableName, regionsMap);
-  }
-
-  /*
-   * Verify that every logs in the table directory has just the specified table and regions.
-   */
-  private void verifyRecoverEdits(final Path tableDir, final byte[] tableName,
-      final Map<byte[], byte[]> regionsMap) throws IOException {
-    for (FileStatus regionStatus: FSUtils.listStatus(fs, tableDir)) {
-      assertTrue(regionStatus.getPath().getName().startsWith(Bytes.toString(tableName)));
-      Path regionEdits = HLog.getRegionDirRecoveredEditsDir(regionStatus.getPath());
-      byte[] regionName = Bytes.toBytes(regionStatus.getPath().getName());
-      assertFalse(regionsMap.containsKey(regionName));
-      for (FileStatus logStatus: FSUtils.listStatus(fs, regionEdits)) {
-        HLog.Reader reader = HLog.getReader(fs, logStatus.getPath(), conf);
-        try {
-          HLog.Entry entry;
-          while ((entry = reader.next()) != null) {
-            HLogKey key = entry.getKey();
-            assertArrayEquals(tableName, key.getTablename());
-            assertArrayEquals(regionName, key.getEncodedRegionName());
-          }
-        } finally {
-          reader.close();
-        }
-      }
-    }
-  }
-
-  /*
-   * Write some entries in the log file.
-   * 7 different tables with name "testtb-%d"
-   * 10 region per table with name "tableName-region-%d"
-   * 50 entry with row key "row-%d"
-   */
-  private void writeTestLog(final Path logFile) throws IOException {
-    fs.mkdirs(logFile.getParent());
-    HLog.Writer writer = HLog.createWriter(fs, logFile, conf);
-    try {
-      for (int i = 0; i < 7; ++i) {
-        byte[] tableName = getTableName(i);
-        for (int j = 0; j < 10; ++j) {
-          byte[] regionName = getRegionName(tableName, j);
-          for (int k = 0; k < 50; ++k) {
-            byte[] rowkey = Bytes.toBytes("row-" + k);
-            HLogKey key = new HLogKey(regionName, tableName, (long)k,
-              System.currentTimeMillis(), HConstants.DEFAULT_CLUSTER_ID);
-            WALEdit edit = new WALEdit();
-            edit.add(new KeyValue(rowkey, TEST_FAMILY, TEST_QUALIFIER, rowkey));
-            writer.append(new HLog.Entry(key, edit));
-          }
-        }
-      }
-    } finally {
-      writer.close();
-    }
-  }
-
-  private byte[] getTableName(int tableId) {
-    return Bytes.toBytes("testtb-" + tableId);
-  }
-
-  private byte[] getRegionName(final byte[] tableName, int regionId) {
-    return Bytes.toBytes(Bytes.toString(tableName) + "-region-" + regionId);
-  }
-
-  private byte[] getNewRegionName(final byte[] tableName, int regionId) {
-    return Bytes.toBytes(Bytes.toString(tableName) + "-new-region-" + regionId);
-  }
-}
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/tool/TestExportSnapshot.java b/src/test/java/org/apache/hadoop/hbase/snapshot/tool/TestExportSnapshot.java
deleted file mode 100644
index c061058..0000000
--- a/src/test/java/org/apache/hadoop/hbase/snapshot/tool/TestExportSnapshot.java
+++ /dev/null
@@ -1,254 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase.snapshot.tool;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-import java.io.IOException;
-import java.net.URI;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.MediumTests;
-import org.apache.hadoop.hbase.MiniHBaseCluster;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.hadoop.hbase.util.Pair;
-import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
-import org.apache.hadoop.mapreduce.Job;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-
-/**
- * Test Export Snapshot Tool
- */
-@Category(MediumTests.class)
-public class TestExportSnapshot {
-  private final Log LOG = LogFactory.getLog(getClass());
-
-  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
-
-  private final static byte[] FAMILY = Bytes.toBytes("cf");
-
-  private byte[] snapshotName;
-  private byte[] tableName;
-  private HBaseAdmin admin;
-
-  @BeforeClass
-  public static void setUpBeforeClass() throws Exception {
-    TEST_UTIL.getConfiguration().setInt("hbase.regionserver.msginterval", 100);
-    TEST_UTIL.getConfiguration().setInt("hbase.client.pause", 250);
-    TEST_UTIL.getConfiguration().setInt("hbase.client.retries.number", 6);
-    TEST_UTIL.getConfiguration().setBoolean("hbase.master.enabletable.roundrobin", true);
-    TEST_UTIL.startMiniCluster(3);
-  }
-
-  @AfterClass
-  public static void tearDownAfterClass() throws Exception {
-    TEST_UTIL.shutdownMiniCluster();
-  }
-
-  /**
-   * Create a table and take a snapshot of the table used by the export test.
-   */
-  @Before
-  public void setUp() throws Exception {
-    this.admin = TEST_UTIL.getHBaseAdmin();
-
-    long tid = System.currentTimeMillis();
-    tableName = Bytes.toBytes("testtb-" + tid);
-    snapshotName = Bytes.toBytes("snaptb0-" + tid);
-
-    // create Table
-    HTableDescriptor htd = new HTableDescriptor(tableName);
-    htd.addFamily(new HColumnDescriptor(FAMILY));
-    admin.createTable(htd, null);
-    HTable table = new HTable(TEST_UTIL.getConfiguration(), tableName);
-    TEST_UTIL.loadTable(table, FAMILY);
-
-    // take a snapshot
-    admin.disableTable(tableName);
-    admin.snapshot(snapshotName, tableName);
-    admin.enableTable(tableName);
-  }
-
-  @After
-  public void tearDown() throws Exception {
-    this.admin.close();
-  }
-
-  /**
-   * Verfy the result of getBalanceSplits() method.
-   * The result are groups of files, used as input list for the "export" mappers.
-   * All the groups should have similar amount of data.
-   *
-   * The input list is a pair of file path and length.
-   * The getBalanceSplits() function sort it by length,
-   * and assign to each group a file, going back and forth through the groups.
-   */
-  @Test
-  public void testBalanceSplit() throws Exception {
-    // Create a list of files
-    List<Pair<Path, Long>> files = new ArrayList<Pair<Path, Long>>();
-    for (long i = 0; i <= 20; i++) {
-      files.add(new Pair<Path, Long>(new Path("file-" + i), i));
-    }
-
-    // Create 5 groups (total size 210)
-    //    group 0: 20, 11, 10,  1 (total size: 42)
-    //    group 1: 19, 12,  9,  2 (total size: 42)
-    //    group 2: 18, 13,  8,  3 (total size: 42)
-    //    group 3: 17, 12,  7,  4 (total size: 42)
-    //    group 4: 16, 11,  6,  5 (total size: 42)
-    List<List<Path>> splits = ExportSnapshot.getBalancedSplits(files, 5);
-    assertEquals(5, splits.size());
-    assertEquals(Arrays.asList(new Path("file-20"), new Path("file-11"),
-      new Path("file-10"), new Path("file-1"), new Path("file-0")), splits.get(0));
-    assertEquals(Arrays.asList(new Path("file-19"), new Path("file-12"),
-      new Path("file-9"), new Path("file-2")), splits.get(1));
-    assertEquals(Arrays.asList(new Path("file-18"), new Path("file-13"),
-      new Path("file-8"), new Path("file-3")), splits.get(2));
-    assertEquals(Arrays.asList(new Path("file-17"), new Path("file-14"),
-      new Path("file-7"), new Path("file-4")), splits.get(3));
-    assertEquals(Arrays.asList(new Path("file-16"), new Path("file-15"),
-      new Path("file-6"), new Path("file-5")), splits.get(4));
-  }
-
-  /**
-   * Verify if exported snapshot and copied files matches the original one.
-   */
-  @Test
-  public void testExportFileSystemState() throws Exception {
-    Path copyDir = TEST_UTIL.getDataTestDir("export-" + System.currentTimeMillis());
-    URI hdfsUri = FileSystem.get(TEST_UTIL.getConfiguration()).getUri();
-    FileSystem fs = FileSystem.get(copyDir.toUri(), new Configuration());
-    copyDir = copyDir.makeQualified(fs);
-
-    // Export Snapshot
-    int res = ExportSnapshot.innerMain(TEST_UTIL.getConfiguration(), new String[] {
-      "-snapshot", Bytes.toString(snapshotName),
-      "-copy-to", copyDir.toString()
-    });
-    assertEquals(0, res);
-
-    // Verify File-System state
-    FileStatus[] rootFiles = fs.listStatus(copyDir);
-    assertEquals(2, rootFiles.length);
-    for (FileStatus fileStatus: rootFiles) {
-      String name = fileStatus.getPath().getName();
-      assertTrue(fileStatus.isDir());
-      assertTrue(name.equals(".snapshot") || name.equals(".archive"));
-    }
-
-    // compare the snapshot metadata and verify the hfiles
-    final FileSystem hdfs = FileSystem.get(hdfsUri, TEST_UTIL.getConfiguration());
-    final Path snapshotDir = new Path(".snapshot", Bytes.toString(snapshotName));
-    verifySnapshot(hdfs, new Path(TEST_UTIL.getDefaultRootDirPath(), snapshotDir),
-        fs, new Path(copyDir, snapshotDir));
-    verifyArchive(fs, copyDir, Bytes.toString(snapshotName));
-
-    // Remove the exported dir
-    fs.delete(copyDir, true);
-  }
-
-  /*
-   * verify if the snapshot folder on file-system 1 match the one on file-system 2
-   */
-  private void verifySnapshot(final FileSystem fs1, final Path root1,
-      final FileSystem fs2, final Path root2) throws IOException {
-    Set<String> s = new HashSet<String>();
-    assertEquals(listFiles(fs1, root1, root1), listFiles(fs2, root2, root2));
-  }
-
-  /*
-   * Verify if the files exists
-   */
-  private void verifyArchive(final FileSystem fs, final Path rootDir, final String snapshotName)
-      throws IOException {
-    final Path exportedSnapshot = new Path(rootDir, new Path(".snapshot", snapshotName));
-    final Path exportedArchive = new Path(rootDir, ".archive");
-    LOG.debug(listFiles(fs, exportedArchive, exportedArchive));
-    SnapshotReferenceUtil.visitReferencedFiles(fs, exportedSnapshot,
-        new SnapshotReferenceUtil.FileVisitor() {
-        public void storeFile (final String region, final String family, final String hfile)
-            throws IOException {
-          verifyNonEmptyFile(new Path(exportedArchive,
-            new Path(Bytes.toString(tableName), new Path(region, new Path(family, hfile)))));
-        }
-
-        public void recoveredEdits (final String region, final String logfile)
-            throws IOException {
-          verifyNonEmptyFile(new Path(exportedSnapshot,
-            new Path(Bytes.toString(tableName), new Path(region, logfile))));
-        }
-
-        public void logFile (final String server, final String logfile)
-            throws IOException {
-          verifyNonEmptyFile(new Path(exportedSnapshot, new Path(server, logfile)));
-        }
-
-        private void verifyNonEmptyFile(final Path path) throws IOException {
-          LOG.debug(path);
-          assertTrue(fs.exists(path));
-          assertTrue(fs.getFileStatus(path).getLen() > 0);
-        }
-    });
-  }
-
-  private Set<String> listFiles(final FileSystem fs, final Path root, final Path dir)
-      throws IOException {
-    Set<String> files = new HashSet<String>();
-    int rootPrefix = root.toString().length();
-    FileStatus[] list = FSUtils.listStatus(fs, dir);
-    if (list != null) {
-      for (FileStatus fstat: list) {
-        LOG.debug(fstat.getPath());
-        if (fstat.isDir()) {
-          files.addAll(listFiles(fs, root, fstat.getPath()));
-        } else {
-          files.add(fstat.getPath().toString().substring(rootPrefix));
-        }
-      }
-    }
-    return files;
-  }
-}
-
-- 
1.7.0.4

