From 641e698a55e5aedd37a35097e1e10698711a93eb Mon Sep 17 00:00:00 2001
From: Matteo Bertozzi <matteo.bertozzi@cloudera.com>
Date: Tue, 15 Jan 2013 12:08:51 -0800
Subject: [PATCH 151/202] HBASE-7365/HBASE-7389 Safer table creation and deletion using .tmp dir

Reason: Snapshots
Author: Matteo Bertozzi
Ref: CDH-9551
---
 .../java/org/apache/hadoop/hbase/HConstants.java   |    5 +-
 .../apache/hadoop/hbase/backup/HFileArchiver.java  |    2 +-
 .../apache/hadoop/hbase/catalog/MetaEditor.java    |   38 ++++++-
 .../java/org/apache/hadoop/hbase/io/HFileLink.java |   20 ++--
 .../hadoop/hbase/master/MasterFileSystem.java      |   70 ++++++++++++
 .../hbase/master/handler/CreateTableHandler.java   |   86 ++++++++++-----
 .../hbase/master/handler/DeleteTableHandler.java   |   39 +++++--
 .../master/snapshot/CloneSnapshotHandler.java      |   27 ++++-
 .../master/snapshot/RestoreSnapshotHandler.java    |   28 ++++--
 .../hbase/snapshot/RestoreSnapshotHelper.java      |  119 +++++++++++++++----
 .../apache/hadoop/hbase/util/HFileArchiveUtil.java |   47 ++++++++-
 .../hadoop/hbase/util/ModifyRegionUtils.java       |   11 +-
 .../hbase/snapshot/TestRestoreSnapshotHelper.java  |    7 +-
 .../hadoop/hbase/util/TestHFileArchiveUtil.java    |    3 +-
 14 files changed, 393 insertions(+), 109 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/HConstants.java b/src/main/java/org/apache/hadoop/hbase/HConstants.java
index d68ac60..e186ff3 100644
--- a/src/main/java/org/apache/hadoop/hbase/HConstants.java
+++ b/src/main/java/org/apache/hadoop/hbase/HConstants.java
@@ -684,10 +684,13 @@ public final class HConstants {
    */
   public static final String SNAPSHOT_DIR_NAME = ".snapshot";
 
+  /** Temporary directory used for table creation and deletion */
+  public static final String HBASE_TEMP_DIRECTORY = ".tmp";
+
   public static final List<String> HBASE_NON_USER_TABLE_DIRS = new ArrayList<String>(
       Arrays.asList(new String[] { HREGION_LOGDIR_NAME, HREGION_OLDLOGDIR_NAME, CORRUPT_DIR_NAME,
           Bytes.toString(META_TABLE_NAME), Bytes.toString(ROOT_TABLE_NAME), SPLIT_LOGDIR_NAME,
-          HBCK_SIDELINEDIR_NAME, HFILE_ARCHIVE_DIRECTORY, SNAPSHOT_DIR_NAME }));
+          HBCK_SIDELINEDIR_NAME, HFILE_ARCHIVE_DIRECTORY, SNAPSHOT_DIR_NAME, HBASE_TEMP_DIRECTORY }));
   
   private HConstants() {
     // Can't be instantiated with this ctor.
diff --git a/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java b/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
index 8d0d745..8797476 100644
--- a/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
+++ b/src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
@@ -107,7 +107,7 @@ public class HFileArchiver {
 
     // make sure the regiondir lives under the tabledir
     Preconditions.checkArgument(regionDir.toString().startsWith(tableDir.toString()));
-    Path regionArchiveDir = HFileArchiveUtil.getRegionArchiveDir(conf, tableDir, regionDir);
+    Path regionArchiveDir = HFileArchiveUtil.getRegionArchiveDir(rootdir, tableDir, regionDir);
 
     LOG.debug("Have an archive directory, preparing to move files");
     FileStatusConverter getAsFile = new FileStatusConverter(fs);
diff --git a/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java b/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
index 8c7b8cf..367c033 100644
--- a/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
+++ b/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
@@ -21,6 +21,7 @@ import java.io.IOException;
 import java.io.InterruptedIOException;
 import java.net.ConnectException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -126,11 +127,22 @@ public class MetaEditor {
    * @param d Delete to add to .META.
    * @throws IOException
    */
-  static void deleteMetaTable(final CatalogTracker ct, final Delete d)
-  throws IOException {
+  static void deleteFromMetaTable(final CatalogTracker ct, final Delete d)
+      throws IOException {
+    deleteFromMetaTable(ct, Arrays.asList(d));
+  }
+
+  /**
+   * Delete the passed <code>deletes</code> from the <code>.META.</code> table.
+   * @param ct CatalogTracker on whose back we will ride the edit.
+   * @param deletes Deletes to add to .META.
+   * @throws IOException
+   */
+  static void deleteFromMetaTable(final CatalogTracker ct, final List<Delete> deletes)
+      throws IOException {
     HTable t = MetaReader.getMetaHTable(ct);
     try {
-      t.delete(d);
+      t.delete(deletes);
     } finally {
       t.close();
     }
@@ -294,11 +306,27 @@ public class MetaEditor {
       HRegionInfo regionInfo)
   throws IOException {
     Delete delete = new Delete(regionInfo.getRegionName());
-    deleteMetaTable(catalogTracker, delete);
+    deleteFromMetaTable(catalogTracker, delete);
     LOG.info("Deleted region " + regionInfo.getRegionNameAsString() + " from META");
   }
 
   /**
+   * Deletes the specified regions from META.
+   * @param catalogTracker
+   * @param regionsInfo list of regions to be deleted from META
+   * @throws IOException
+   */
+  public static void deleteRegions(CatalogTracker catalogTracker,
+      List<HRegionInfo> regionsInfo) throws IOException {
+    List<Delete> deletes = new ArrayList<Delete>(regionsInfo.size());
+    for (HRegionInfo hri: regionsInfo) {
+      deletes.add(new Delete(hri.getRegionName()));
+    }
+    deleteFromMetaTable(catalogTracker, deletes);
+    LOG.info("Deleted from META, regions: " + regionsInfo);
+  }
+
+  /**
    * Adds and Removes the specified regions from .META.
    * @param catalogTracker
    * @param regionsToRemove list of regions to be deleted from META
@@ -341,7 +369,7 @@ public class MetaEditor {
     Delete delete = new Delete(parent.getRegionName());
     delete.deleteColumns(HConstants.CATALOG_FAMILY, HConstants.SPLITA_QUALIFIER);
     delete.deleteColumns(HConstants.CATALOG_FAMILY, HConstants.SPLITB_QUALIFIER);
-    deleteMetaTable(catalogTracker, delete);
+    deleteFromMetaTable(catalogTracker, delete);
     LOG.info("Deleted daughters references, qualifier=" + Bytes.toStringBinary(HConstants.SPLITA_QUALIFIER) +
       " and qualifier=" + Bytes.toStringBinary(HConstants.SPLITB_QUALIFIER) +
       ", from parent " + parent.getRegionNameAsString());
diff --git a/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java b/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java
index 6a78523..edf8c49 100644
--- a/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java
+++ b/src/main/java/org/apache/hadoop/hbase/io/HFileLink.java
@@ -32,6 +32,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -89,6 +90,7 @@ public class HFileLink extends FileLink {
 
   private final Path archivePath;
   private final Path originPath;
+  private final Path tempPath;
 
   /**
    * @param conf {@link Configuration} from which to extract specific archive locations
@@ -106,22 +108,13 @@ public class HFileLink extends FileLink {
    */
   public HFileLink(final Path rootDir, final Path archiveDir, final Path path) {
     Path hfilePath = getRelativeTablePath(path);
+    this.tempPath = new Path(new Path(rootDir, HConstants.HBASE_TEMP_DIRECTORY), hfilePath);
     this.originPath = new Path(rootDir, hfilePath);
     this.archivePath = new Path(archiveDir, hfilePath);
     setLocations(originPath, archivePath);
   }
 
   /**
-   * @param originPath Path to the hfile in the table directory
-   * @param archiveDir Path to the hfile in the archive directory
-   */
-  public HFileLink(final Path originPath, final Path archivePath) {
-    this.originPath = originPath;
-    this.archivePath = archivePath;
-    setLocations(originPath, archivePath);
-  }
-
-  /**
    * @return the origin path of the hfile.
    */
   public Path getOriginPath() {
@@ -191,7 +184,12 @@ public class HFileLink extends FileLink {
       return originPath;
     }
 
-    return new Path(archiveDir, hfilePath);
+    Path archivePath = new Path(archiveDir, hfilePath);
+    if (fs.exists(archivePath)) {
+      return archivePath;
+    }
+
+    return new Path(new Path(rootDir, HConstants.HBASE_TEMP_DIRECTORY), hfilePath);
   }
 
   /**
diff --git a/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java b/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
index da3e050..1cfb0bc 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
@@ -75,6 +75,8 @@ public class MasterFileSystem {
   private final Path oldLogDir;
   // root hbase directory on the FS
   private final Path rootdir;
+  // hbase temp directory used for table construction and deletion
+  private final Path tempdir;
   // create the split log lock
   final Lock splitLogLock = new ReentrantLock();
   final boolean distributedLogSplitting;
@@ -93,6 +95,7 @@ public class MasterFileSystem {
     // default localfs.  Presumption is that rootdir is fully-qualified before
     // we get to here with appropriate fs scheme.
     this.rootdir = FSUtils.getRootDir(conf);
+    this.tempdir = new Path(this.rootdir, HConstants.HBASE_TEMP_DIRECTORY);
     // Cover both bases, the old way of setting default fs and the new.
     // We're supposed to run on 0.20 and 0.21 anyways.
     this.fs = this.rootdir.getFileSystem(conf);
@@ -129,6 +132,9 @@ public class MasterFileSystem {
     // check if the root directory exists
     checkRootDir(this.rootdir, conf, this.fs);
 
+    // check if temp directory exists and clean it
+    checkTempDir(this.tempdir, conf, this.fs);
+
     Path oldLogDir = new Path(this.rootdir, HConstants.HREGION_OLDLOGDIR_NAME);
 
     // Make sure the region servers can archive their old logs
@@ -177,6 +183,13 @@ public class MasterFileSystem {
   }
 
   /**
+   * @return HBase temp dir.
+   */
+  public Path getTempDir() {
+    return this.tempdir;
+  }
+
+  /**
    * @return The unique identifier generated for this cluster
    */
   public String getClusterId() {
@@ -384,6 +397,32 @@ public class MasterFileSystem {
     }
   }
 
+  /**
+   * Make sure the hbase temp directory exists and is empty.
+   * NOTE that this method is only executed once just after the master becomes the active one.
+   */
+  private void checkTempDir(final Path tmpdir, final Configuration c, final FileSystem fs)
+      throws IOException {
+    // If the temp directory exists, clear the content (left over, from the previous run)
+    if (fs.exists(tmpdir)) {
+      // Archive table in temp, maybe left over from failed deletion,
+      // if not the cleaner will take care of them.
+      for (Path tabledir: FSUtils.getTableDirs(fs, tmpdir)) {
+        for (Path regiondir: FSUtils.getRegionDirs(fs, tabledir)) {
+          HFileArchiver.archiveRegion(c, fs, this.rootdir, tabledir, regiondir);
+        }
+      }
+      if (!fs.delete(tmpdir, true)) {
+        throw new IOException("Unable to clean the temp directory: " + tmpdir);
+      }
+    }
+
+    // Create the temp directory
+    if (!fs.mkdirs(tmpdir)) {
+      throw new IOException("HBase temp directory '" + tmpdir + "' creation failure.");
+    }
+  }
+
   private static void bootstrap(final Path rd, final Configuration c)
   throws IOException {
     LOG.info("BOOTSTRAP: creating ROOT and first META regions");
@@ -450,6 +489,37 @@ public class MasterFileSystem {
     fs.delete(new Path(rootdir, Bytes.toString(tableName)), true);
   }
 
+  /**
+   * Move the specified file/directory to the hbase temp directory.
+   * @param path The path of the file/directory to move
+   * @return The temp location of the file/directory moved
+   * @throws IOException in case of file-system failure
+   */
+  public Path moveToTemp(final Path path) throws IOException {
+    Path tempPath = new Path(this.tempdir, path.getName());
+
+    // Ensure temp exists
+    if (!fs.exists(tempdir) && !fs.mkdirs(tempdir)) {
+      throw new IOException("HBase temp directory '" + tempdir + "' creation failure.");
+    }
+
+    if (!fs.rename(path, tempPath)) {
+      throw new IOException("Unable to move '" + path + "' to temp '" + tempPath + "'");
+    }
+
+    return tempPath;
+  }
+
+  /**
+   * Move the specified table to the hbase temp directory
+   * @param tableName Table name to move
+   * @return The temp location of the table moved
+   * @throws IOException in case of file-system failure
+   */
+  public Path moveTableToTemp(byte[] tableName) throws IOException {
+    return moveToTemp(HTableDescriptor.getTableDir(this.rootdir, tableName));
+  }
+
   public void updateRegionInfo(HRegionInfo region) {
     // TODO implement this.  i think this is currently broken in trunk i don't
     //      see this getting updated.
diff --git a/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java b/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
index 3dfa29d..62ed965 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
@@ -36,6 +36,8 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException;
@@ -140,38 +142,73 @@ public class CreateTableHandler extends EventHandler {
     }
   }
 
-  private void handleCreateTable(String tableName) throws IOException,
-      KeeperException {
-    // TODO: Currently we make the table descriptor and as side-effect the
-    // tableDir is created.  Should we change below method to be createTable
-    // where we create table in tmp dir with its table descriptor file and then
-    // do rename to move it into place?
-    FSTableDescriptors.createTableDescriptor(this.hTableDescriptor, this.conf);
+  /**
+   * Responsible of table creation (on-disk and META) and assignment.
+   * - Create the table directory and descriptor (temp folder)
+   * - Create the on-disk regions (temp folder)
+   *   [If something fails here: we've just some trash in temp]
+   * - Move the table from temp to the root directory
+   *   [If something fails here: we've the table in place but some of the rows required
+   *    present in META. (hbck needed)]
+   * - Add regions to META
+   *   [If something fails here: we don't have regions assigned: table disabled]
+   * - Assign regions to Region Servers
+   *   [If something fails here: we still have the table in disabled state]
+   * - Update ZooKeeper with the enabled state
+   */
+  private void handleCreateTable(String tableName) throws IOException, KeeperException {
+    Path tempdir = fileSystemManager.getTempDir();
+    FileSystem fs = fileSystemManager.getFileSystem();
 
-    List<HRegionInfo> regionInfos = handleCreateRegions(this.hTableDescriptor.getNameAsString());
+    // 1. Create Table Descriptor
+    FSTableDescriptors.createTableDescriptor(fs, tempdir, this.hTableDescriptor);
+    Path tempTableDir = new Path(tempdir, tableName);
+    Path tableDir = new Path(fileSystemManager.getRootDir(), tableName);
 
-    // 4. Trigger immediate assignment of the regions in round-robin fashion
-    List<ServerName> servers = serverManager.getOnlineServersList();
-    // Remove the deadNotExpired servers from the server list.
-    assignmentManager.removeDeadNotExpiredServers(servers);
-    try {
-      this.assignmentManager.assignUserRegions(regionInfos, servers);
-    } catch (InterruptedException ie) {
-      LOG.error("Caught " + ie + " during round-robin assignment");
-      throw new IOException(ie);
+    // 2. Create Regions
+    List<HRegionInfo> regionInfos = handleCreateHdfsRegions(tempdir, tableName);
+
+    // 3. Move Table temp directory to the hbase root location
+    if (!fs.rename(tempTableDir, tableDir)) {
+      throw new IOException("Unable to move table from temp=" + tempTableDir +
+        " to hbase root=" + tableDir);
     }
 
-    // 5. Set table enabled flag up in zk.
+    if (regionInfos != null && regionInfos.size() > 0) {
+      // 4. Add regions to META
+      MetaEditor.addRegionsToMeta(this.catalogTracker, regionInfos);
+
+      // 5. Trigger immediate assignment of the regions in round-robin fashion
+      List<ServerName> servers = serverManager.getOnlineServersList();
+      // Remove the deadNotExpired servers from the server list.
+      assignmentManager.removeDeadNotExpiredServers(servers);
+      try {
+        this.assignmentManager.assignUserRegions(regionInfos, servers);
+      } catch (InterruptedException e) {
+        LOG.error("Caught " + e + " during round-robin assignment");
+        InterruptedIOException ie = new InterruptedIOException(e.getMessage());
+        ie.initCause(e);
+        throw ie;
+      }
+    }
+
+    // 6. Set table enabled flag up in zk.
     try {
-      assignmentManager.getZKTable().
-        setEnabledTable(this.hTableDescriptor.getNameAsString());
+      assignmentManager.getZKTable().setEnabledTable(tableName);
     } catch (KeeperException e) {
       throw new IOException("Unable to ensure that the table will be" +
         " enabled because of a ZooKeeper issue", e);
     }
   }
 
-  protected List<HRegionInfo> handleCreateRegions(String tableName) throws IOException {
+  /**
+   * Create the on-disk structure for the table, and returns the regions info.
+   * @param tableRootDir directory where the table is being created
+   * @param tableName name of the table under construction
+   * @return the list of regions created
+   */
+  protected List<HRegionInfo> handleCreateHdfsRegions(final Path tableRootDir, final String tableName)
+      throws IOException {
     int regionNumber = newRegions.length;
     ThreadPoolExecutor regionOpenAndInitThreadPool = getRegionOpenAndInitThreadPool(
         "RegionOpenAndInitThread-" + tableName, regionNumber);
@@ -185,9 +222,8 @@ public class CreateTableHandler extends EventHandler {
 
           // 1. Create HRegion
           HRegion region = HRegion.createHRegion(newRegion,
-              fileSystemManager.getRootDir(), conf, hTableDescriptor, null,
+              tableRootDir, conf, hTableDescriptor, null,
               false, true);
-
           // 2. Close the new region to flush to disk. Close log file too.
           region.close();
           return region;
@@ -208,9 +244,7 @@ public class CreateTableHandler extends EventHandler {
     } finally {
       regionOpenAndInitThreadPool.shutdownNow();
     }
-    if (regionInfos.size() > 0) {
-      MetaEditor.addRegionsToMeta(this.catalogTracker, regionInfos);
-    }
+
     return regionInfos;
   }
 
diff --git a/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java b/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
index b299bee..f014e05 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
@@ -24,10 +24,14 @@ import java.util.List;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.backup.HFileArchiver;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.master.AssignmentManager;
+import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Threads;
@@ -47,6 +51,7 @@ public class DeleteTableHandler extends TableEventHandler {
   @Override
   protected void handleTableOperation(List<HRegionInfo> regions)
   throws IOException, KeeperException {
+    // 1. Wait because of region in transition
     AssignmentManager am = this.masterServices.getAssignmentManager();
     long waitTime = server.getConfiguration().
       getLong("hbase.master.wait.on.region", 5 * 60 * 1000);
@@ -63,23 +68,33 @@ public class DeleteTableHandler extends TableEventHandler {
           waitTime + "ms) for region to leave region " +
           region.getRegionNameAsString() + " in transitions");
       }
-      LOG.debug("Deleting region " + region.getRegionNameAsString() +
-        " from META and FS");
-      // Remove region from META
-      MetaEditor.deleteRegion(this.server.getCatalogTracker(), region);
-      // Delete region from FS
-      this.masterServices.getMasterFileSystem().deleteRegion(region);
     }
-    // Delete table from FS
-    this.masterServices.getMasterFileSystem().deleteTable(tableName);
-    // Update table descriptor cache
-    this.masterServices.getTableDescriptors().remove(Bytes.toString(tableName));
 
-    // If entry for this table in zk, and up in AssignmentManager, remove it.
+    // 2. Remove regions from META
+    LOG.debug("Deleting regions from META");
+    MetaEditor.deleteRegions(this.server.getCatalogTracker(), regions);
+
+    // 3. Move the table in /hbase/.tmp
+    MasterFileSystem mfs = this.masterServices.getMasterFileSystem();
+    Path tempTableDir = mfs.moveTableToTemp(tableName);
+
+    // 4. Update table descriptor cache
+    this.masterServices.getTableDescriptors().remove(Bytes.toString(tableName));
 
+    // 5. If entry for this table in zk, and up in AssignmentManager, remove it.
     am.getZKTable().setDeletedTable(Bytes.toString(tableName));
+
+    // 6. Delete regions from FS (temp directory)
+    FileSystem fs = mfs.getFileSystem();
+    for (HRegionInfo hri: regions) {
+      LOG.debug("Deleting region " + hri.getRegionNameAsString() + " from FS");
+      HFileArchiver.archiveRegion(masterServices.getConfiguration(), fs, mfs.getRootDir(),
+          tempTableDir, new Path(tempTableDir, hri.getEncodedName()));
+    }
+    // 7. Delete table from FS (temp directory)
+    fs.delete(tempTableDir, true);
   }
-  
+
   @Override
   public String toString() {
     String name = "UnknownServerName";
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
index d2128fc..8490e09 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
@@ -44,6 +44,8 @@ import org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
 import org.apache.hadoop.hbase.util.Bytes;
 
+import com.google.common.base.Preconditions;
+
 /**
  * Handler to Clone a snapshot.
  *
@@ -77,23 +79,36 @@ public class CloneSnapshotHandler extends CreateTableHandler implements Snapshot
     this.monitor = new ForeignExceptionDispatcher();
   }
 
+  /**
+   * Create the on-disk regions, using the tableRootDir provided by the CreateTableHandler.
+   * The cloned table will be created in a temp directory, and then the CreateTableHandler
+   * will be responsible to add the regions returned by this method to META and do the assignment.
+   */
   @Override
-  protected List<HRegionInfo> handleCreateRegions(String tableName) throws IOException {
+  protected List<HRegionInfo> handleCreateHdfsRegions(final Path tableRootDir, final String tableName)
+      throws IOException {
     FileSystem fs = fileSystemManager.getFileSystem();
     Path rootDir = fileSystemManager.getRootDir();
-    Path tableDir = HTableDescriptor.getTableDir(rootDir, Bytes.toBytes(tableName));
+    Path tableDir = new Path(tableRootDir, tableName);
 
     try {
-      // Execute the Clone
+      // 1. Execute the on-disk Clone
       Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshot, rootDir);
       RestoreSnapshotHelper restoreHelper = new RestoreSnapshotHelper(conf, fs,
-          catalogTracker, snapshot, snapshotDir, hTableDescriptor, tableDir, monitor);
-      restoreHelper.restore();
+          snapshot, snapshotDir, hTableDescriptor, tableDir, monitor);
+      RestoreSnapshotHelper.RestoreMetaChanges metaChanges = restoreHelper.restoreHdfsRegions();
+
+      // Clone operation should not have stuff to restore or remove
+      Preconditions.checkArgument(metaChanges.getRegionsToRestore() == null,
+          "A clone should not have regions to restore");
+      Preconditions.checkArgument(metaChanges.getRegionsToRemove() == null,
+          "A clone should not have regions to remove");
 
       // At this point the clone is complete. Next step is enabling the table.
       LOG.info("Clone snapshot=" + snapshot.getName() + " on table=" + tableName + " completed!");
 
-      return MetaReader.getTableRegions(catalogTracker, Bytes.toBytes(tableName));
+      // 2. let the CreateTableHandler add the regions to meta
+      return metaChanges.getRegionsToAdd();
     } catch (Exception e) {
       String msg = "clone snapshot=" + snapshot + " failed";
       LOG.error(msg, e);
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java
index 795b14e..a58bb52 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java
@@ -31,6 +31,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
+import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.errorhandling.ForeignException;
 import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
@@ -78,6 +79,13 @@ public class RestoreSnapshotHandler extends TableEventHandler implements Snapsho
     this.hTableDescriptor = htd;
   }
 
+  /**
+   * The restore table is executed in place.
+   *  - The on-disk data will be restored
+   *  - [if something fail here: you need to delete the table and re-run the restore]
+   *  - META will be updated
+   *  - [if something fail here: you need to run hbck to fix META entries]
+   */
   @Override
   protected void handleTableOperation(List<HRegionInfo> hris) throws IOException {
     MasterFileSystem fileSystemManager = masterServices.getMasterFileSystem();
@@ -88,25 +96,29 @@ public class RestoreSnapshotHandler extends TableEventHandler implements Snapsho
     Path tableDir = HTableDescriptor.getTableDir(rootDir, tableName);
 
     try {
-      // Update descriptor
+      // 1. Update descriptor
       this.masterServices.getTableDescriptors().add(hTableDescriptor);
 
-      // Execute the Restore
+      // 2. Execute the on-disk Restore
       LOG.debug("Starting restore snapshot=" + snapshot);
       Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshot, rootDir);
       RestoreSnapshotHelper restoreHelper = new RestoreSnapshotHelper(
-          masterServices.getConfiguration(), fs, catalogTracker,
+          masterServices.getConfiguration(), fs,
           snapshot, snapshotDir, hTableDescriptor, tableDir, monitor);
-      restoreHelper.restore();
+      RestoreSnapshotHelper.RestoreMetaChanges metaChanges = restoreHelper.restoreHdfsRegions();
+
+      // 3. Applies changes to .META.
+      hris.clear();
+      hris.addAll(metaChanges.getRegionsToAdd());
+      hris.addAll(metaChanges.getRegionsToRestore());
+      List<HRegionInfo> hrisToRemove = metaChanges.getRegionsToRemove();
+      MetaEditor.mutateRegions(catalogTracker, hrisToRemove, hris);
 
       // At this point the restore is complete. Next step is enabling the table.
       LOG.info("Restore snapshot=" + snapshot.getName() + " on table=" +
         Bytes.toString(tableName) + " completed!");
-
-      hris.clear();
-      hris.addAll(MetaReader.getTableRegions(catalogTracker, tableName));
     } catch (IOException e) {
-      String msg = "restore snapshot=" + snapshot + " failed";
+      String msg = "restore snapshot=" + snapshot + " failed. re-run the restore command.";
       LOG.error(msg, e);
       monitor.receive(new ForeignException(masterServices.getServerName().toString(), e));
       throw new RestoreSnapshotException(msg, e);
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java b/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
index d4ba669..be3d8bd 100644
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
@@ -21,6 +21,7 @@ package org.apache.hadoop.hbase.snapshot;
 import java.io.InputStream;
 import java.io.IOException;
 import java.io.OutputStream;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedList;
@@ -40,7 +41,6 @@ import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.backup.HFileArchiver;
-import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
 import org.apache.hadoop.hbase.io.HFileLink;
@@ -110,19 +110,16 @@ public class RestoreSnapshotHelper {
   private final HTableDescriptor tableDesc;
   private final Path tableDir;
 
-  private final CatalogTracker catalogTracker;
   private final Configuration conf;
   private final FileSystem fs;
 
   public RestoreSnapshotHelper(final Configuration conf, final FileSystem fs,
-      final CatalogTracker catalogTracker,
       final SnapshotDescription snapshotDescription, final Path snapshotDir,
       final HTableDescriptor tableDescriptor, final Path tableDir,
       final ForeignExceptionDispatcher monitor)
   {
     this.fs = fs;
     this.conf = conf;
-    this.catalogTracker = catalogTracker;
     this.snapshotDesc = snapshotDescription;
     this.snapshotDir = snapshotDir;
     this.tableDesc = tableDescriptor;
@@ -131,45 +128,45 @@ public class RestoreSnapshotHelper {
   }
 
   /**
-   * Restore table to a specified snapshot state.
+   * Restore the on-disk table to a specified snapshot state.
+   * @return the set of regions touched by the restore operation
    */
-  public void restore() throws IOException {
+  public RestoreMetaChanges restoreHdfsRegions() throws IOException {
     long startTime = EnvironmentEdgeManager.currentTimeMillis();
 
     LOG.debug("starting restore");
     Set<String> snapshotRegionNames = SnapshotReferenceUtil.getSnapshotRegionNames(fs, snapshotDir);
     if (snapshotRegionNames == null) {
       LOG.warn("Nothing to restore. Snapshot " + snapshotDesc + " looks empty");
-      return;
+      return null;
     }
 
+    RestoreMetaChanges metaChanges = new RestoreMetaChanges();
+
     // Identify which region are still available and which not.
     // NOTE: we rely upon the region name as: "table name, start key, end key"
     List<HRegionInfo> tableRegions = getTableRegions();
     if (tableRegions != null) {
       monitor.rethrowException();
-      List<HRegionInfo> regionsToRestore = new LinkedList<HRegionInfo>();
-      List<HRegionInfo> regionsToRemove = new LinkedList<HRegionInfo>();
-
       for (HRegionInfo regionInfo: tableRegions) {
         String regionName = regionInfo.getEncodedName();
         if (snapshotRegionNames.contains(regionName)) {
           LOG.info("region to restore: " + regionName);
           snapshotRegionNames.remove(regionInfo);
-          regionsToRestore.add(regionInfo);
+          metaChanges.addRegionToRestore(regionInfo);
         } else {
           LOG.info("region to remove: " + regionName);
-          regionsToRemove.add(regionInfo);
+          metaChanges.addRegionToRemove(regionInfo);
         }
       }
 
       // Restore regions using the snapshot data
       monitor.rethrowException();
-      restoreRegions(regionsToRestore);
+      restoreHdfsRegions(metaChanges.getRegionsToRestore());
 
       // Remove regions from the current table
       monitor.rethrowException();
-      ModifyRegionUtils.deleteRegions(conf, fs, catalogTracker, regionsToRemove);
+      removeHdfsRegions(metaChanges.getRegionsToRemove());
     }
 
     // Regions to Add: present in the snapshot but not in the current table
@@ -185,18 +182,92 @@ public class RestoreSnapshotHelper {
 
       // Create new regions cloning from the snapshot
       monitor.rethrowException();
-      cloneRegions(regionsToAdd);
+      HRegionInfo[] clonedRegions = cloneHdfsRegions(regionsToAdd);
+      metaChanges.setNewRegions(clonedRegions);
     }
 
     // Restore WALs
     monitor.rethrowException();
     restoreWALs();
+
+    return metaChanges;
+  }
+
+  /**
+   * Describe the set of operations needed to update META after restore.
+   */
+  public class RestoreMetaChanges {
+    private List<HRegionInfo> regionsToRestore = null;
+    private List<HRegionInfo> regionsToRemove = null;
+    private List<HRegionInfo> regionsToAdd = null;
+
+    /**
+     * Returns the list of new regions added during the on-disk restore.
+     * The caller is responsible to add the regions to META.
+     * e.g MetaEditor.addRegionsToMeta(...)
+     * @return the list of regions to add to META
+     */
+    public List<HRegionInfo> getRegionsToAdd() {
+      return this.regionsToAdd;
+    }
+
+    /**
+     * Returns the list of 'restored regions' during the on-disk restore.
+     * The caller is responsible to add the regions to META if not present.
+     * @return the list of regions restored
+     */
+    public List<HRegionInfo> getRegionsToRestore() {
+      return this.regionsToRestore;
+    }
+
+    /**
+     * Returns the list of regions removed during the on-disk restore.
+     * The caller is responsible to remove the regions from META.
+     * e.g. MetaEditor.deleteRegions(...)
+     * @return the list of regions to remove from META
+     */
+    public List<HRegionInfo> getRegionsToRemove() {
+      return this.regionsToRemove;
+    }
+
+    void setNewRegions(final HRegionInfo[] hris) {
+      if (hris != null) {
+        regionsToAdd = Arrays.asList(hris);
+      } else {
+        regionsToAdd = null;
+      }
+    }
+
+    void addRegionToRemove(final HRegionInfo hri) {
+      if (regionsToRemove == null) {
+        regionsToRemove = new LinkedList<HRegionInfo>();
+      }
+      regionsToRemove.add(hri);
+    }
+
+    void addRegionToRestore(final HRegionInfo hri) {
+      if (regionsToRestore == null) {
+        regionsToRestore = new LinkedList<HRegionInfo>();
+      }
+      regionsToRestore.add(hri);
+    }
+  }
+
+  /**
+   * Remove specified regions from the file-system, using the archiver.
+   */
+  private void removeHdfsRegions(final List<HRegionInfo> regions) throws IOException {
+    if (regions != null && regions.size() > 0) {
+      for (HRegionInfo hri: regions) {
+        HFileArchiver.archiveRegion(conf, fs, hri);
+      }
+    }
   }
 
   /**
    * Restore specified regions by restoring content to the snapshot state.
    */
-  private void restoreRegions(final List<HRegionInfo> regions) throws IOException {
+  private void restoreHdfsRegions(final List<HRegionInfo> regions) throws IOException {
     if (regions == null || regions.size() == 0) return;
     for (HRegionInfo hri: regions) restoreRegion(hri);
   }
@@ -289,8 +360,8 @@ public class RestoreSnapshotHelper {
    * Clone specified regions. For each region create a new region
    * and create a HFileLink for each hfile.
    */
-  private void cloneRegions(final List<HRegionInfo> regions) throws IOException {
-    if (regions == null || regions.size() == 0) return;
+  private HRegionInfo[] cloneHdfsRegions(final List<HRegionInfo> regions) throws IOException {
+    if (regions == null || regions.size() == 0) return null;
 
     final Map<String, HRegionInfo> snapshotRegions =
       new HashMap<String, HRegionInfo>(regions.size());
@@ -313,16 +384,14 @@ public class RestoreSnapshotHelper {
     }
 
     // create the regions on disk
-    List<HRegionInfo> clonedRegions = ModifyRegionUtils.createRegions(conf, FSUtils.getRootDir(conf),
-      tableDesc, clonedRegionsInfo, catalogTracker, new ModifyRegionUtils.RegionFillTask() {
+    List<HRegionInfo> clonedRegions = ModifyRegionUtils.createRegions(conf, tableDir.getParent(),
+      tableDesc, clonedRegionsInfo, new ModifyRegionUtils.RegionFillTask() {
         public void fillRegion(final HRegion region) throws IOException {
           cloneRegion(region, snapshotRegions.get(region.getRegionInfo().getEncodedName()));
         }
       });
-    if (regions != null && regions.size() > 0) {
-      // add regions to .META.
-      MetaEditor.addRegionsToMeta(catalogTracker, clonedRegions);
-    }
+
+    return clonedRegionsInfo;
   }
 
   /**
@@ -386,7 +455,7 @@ public class RestoreSnapshotHelper {
    *   wxyz/table=1234-abc
    *   stuv/table=1234-abc.wxyz
    *
-   * NOTE that the region name in the clone change (md5 of regioninfo)
+   * NOTE that the region name in the clone changes (md5 of regioninfo)
    * and the reference should reflect that change.
    * </pre></blockquote>
    * @param familyDir destination directory for the store file
diff --git a/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java b/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
index 5f2cfe6..92a2408 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
@@ -100,6 +100,24 @@ public class HFileArchiveUtil {
   }
 
   /**
+   * Get the archive directory for a given region under the specified table
+   * @param rootdir {@link Path} to the root directory where hbase files are stored (for building
+   *          the archive path)
+   * @param tabledir the original table directory. Cannot be null.
+   * @param regiondir the path to the region directory. Cannot be null.
+   * @return {@link Path} to the directory to archive the given region, or <tt>null</tt> if it
+   *         should not be archived
+   */
+  public static Path getRegionArchiveDir(Path rootdir, Path tabledir, Path regiondir) {
+    // get the archive directory for a table
+    Path archiveDir = getTableArchivePath(rootdir, tabledir.getName());
+
+    // then add on the region path under the archive
+    String encodedRegionName = regiondir.getName();
+    return HRegion.getRegionDir(archiveDir, encodedRegionName);
+  }
+
+  /**
    * Get the path to the table archive directory based on the configured archive directory.
    * <p>
    * Get the path to the table's archive directory.
@@ -110,7 +128,22 @@ public class HFileArchiveUtil {
    */
   public static Path getTableArchivePath(Path tabledir) {
     Path root = tabledir.getParent();
-    return new Path(new Path(root,HConstants.HFILE_ARCHIVE_DIRECTORY), tabledir.getName());
+    return getTableArchivePath(root, tabledir.getName());
+  }
+
+  /**
+   * Get the path to the table archive directory based on the configured archive directory.
+   * <p>
+   * Get the path to the table's archive directory.
+   * <p>
+   * Generally of the form: /hbase/.archive/[tablename]
+   * @param rootdir {@link Path} to the root directory where hbase files are stored (for building
+   *          the archive path)
+   * @param tableName Name of the table to be archived. Cannot be null.
+   * @return {@link Path} to the archive directory for the table
+   */
+  public static Path getTableArchivePath(final Path rootdir, final String tableName) {
+    return new Path(getArchivePath(rootdir), tableName);
   }
 
   /**
@@ -134,6 +167,16 @@ public class HFileArchiveUtil {
    * @throws IOException if an unexpected error occurs
    */
   public static Path getArchivePath(Configuration conf) throws IOException {
-    return new Path(FSUtils.getRootDir(conf), HConstants.HFILE_ARCHIVE_DIRECTORY);
+    return getArchivePath(FSUtils.getRootDir(conf));
+  }
+
+  /**
+   * Get the full path to the archive directory on the configured {@link FileSystem}
+   * @param rootdir {@link Path} to the root directory where hbase files are stored (for building
+   *          the archive path)
+   * @return the full {@link Path} to the archive directory, as defined by the configuration
+   */
+  private static Path getArchivePath(final Path rootdir) {
+    return new Path(rootdir, HConstants.HFILE_ARCHIVE_DIRECTORY);
   }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java b/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java
index 33a88eb..7845859 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java
@@ -70,13 +70,11 @@ public abstract class ModifyRegionUtils {
    * @param rootDir Root directory for HBase instance
    * @param hTableDescriptor description of the table
    * @param newRegions {@link HRegionInfo} that describes the regions to create
-   * @param catalogTracker the catalog tracker
    * @throws IOException
    */
   public static List<HRegionInfo> createRegions(final Configuration conf, final Path rootDir,
-      final HTableDescriptor hTableDescriptor, final HRegionInfo[] newRegions,
-      final CatalogTracker catalogTracker) throws IOException {
-    return createRegions(conf, rootDir, hTableDescriptor, newRegions, catalogTracker, null);
+      final HTableDescriptor hTableDescriptor, final HRegionInfo[] newRegions) throws IOException {
+    return createRegions(conf, rootDir, hTableDescriptor, newRegions, null);
   }
 
   /**
@@ -87,13 +85,12 @@ public abstract class ModifyRegionUtils {
    * @param rootDir Root directory for HBase instance
    * @param hTableDescriptor description of the table
    * @param newRegions {@link HRegionInfo} that describes the regions to create
-   * @param catalogTracker the catalog tracker
    * @param task {@link RegionFillTask} custom code to populate region after creation
    * @throws IOException
    */
   public static List<HRegionInfo> createRegions(final Configuration conf, final Path rootDir,
       final HTableDescriptor hTableDescriptor, final HRegionInfo[] newRegions,
-      final CatalogTracker catalogTracker, final RegionFillTask task) throws IOException {
+      final RegionFillTask task) throws IOException {
     if (newRegions == null) return null;
     int regionNumber = newRegions.length;
     ThreadPoolExecutor regionOpenAndInitThreadPool = getRegionOpenAndInitThreadPool(conf,
@@ -108,6 +105,8 @@ public abstract class ModifyRegionUtils {
           HRegion region = HRegion.createHRegion(newRegion,
               rootDir, conf, hTableDescriptor, null,
               false, true);
+          HRegion.writeRegioninfoOnFilesystem(region.getRegionInfo(), region.getRegionDir(),
+            region.getFilesystem(), conf);
           try {
             // 2. Custom user code to interact with the created region
             if (task != null) {
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java
index 3218ab8..e5cb149 100644
--- a/src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java
@@ -131,7 +131,7 @@ public class TestRestoreSnapshotHelper {
 
     FSTableDescriptors.createTableDescriptor(htdClone, conf);
     RestoreSnapshotHelper helper = getRestoreHelper(rootDir, snapshotDir, sourceTableName, htdClone);
-    helper.restore();
+    helper.restoreHdfsRegions();
 
     LOG.debug("post-restore table=" + htdClone.getNameAsString() + " snapshot=" + snapshotDir);
     FSUtils.logFileSystemState(fs, rootDir, LOG);
@@ -146,13 +146,10 @@ public class TestRestoreSnapshotHelper {
     HTableDescriptor tableDescriptor = Mockito.mock(HTableDescriptor.class);
     ForeignExceptionDispatcher monitor = Mockito.mock(ForeignExceptionDispatcher.class);
 
-    HConnection hconnection = HConnectionTestingUtility.getMockedConnection(conf);
-    Mockito.when(catalogTracker.getConnection()).thenReturn(hconnection);
-
     SnapshotDescription sd = SnapshotDescription.newBuilder()
       .setName("snapshot").setTable(sourceTableName).build();
 
-    return new RestoreSnapshotHelper(conf, fs, catalogTracker, sd, snapshotDir,
+    return new RestoreSnapshotHelper(conf, fs, sd, snapshotDir,
       htdClone, HTableDescriptor.getTableDir(rootDir, htdClone.getName()), monitor);
   }
 
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestHFileArchiveUtil.java b/src/test/java/org/apache/hadoop/hbase/util/TestHFileArchiveUtil.java
index 9175bc2..82f96d9 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/TestHFileArchiveUtil.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestHFileArchiveUtil.java
@@ -50,9 +50,10 @@ public class TestHFileArchiveUtil {
   
   @Test
   public void testRegionArchiveDir() {
+    Configuration conf = null;
     Path tableDir = new Path("table");
     Path regionDir = new Path("region");
-    assertNotNull(HFileArchiveUtil.getRegionArchiveDir(null, tableDir, regionDir));
+    assertNotNull(HFileArchiveUtil.getRegionArchiveDir(conf, tableDir, regionDir));
   }
   
   @Test
-- 
1.7.0.4

