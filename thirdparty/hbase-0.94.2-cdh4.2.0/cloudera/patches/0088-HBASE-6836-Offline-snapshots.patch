From 54795e302a037d32c2da7dd20d8c981041aa3304 Mon Sep 17 00:00:00 2001
From: Matteo Bertozzi <matteo.bertozzi@cloudera.com>
Date: Wed, 19 Dec 2012 21:56:05 +0000
Subject: [PATCH 088/202] HBASE-6836 Offline snapshots

Reason: Snapshots
Author: Jesse Yates
Ref: CDH-9551
---
 .../java/org/apache/hadoop/hbase/HConstants.java   |    9 +-
 .../org/apache/hadoop/hbase/client/HBaseAdmin.java |   26 +-
 .../apache/hadoop/hbase/executor/EventHandler.java |    1 +
 .../hadoop/hbase/executor/ExecutorService.java     |    1 +
 .../org/apache/hadoop/hbase/master/HMaster.java    |  217 +++++++++++-
 .../hadoop/hbase/master/SnapshotSentinel.java      |   52 +++
 .../snapshot/DisabledTableSnapshotHandler.java     |  236 ++++++++++++
 .../master/snapshot/MasterSnapshotVerifier.java    |  253 +++++++++++++
 .../master/snapshot/manage/SnapshotManager.java    |  182 ++++++++++
 .../apache/hadoop/hbase/regionserver/HRegion.java  |   41 ++-
 .../errorhandling/impl/ExceptionOrchestrator.java  |    2 +-
 .../hbase/server/snapshot/TakeSnapshotUtils.java   |  325 +++++++++++++++++
 .../snapshot/error/SnapshotExceptionSnare.java     |   71 ++++
 .../snapshot/error/SnapshotFailureListener.java    |   41 +++
 .../snapshot/task/CopyRecoveredEditsTask.java      |   89 +++++
 .../snapshot/task/ReferenceRegionHFilesTask.java   |  126 +++++++
 .../snapshot/task/ReferenceServerWALsTask.java     |  103 ++++++
 .../hbase/server/snapshot/task/SnapshotTask.java   |   81 +++++
 .../server/snapshot/task/TableInfoCopyTask.java    |   75 ++++
 .../hbase/snapshot/HBaseSnapshotException.java     |   61 ----
 .../hbase/snapshot/SnapshotCreationException.java  |   51 ---
 .../hbase/snapshot/SnapshotDescriptionUtils.java   |  326 +++++++++++++++++-
 .../hbase/snapshot/UnknownSnapshotException.java   |   29 --
 .../exception/CorruptedSnapshotException.java      |   49 +++
 .../snapshot/exception/HBaseSnapshotException.java |   77 ++++
 .../exception/SnapshotCreationException.java       |   54 +++
 .../exception/SnapshotDoesNotExistException.java   |   40 ++
 .../exception/SnapshotExistsException.java         |   40 ++
 .../exception/TablePartiallyOpenException.java     |   51 +++
 .../exception/UnexpectedSnapshotException.java     |   42 +++
 .../exception/UnknownSnapshotException.java        |   38 ++
 .../hadoop/hbase/util/FSTableDescriptors.java      |   24 +-
 .../hadoop/hbase/client/TestSnapshotFromAdmin.java |  151 ++++++++
 .../hbase/client/TestSnapshotFromClient.java       |  200 ++++++++++
 .../master/cleaner/TestSnapshotFromMaster.java     |  380 ++++++++++++++++++++
 .../snapshot/manage/TestSnapshotManager.java       |  132 +++++++
 .../snapshot/error/TestSnapshotExceptionSnare.java |   74 ++++
 .../snapshot/task/TestCopyRecoveredEditsTask.java  |  128 +++++++
 .../task/TestReferenceRegionHFilesTask.java        |   92 +++++
 .../server/snapshot/task/TestSnapshotTask.java     |   52 +++
 .../server/snapshot/task/TestWALReferenceTask.java |   99 +++++
 .../hbase/snapshot/SnapshotTestingUtils.java       |  187 ++++++++++
 .../snapshot/TestSnapshotDescriptionUtils.java     |  138 +++++++
 43 files changed, 4267 insertions(+), 179 deletions(-)
 create mode 100644 src/main/java/org/apache/hadoop/hbase/master/SnapshotSentinel.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotExceptionSnare.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotFailureListener.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java
 delete mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java
 create mode 100644 src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromAdmin.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/error/TestSnapshotExceptionSnare.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
 create mode 100644 src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotDescriptionUtils.java

diff --git a/src/main/java/org/apache/hadoop/hbase/HConstants.java b/src/main/java/org/apache/hadoop/hbase/HConstants.java
index d51c22f..b974f27 100644
--- a/src/main/java/org/apache/hadoop/hbase/HConstants.java
+++ b/src/main/java/org/apache/hadoop/hbase/HConstants.java
@@ -661,10 +661,17 @@ public final class HConstants {
   /** Directory under /hbase where archived hfiles are stored */
   public static final String HFILE_ARCHIVE_DIRECTORY = ".archive";
 
+  /**
+   * Name of the directory to store snapshots all snapshots. See SnapshotDescriptionUtils for
+   * remaining snapshot constants; this is here to keep HConstants dependencies at a minimum and
+   * uni-directional.
+   */
+  public static final String SNAPSHOT_DIR_NAME = ".snapshot";
+
   public static final List<String> HBASE_NON_USER_TABLE_DIRS = new ArrayList<String>(
       Arrays.asList(new String[] { HREGION_LOGDIR_NAME, HREGION_OLDLOGDIR_NAME, CORRUPT_DIR_NAME,
           Bytes.toString(META_TABLE_NAME), Bytes.toString(ROOT_TABLE_NAME), SPLIT_LOGDIR_NAME,
-          HBCK_SIDELINEDIR_NAME, HFILE_ARCHIVE_DIRECTORY }));
+          HBCK_SIDELINEDIR_NAME, HFILE_ARCHIVE_DIRECTORY, SNAPSHOT_DIR_NAME }));
   
   private HConstants() {
     // Can't be instantiated with this ctor.
diff --git a/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java b/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index 42368de..e3f3725 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -60,9 +60,9 @@ import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.CompactionState;
 import org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException;
-import org.apache.hadoop.hbase.snapshot.HBaseSnapshotException;
-import org.apache.hadoop.hbase.snapshot.SnapshotCreationException;
-import org.apache.hadoop.hbase.snapshot.UnknownSnapshotException;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
+import org.apache.hadoop.hbase.snapshot.exception.UnknownSnapshotException;
 import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
 import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
 import org.apache.hadoop.hbase.util.Addressing;
@@ -93,7 +93,7 @@ public class HBaseAdmin implements Abortable, Closeable {
   // want to wait a long time.
   private final int retryLongerMultiplier;
   private boolean aborted;
-  
+
   /**
    * Constructor
    *
@@ -195,7 +195,7 @@ public class HBaseAdmin implements Abortable, Closeable {
     this.aborted = true;
     throw new RuntimeException(why, e);
   }
-  
+
   @Override
   public boolean isAborted(){
     return this.aborted;
@@ -602,7 +602,7 @@ public class HBaseAdmin implements Abortable, Closeable {
         // continue
       }
     }
-    
+
     if (tableExists) {
       throw new IOException("Retries exhausted, it took too long to wait"+
         " for the table " + Bytes.toString(tableName) + " to be deleted.");
@@ -1120,7 +1120,7 @@ public class HBaseAdmin implements Abortable, Closeable {
    * servername is provided then based on the online regions in the specified
    * regionserver the specified region will be closed. The master will not be
    * informed of the close. Note that the regionname is the encoded regionname.
-   * 
+   *
    * @param encodedRegionName
    *          The encoded region name; i.e. the hash that makes up the region
    *          name suffix: e.g. if regionname is
@@ -1256,7 +1256,7 @@ public class HBaseAdmin implements Abortable, Closeable {
   throws IOException, InterruptedException {
     compact(tableNameOrRegionName, null, false);
   }
-  
+
   /**
    * Compact a column family within a table or region.
    * Asynchronous operation.
@@ -1310,7 +1310,7 @@ public class HBaseAdmin implements Abortable, Closeable {
   throws IOException, InterruptedException {
     compact(tableNameOrRegionName, null, true);
   }
-  
+
   /**
    * Major compact a column family within a table or region.
    * Asynchronous operation.
@@ -1751,7 +1751,7 @@ public class HBaseAdmin implements Abortable, Closeable {
    * @param tableName the name of the table
    * @return Ordered list of {@link HRegionInfo}.
    * @throws IOException
-   */  
+   */
   public List<HRegionInfo> getTableRegions(final byte[] tableName)
   throws IOException {
     CatalogTracker ct = getCatalogTracker();
@@ -1763,7 +1763,7 @@ public class HBaseAdmin implements Abortable, Closeable {
     }
     return Regions;
   }
-  
+
   public void close() throws IOException {
     if (this.connection != null) {
       this.connection.close();
@@ -1783,14 +1783,14 @@ public class HBaseAdmin implements Abortable, Closeable {
 
   /**
    * Roll the log writer. That is, start writing log messages to a new file.
-   * 
+   *
    * @param serverName
    *          The servername of the regionserver. A server name is made of host,
    *          port and startcode. This is mandatory. Here is an example:
    *          <code> host187.example.com,60020,1289493121758</code>
    * @return If lots of logs, flush the returned regions so next time through
    * we can clean logs. Returns null if nothing to flush.  Names are actual
-   * region names as returned by {@link HRegionInfo#getEncodedName()}  
+   * region names as returned by {@link HRegionInfo#getEncodedName()}
    * @throws IOException if a remote or network exception occurs
    * @throws FailedLogCloseException
    */
diff --git a/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java b/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java
index c9acee3..0c9cc6a 100644
--- a/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java
+++ b/src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java
@@ -128,6 +128,7 @@ public abstract class EventHandler implements Runnable, Comparable<Runnable> {
     C_M_DELETE_FAMILY         (45),   // Client asking Master to delete family of table
     C_M_MODIFY_FAMILY         (46),   // Client asking Master to modify family of table
     C_M_CREATE_TABLE          (47),   // Client asking Master to create a table
+    C_M_SNAPSHOT_TABLE        (48),   // Client asking Master to snapshot an offline table
 
     // Updates from master to ZK. This is done by the master and there is
     // nothing to process by either Master or RS
diff --git a/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java b/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
index 012dc0c..371560f 100644
--- a/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
+++ b/src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
@@ -136,6 +136,7 @@ public class ExecutorService {
       case C_M_ENABLE_TABLE:
       case C_M_MODIFY_TABLE:
       case C_M_CREATE_TABLE:
+      case C_M_SNAPSHOT_TABLE:
         return ExecutorType.MASTER_TABLE_OPERATIONS;
 
       // RegionServer executor services
diff --git a/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index f0d357f..e0914d6 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -19,6 +19,7 @@
  */
 package org.apache.hadoop.hbase.master;
 
+import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
@@ -44,6 +45,9 @@ import javax.management.ObjectName;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Chore;
 import org.apache.hadoop.hbase.ClusterStatus;
@@ -88,15 +92,25 @@ import org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler;
 import org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler;
 import org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler;
 import org.apache.hadoop.hbase.master.metrics.MasterMetrics;
+import org.apache.hadoop.hbase.master.snapshot.manage.SnapshotManager;
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.replication.regionserver.Replication;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotDoesNotExistException;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotExistsException;
+import org.apache.hadoop.hbase.snapshot.exception.TablePartiallyOpenException;
+import org.apache.hadoop.hbase.snapshot.exception.UnknownSnapshotException;
 import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
+import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.HFileArchiveUtil;
 import org.apache.hadoop.hbase.util.HasThread;
 import org.apache.hadoop.hbase.util.InfoServer;
@@ -224,6 +238,9 @@ Server {
    */
   private ObjectName mxBean = null;
 
+  // monitor for snapshot of hbase tables
+  private SnapshotManager snapshotManager;
+
   /**
    * Initializes the HMaster. The steps are as follows:
    * <p>
@@ -383,6 +400,7 @@ Server {
       if (this.serverManager != null) this.serverManager.stop();
       if (this.assignmentManager != null) this.assignmentManager.stop();
       if (this.fileSystemManager != null) this.fileSystemManager.stop();
+      if (this.snapshotManager != null) this.snapshotManager.stop("server shutting down.");
       this.zooKeeper.close();
     }
     LOG.info("HMaster main thread exiting");
@@ -445,6 +463,10 @@ Server {
         ", sessionid=0x" +
         Long.toHexString(this.zooKeeper.getRecoverableZooKeeper().getSessionId()) +
         ", cluster-up flag was=" + wasUp);
+
+    // create the snapshot monitor
+    // TODO should this be config based?
+    this.snapshotManager = new SnapshotManager(this, zooKeeper, this.executorService);
   }
 
   // Check if we should stop every second.
@@ -1843,27 +1865,200 @@ Server {
     return this.hfileCleaner;
   }
 
+  /**
+   * Exposed for TESTING!
+   * @return the underlying snapshot manager
+   */
+  SnapshotManager getSnapshotManagerForTesting() {
+    return this.snapshotManager;
+   }
+
   @Override
-  public long snapshot(final HSnapshotDescription snapshot) throws IOException {
-    throw new IOException(new UnsupportedOperationException(
-        "Snapshots are not implemented yet."));
+  public long snapshot(final HSnapshotDescription request) throws IOException {
+    LOG.debug("Starting snapshot for:" + request.getProto());
+
+    // get the snapshot information
+    SnapshotDescription snapshot = SnapshotDescriptionUtils.validate(request.getProto(),
+      this.conf);
+
+    // check to see if we already completed the snapshot
+    if (isSnapshotCompleted(snapshot)) {
+      throw new SnapshotExistsException("Snapshot '" + snapshot.getName() +
+           "' already stored on the filesystem.", snapshot);
+    }
+
+    LOG.debug("No existing snapshot, attempting snapshot...");
+
+    // check to see if the table exists
+    HTableDescriptor desc = null;
+    try {
+      desc = this.tableDescriptors.get(snapshot.getTable());
+    } catch (FileNotFoundException e) {
+      String msg = "Table:" + snapshot.getTable() + " info doesn't exist!";
+      LOG.error(msg);
+      throw new SnapshotCreationException(msg, e, snapshot);
+    } catch (IOException e) {
+      throw new SnapshotCreationException(
+          "Error while geting table description for table " + snapshot.getTable(), e, snapshot);
+    }
+    if (desc == null) {
+      throw new SnapshotCreationException("Table '" + snapshot.getTable() +
+           "' doesn't exist, can't take snapshot.", snapshot);
+    }
+
+    // set the snapshot version, now that we are ready to take it
+    snapshot = snapshot.toBuilder().setVersion(SnapshotDescriptionUtils.SNAPSHOT_LAYOUT_VERSION)
+        .build();
+
+    // if the table is enabled, then have the RS run actually the snapshot work
+    if (this.assignmentManager.getZKTable().isEnabledTable(snapshot.getTable())) {
+      LOG.debug("Table enabled, starting distributed snapshot.");
+      throw new UnsupportedOperationException(
+          "Enabled table snapshots are not yet supported");
+    }
+    // For disabled table, snapshot is created by the master
+    else if (this.assignmentManager.getZKTable().isDisabledTable(snapshot.getTable())) {
+      LOG.debug("Table is disabled, running snapshot entirely on master.");
+      snapshotManager.snapshotDisabledTable(snapshot);
+
+      LOG.debug("Started snapshot: " + snapshot);
+    } else {
+      LOG.error("Can't snapshot table '" + snapshot.getTable() +
+           "', isn't open or closed, we don't know what to do!");
+      throw new SnapshotCreationException(
+          "Table is not entirely open or closed", new TablePartiallyOpenException(
+              snapshot.getTable() + " isn't fully open."), snapshot);
+    }
+    // send back the max amount of time the client should wait for the snapshot to complete
+    long waitTime = SnapshotDescriptionUtils.getMaxMasterTimeout(conf, snapshot.getType(),
+      SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME);
+    return waitTime;
   }
 
   @Override
   public List<HSnapshotDescription> listSnapshots() throws IOException {
-     throw new IOException(new UnsupportedOperationException(
-        "Snapshots are not implemented yet."));
+    List<HSnapshotDescription> availableSnapshots = new ArrayList<HSnapshotDescription>();
+
+    // first create the snapshot description and check to see if it exists
+    Path snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(this.getMasterFileSystem()
+        .getRootDir());
+
+    // if there are no snapshots, return an empty list
+    if (!this.getMasterFileSystem().getFileSystem().exists(snapshotDir)) {
+      return availableSnapshots;
+    }
+
+    FileSystem fs = this.getMasterFileSystem().getFileSystem();
+
+    // ignore all the snapshots in progress
+    FileStatus[] snapshots = fs.listStatus(snapshotDir,
+      new SnapshotDescriptionUtils.CompletedSnaphotDirectoriesFilter(fs));
+    // look through all the completed snapshots
+    for (FileStatus snapshot : snapshots) {
+      Path info = new Path(snapshot.getPath(), SnapshotDescriptionUtils.SNAPSHOTINFO_FILE);
+      // if the snapshot is bad
+      if (!fs.exists(info)) {
+        LOG.error("Snapshot information for " + snapshot.getPath() + " doesn't exist");
+        continue;
+      }
+      FSDataInputStream in = null;
+      try {
+        in = fs.open(info);
+        SnapshotDescription desc = SnapshotDescription.parseFrom(in);
+        availableSnapshots.add(new HSnapshotDescription(desc));
+      } catch (IOException e) {
+        LOG.warn("Found a corrupted snapshot " + snapshot.getPath(), e);
+      } finally {
+        if (in != null) {
+          in.close();
+        }
+      }
+    }
+    return availableSnapshots;
   }
 
   @Override
-  public void deleteSnapshot(final HSnapshotDescription snapshot) throws IOException {
-    throw new IOException(new UnsupportedOperationException(
-        "Snapshots are not implemented yet."));
+  public void deleteSnapshot(final HSnapshotDescription request) throws IOException {
+    SnapshotDescription snapshot = request.getProto();
+
+    // check to see if it is completed
+    if (!isSnapshotCompleted(snapshot)) {
+      throw new SnapshotDoesNotExistException(snapshot);
+    }
+
+    String snapshotName = snapshot.getName();
+    LOG.debug("Deleting snapshot: " + snapshotName);
+    // first create the snapshot description and check to see if it exists
+    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, this
+        .getMasterFileSystem().getRootDir());
+
+    // delete the existing snapshot
+    if (!this.getMasterFileSystem().getFileSystem().delete(snapshotDir, true)) {
+      throw new IOException("Failed to delete snapshot directory: " + snapshotDir);
+    }
   }
 
   @Override
-  public boolean isSnapshotDone(final HSnapshotDescription snapshot) throws IOException {
-    throw new IOException(new UnsupportedOperationException(
-        "Snapshots are not implemented yet."));
+  public boolean isSnapshotDone(final HSnapshotDescription request) throws IOException {
+    LOG.debug("Checking to see if snapshot from request:" + request + " is done");
+
+    SnapshotDescription expected = request.getProto();
+
+    // check the request to make sure it has a snapshot
+    if (expected == null) {
+      throw new UnknownSnapshotException(
+          "No snapshot name passed in request, can't figure out which snapshot you want to check.");
+    }
+
+    // check to see if the sentinel exists
+    SnapshotSentinel sentinel = this.snapshotManager.getCurrentSnapshotSentinel();
+    if (sentinel != null) {
+
+      // pass on any failure we find in the sentinel
+      HBaseSnapshotException e = sentinel.getExceptionIfFailed();
+      if (e != null) throw e;
+
+      // get the current snapshot and compare it against the requested
+      SnapshotDescription snapshot = sentinel.getSnapshot();
+      LOG.debug("Have a snapshot to compare:" + snapshot);
+      if (expected.getName().equals(snapshot.getName())) {
+        LOG.trace("Running snapshot (" + snapshot.getName() + ") does match request:" +
+             expected.getName());
+
+        // check to see if we are done
+        boolean isDone = false;
+        if (sentinel.isFinished()) {
+          isDone = true;
+          LOG.debug("Snapshot " + snapshot + " has completed, notifying client.");
+        } else if (LOG.isDebugEnabled()) {
+          LOG.debug("Sentinel isn't finished with snapshot!");
+        }
+        return isDone;
+      }
+    }
+
+    // check to see if the snapshot is already on the fs
+    if (!isSnapshotCompleted(expected)) {
+      throw new UnknownSnapshotException("Snapshot:" + expected.getName() +
+           " is not currently running or one of the known completed snapshots.");
+    }
+
+    return true;
+  }
+
+  /**
+   * Check to see if the snapshot is one of the currently completed snapshots
+   * @param expected snapshot to check
+   * @return <tt>true</tt> if the snapshot is stored on the {@link FileSystem}, <tt>false</tt> if is
+   *         not stored
+   * @throws IOException if the filesystem throws an unexpected exception
+   */
+  private boolean isSnapshotCompleted(SnapshotDescription snapshot) throws IOException {
+    final Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshot, this
+        .getMasterFileSystem().getRootDir());
+    FileSystem fs = this.getMasterFileSystem().getFileSystem();
+
+    // check to see if the snapshot already exists
+    return fs.exists(snapshotDir);
   }
 }
diff --git a/src/main/java/org/apache/hadoop/hbase/master/SnapshotSentinel.java b/src/main/java/org/apache/hadoop/hbase/master/SnapshotSentinel.java
new file mode 100644
index 0000000..cf67f20
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/master/SnapshotSentinel.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.Stoppable;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+
+/**
+ * Watch the current snapshot under process
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public interface SnapshotSentinel extends Stoppable {
+
+  /**
+   * Check to see if the snapshot is finished, where finished may be success or failure.
+   * @return <tt>false</tt> if the snapshot is still in progress, <tt>true</tt> if the snapshot has
+   *         finished
+   */
+  public boolean isFinished();
+
+  /**
+   * @return the description of the snapshot being run
+   */
+  public SnapshotDescription getSnapshot();
+
+  /**
+   * Get the exception that caused the snapshot to fail, if the snapshot has failed.
+   * @return <tt>null</tt> if the snapshot succeeded, or the {@link HBaseSnapshotException} that
+   *         caused the snapshot to fail.
+   */
+  public HBaseSnapshotException getExceptionIfFailed();
+
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
new file mode 100644
index 0000000..3a27f8f
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
@@ -0,0 +1,236 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.snapshot;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.executor.EventHandler;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.SnapshotSentinel;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.server.errorhandling.OperationAttemptTimer;
+import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.server.snapshot.task.CopyRecoveredEditsTask;
+import org.apache.hadoop.hbase.server.snapshot.task.ReferenceRegionHFilesTask;
+import org.apache.hadoop.hbase.server.snapshot.task.TableInfoCopyTask;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.Pair;
+
+/**
+ * Take a snapshot of a disabled table.
+ * <p>
+ * Table must exist when taking the snapshot, or results are undefined.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class DisabledTableSnapshotHandler extends EventHandler implements SnapshotSentinel {
+  private static final Log LOG = LogFactory.getLog(DisabledTableSnapshotHandler.class);
+
+  private volatile boolean stopped = false;
+
+  protected final Configuration conf;
+  protected final FileSystem fs;
+  protected final Path rootDir;
+
+  private final MasterServices masterServices;
+
+  private final SnapshotDescription snapshot;
+
+  private final Path workingDir;
+
+  private final String tableName;
+
+  private final OperationAttemptTimer timer;
+  private final SnapshotExceptionSnare monitor;
+
+  private final MasterSnapshotVerifier verify;
+
+  /**
+   * @param snapshot descriptor of the snapshot to take
+   * @param server parent server
+   * @param masterServices master services provider
+   * @throws IOException on unexpected error
+   */
+  public DisabledTableSnapshotHandler(SnapshotDescription snapshot, Server server,
+      final MasterServices masterServices)
+      throws IOException {
+    super(server, EventType.C_M_SNAPSHOT_TABLE);
+    this.masterServices = masterServices;
+    this.tableName = snapshot.getTable();
+
+    this.snapshot = snapshot;
+    this.monitor = new SnapshotExceptionSnare(snapshot);
+
+    this.conf = this.masterServices.getConfiguration();
+    this.fs = this.masterServices.getMasterFileSystem().getFileSystem();
+
+    this.rootDir = FSUtils.getRootDir(this.conf);
+    this.workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
+
+    // prepare the verify
+    this.verify = new MasterSnapshotVerifier(masterServices, snapshot, rootDir);
+
+    // setup the timer
+    timer = TakeSnapshotUtils.getMasterTimerAndBindToMonitor(snapshot, conf, monitor);
+  }
+
+  // TODO consider parallelizing these operations since they are independent. Right now its just
+  // easier to keep them serial though
+  @Override
+  public void process() {
+    LOG.info("Running table snapshot operation " + eventType + " on table " + tableName);
+    try {
+      timer.start();
+      // write down the snapshot info in the working directory
+      SnapshotDescriptionUtils.writeSnapshotInfo(snapshot, workingDir, this.fs);
+
+      // 1. get all the regions hosting this table.
+      List<Pair<HRegionInfo, ServerName>> regionsAndLocations = null;
+      while (regionsAndLocations == null) {
+        try {
+          regionsAndLocations = MetaReader.getTableRegionsAndLocations(
+            this.server.getCatalogTracker(), Bytes.toBytes(tableName), true);
+        } catch (InterruptedException e) {
+          // check to see if we failed, in which case return
+          if (this.monitor.checkForError()) return;
+          // otherwise, just reset the interrupt and keep on going
+          Thread.currentThread().interrupt();
+        }
+      }
+
+      // extract each pair to separate lists
+      Set<String> serverNames = new HashSet<String>();
+      Set<HRegionInfo> regions = new HashSet<HRegionInfo>();
+      for (Pair<HRegionInfo, ServerName> p : regionsAndLocations) {
+        regions.add(p.getFirst());
+        serverNames.add(p.getSecond().toString());
+      }
+
+      // 2. for each region, write all the info to disk
+      LOG.info("Starting to write region info and WALs for regions for offline snapshot:"
+          + snapshot);
+      for (HRegionInfo regionInfo : regions) {
+        // 2.1 copy the regionInfo files to the snapshot
+        Path snapshotRegionDir = TakeSnapshotUtils.getRegionSnapshotDirectory(snapshot, rootDir,
+          regionInfo.getEncodedName());
+        HRegion.writeRegioninfoOnFilesystem(regionInfo, snapshotRegionDir, fs, conf);
+        // check for error for each region
+        monitor.failOnError();
+
+        // 2.2 for each region, copy over its recovered.edits directory
+        Path regionDir = HRegion.getRegionDir(rootDir, regionInfo);
+        new CopyRecoveredEditsTask(snapshot, monitor, fs, regionDir, snapshotRegionDir).run();
+        monitor.failOnError();
+
+        // 2.3 reference all the files in the region
+        new ReferenceRegionHFilesTask(snapshot, monitor, regionDir, fs, snapshotRegionDir).run();
+        monitor.failOnError();
+      }
+
+      // 3. write the table info to disk
+      LOG.info("Starting to copy tableinfo for offline snapshot:\n" + snapshot);
+      TableInfoCopyTask tableInfo = new TableInfoCopyTask(this.monitor, snapshot, fs,
+          FSUtils.getRootDir(conf));
+      tableInfo.run();
+      monitor.failOnError();
+
+      // 4. verify the snapshot is valid
+      verify.verifySnapshot(this.workingDir, serverNames);
+
+      // 5. complete the snapshot
+      SnapshotDescriptionUtils.completeSnapshot(this.snapshot, this.rootDir, this.workingDir,
+        this.fs);
+
+    } catch (Exception e) {
+      // make sure we capture the exception to propagate back to the client later
+      monitor.snapshotFailure("Failed due to exception:" + e.getMessage(), snapshot, e);
+    } finally {
+      LOG.debug("Marking snapshot" + this.snapshot + " as finished.");
+      this.stopped = true;
+
+      // 6. mark the timer as finished - even if we got an exception, we don't need to time the
+      // operation any further
+      timer.complete();
+
+      LOG.debug("Launching cleanup of working dir:" + workingDir);
+      try {
+        // don't mark the snapshot as a failure if we can't cleanup - the snapshot worked.
+        if (!this.fs.delete(this.workingDir, true)) {
+          LOG.error("Couldn't delete snapshot working directory:" + workingDir);
+        }
+      } catch (IOException e) {
+        LOG.error("Couldn't delete snapshot working directory:" + workingDir);
+      }
+    }
+  }
+
+  @Override
+  public boolean isFinished() {
+    return this.stopped;
+  }
+
+  @Override
+  public SnapshotDescription getSnapshot() {
+    return snapshot;
+  }
+
+  @Override
+  public void stop(String why) {
+    if (this.stopped) return;
+    this.stopped = true;
+    LOG.info("Stopping disabled snapshot because: " + why);
+    // pass along the stop as a failure. This keeps all the 'should I stop running?' logic in a
+    // single place, though it is technically a little bit of an overload of how the error handler
+    // should be used.
+    this.monitor.snapshotFailure("Failing snapshot because server is stopping.", snapshot);
+  }
+
+  @Override
+  public boolean isStopped() {
+    return this.stopped;
+  }
+
+  @Override
+  public HBaseSnapshotException getExceptionIfFailed() {
+    try {
+      this.monitor.failOnError();
+    } catch (HBaseSnapshotException e) {
+      return e;
+    }
+    return null;
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java
new file mode 100644
index 0000000..1e77d82
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java
@@ -0,0 +1,253 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.snapshot;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription.Type;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.exception.CorruptedSnapshotException;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.HFileArchiveUtil;
+
+/**
+ * General snapshot verification on the master.
+ * <p>
+ * This is a light-weight verification mechanism for all the files in a snapshot. It doesn't attempt
+ * to verify that the files are exact copies (that would be paramount to taking the snapshot
+ * again!), but instead just attempts to ensure that the files match the expected files and are the
+ * same length.
+ * <p>
+ * Current snapshot files checked:
+ * <ol>
+ * <li>SnapshotDescription is readable</li>
+ * <li>Table info is readable</li>
+ * <li>Regions</li>
+ * <ul>
+ * <li>Matching regions in the snapshot as currently in the table</li>
+ * <li>{@link HRegionInfo} matches the current and stored regions</li>
+ * <li>All referenced hfiles have valid names</li>
+ * <li>All the hfiles are present (either in .archive directory in the region)</li>
+ * <li>All recovered.edits files are present (by name) and have the correct file size</li>
+ * </ul>
+ * <li>HLogs for each server running the snapshot have been referenced
+ * <ul>
+ * <li>Only checked for {@link Type#GLOBAL} snapshots</li>
+ * </ul>
+ * </li>
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public final class MasterSnapshotVerifier {
+
+  private SnapshotDescription snapshot;
+  private FileSystem fs;
+  private Path rootDir;
+  private String tableName;
+  private MasterServices services;
+
+  /**
+   * Build a util for the given snapshot
+   * @param services services for the master
+   * @param snapshot snapshot to check
+   * @param rootDir root directory of the hbase installation.
+   */
+  public MasterSnapshotVerifier(MasterServices services, SnapshotDescription snapshot, Path rootDir) {
+    this.fs = services.getMasterFileSystem().getFileSystem();
+    this.services = services;
+    this.snapshot = snapshot;
+    this.rootDir = rootDir;
+    this.tableName = snapshot.getTable();
+  }
+
+  /**
+   * Verify that the snapshot in the directory is a valid snapshot
+   * @param snapshotDir snapshot directory to check
+   * @param snapshotServers {@link ServerName} of the servers that are involved in the snapshot
+   * @throws CorruptedSnapshotException if the snapshot is invalid
+   * @throws IOException if there is an unexpected connection issue to the filesystem
+   */
+  public void verifySnapshot(Path snapshotDir, Set<String> snapshotServers)
+      throws CorruptedSnapshotException, IOException {
+    // verify snapshot info matches
+    verifySnapshotDescription(snapshotDir);
+
+    // check that tableinfo is a valid table description
+    verifyTableInfo(snapshotDir);
+
+    // check that each region is valid
+    verifyRegions(snapshotDir);
+
+    // check that the hlogs, if they exist, are valid
+    if (shouldCheckLogs(snapshot.getType())) {
+      verifyLogs(snapshotDir, snapshotServers);
+    }
+  }
+
+  /**
+   * Check to see if the snapshot should verify the logs directory based on the type of the logs.
+   * @param type type of snapshot being taken
+   * @return <tt>true</tt> if the logs directory should be verified, <tt>false</tt> otherwise
+   */
+  private boolean shouldCheckLogs(Type type) {
+    // This is better handled in the Type enum via type, but since its PB based, this is the
+    // simplest way to handle it
+    return type.equals(Type.GLOBAL);
+  }
+
+  /**
+   * Check that the snapshot description written in the filesystem matches the current snapshot
+   * @param snapshotDir snapshot directory to check
+   */
+  private void verifySnapshotDescription(Path snapshotDir) throws CorruptedSnapshotException {
+    SnapshotDescription found = SnapshotDescriptionUtils.readSnapshotInfo(fs, snapshotDir);
+    if (!this.snapshot.equals(found)) {
+      throw new CorruptedSnapshotException("Snapshot read (" + found
+          + ") doesn't equal snapshot we ran (" + snapshot + ").", snapshot);
+    }
+  }
+
+  /**
+   * Check that the table descriptor for the snapshot is a valid table descriptor
+   * @param snapshotDir snapshot directory to check
+   */
+  private void verifyTableInfo(Path snapshotDir) throws IOException {
+    FSTableDescriptors.getTableDescriptor(fs, snapshotDir);
+  }
+
+  /**
+   * Check that all the regions in the the snapshot are valid
+   * @param snapshotDir snapshot directory to check
+   * @throws IOException if we can't reach .META. or read the files from the FS
+   */
+  private void verifyRegions(Path snapshotDir) throws IOException {
+    List<HRegionInfo> regions = MetaReader.getTableRegions(this.services.getCatalogTracker(),
+      Bytes.toBytes(tableName));
+    for (HRegionInfo region : regions) {
+      verifyRegion(fs, snapshotDir, region);
+    }
+  }
+
+  /**
+   * Verify that the region (regioninfo, hfiles) are valid
+   * @param snapshotDir snapshot directory to check
+   * @param region the region to check
+   */
+  private void verifyRegion(FileSystem fs, Path snapshotDir, HRegionInfo region) throws IOException {
+    // make sure we have region in the snapshot
+    Path regionDir = new Path(snapshotDir, region.getEncodedName());
+    if (!fs.exists(regionDir)) {
+      throw new CorruptedSnapshotException("No region directory found for region:" + region,
+          snapshot);
+    }
+    // make sure we have the region info in the snapshot
+    Path regionInfo = new Path(regionDir, HRegion.REGIONINFO_FILE);
+    // make sure the file exists
+    if (!fs.exists(regionInfo)) {
+      throw new CorruptedSnapshotException("No region info found for region:" + region, snapshot);
+    }
+    FSDataInputStream in = fs.open(regionInfo);
+    HRegionInfo found = new HRegionInfo();
+    found.readFields(in);
+    if (!region.equals(found)) {
+      throw new CorruptedSnapshotException("Found region info (" + found
+          + ") doesn't match expected region:" + region, snapshot);
+    }
+
+    // make sure we have the expected recovered edits files
+    TakeSnapshotUtils.verifyRecoveredEdits(fs, snapshotDir, found, snapshot);
+
+    // check for the existance of each hfile
+    PathFilter familiesDirs = new FSUtils.FamilyDirFilter(fs);
+    FileStatus[] columnFamilies = FSUtils.listStatus(fs, regionDir, familiesDirs);
+    // should we do some checking here to make sure the cfs are correct?
+    if (columnFamilies == null) return;
+
+    // setup the suffixes for the snapshot directories
+    Path tableNameSuffix = new Path(tableName);
+    Path regionNameSuffix = new Path(tableNameSuffix, region.getEncodedName());
+
+    // get the potential real paths
+    Path archivedRegion = new Path(HFileArchiveUtil.getArchivePath(services.getConfiguration()),
+        regionNameSuffix);
+    Path realRegion = new Path(rootDir, regionNameSuffix);
+
+    // loop through each cf and check we can find each of the hfiles
+    for (FileStatus cf : columnFamilies) {
+      FileStatus[] hfiles = FSUtils.listStatus(fs, cf.getPath(), null);
+      // should we check if there should be hfiles?
+      if (hfiles == null || hfiles.length == 0) continue;
+
+      Path realCfDir = new Path(realRegion, cf.getPath().getName());
+      Path archivedCfDir = new Path(archivedRegion, cf.getPath().getName());
+      for (FileStatus hfile : hfiles) {
+        // make sure the name is correct
+        if (!StoreFile.validateStoreFileName(hfile.getPath().getName())) {
+          throw new CorruptedSnapshotException("HFile: " + hfile.getPath()
+              + " is not a valid hfile name.", snapshot);
+        }
+
+        // check to see if hfile is present in the real table
+        String fileName = hfile.getPath().getName();
+        Path file = new Path(realCfDir, fileName);
+        Path archived = new Path(archivedCfDir, fileName);
+        if (!fs.exists(file) && !fs.equals(archived)) {
+          throw new CorruptedSnapshotException("Can't find hfile: " + hfile.getPath()
+              + " in the real (" + archivedCfDir + ") or archive (" + archivedCfDir
+              + ") directory for the primary table.", snapshot);
+        }
+      }
+    }
+  }
+
+  /**
+   * Check that the logs stored in the log directory for the snapshot are valid - it contains all
+   * the expected logs for all servers involved in the snapshot.
+   * @param snapshotDir snapshot directory to check
+   * @param snapshotServers list of the names of servers involved in the snapshot.
+   * @throws CorruptedSnapshotException if the hlogs in the snapshot are not correct
+   * @throws IOException if we can't reach the filesystem
+   */
+  private void verifyLogs(Path snapshotDir, Set<String> snapshotServers)
+      throws CorruptedSnapshotException, IOException {
+    Path snapshotLogDir = new Path(snapshotDir, HConstants.HREGION_LOGDIR_NAME);
+    Path logsDir = new Path(rootDir, HConstants.HREGION_LOGDIR_NAME);
+    TakeSnapshotUtils.verifyAllLogsGotReferenced(fs, logsDir, snapshotServers, snapshot,
+      snapshotLogDir);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java b/src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java
new file mode 100644
index 0000000..3f22097
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/master/snapshot/manage/SnapshotManager.java
@@ -0,0 +1,182 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.snapshot.manage;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Stoppable;
+import org.apache.hadoop.hbase.executor.ExecutorService;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.SnapshotSentinel;
+import org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription.Type;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+
+/**
+ * This class monitors the whole process of snapshots via ZooKeeper. There is only one
+ * SnapshotMonitor for the master.
+ * <p>
+ * Start monitoring a snapshot by calling method monitor() before the snapshot is started across the
+ * cluster via ZooKeeper. SnapshotMonitor would stop monitoring this snapshot only if it is finished
+ * or aborted.
+ * <p>
+ * Note: There could be only one snapshot being processed and monitored at a time over the cluster.
+ * Start monitoring a snapshot only when the previous one reaches an end status.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public class SnapshotManager implements Stoppable {
+  private static final Log LOG = LogFactory.getLog(SnapshotManager.class);
+
+  // TODO - enable having multiple snapshots with multiple monitors
+
+  private final MasterServices master;
+  private SnapshotSentinel handler;
+  private ExecutorService pool;
+  private final Path rootDir;
+
+  private boolean stopped;
+
+  public SnapshotManager(final MasterServices master, final ZooKeeperWatcher watcher,
+      final ExecutorService executorService) throws KeeperException {
+    this.master = master;
+    this.pool = executorService;
+    this.rootDir = master.getMasterFileSystem().getRootDir();
+  }
+
+  /**
+   * @return <tt>true</tt> if there is a snapshot currently being taken, <tt>false</tt> otherwise
+   */
+  public boolean isTakingSnapshot() {
+    return handler != null && !handler.isFinished();
+  }
+
+  /**
+   * Check to make sure that we are OK to run the passed snapshot. Checks to make sure that we
+   * aren't already running a snapshot.
+   * @param snapshot description of the snapshot we want to start
+   * @throws HBaseSnapshotException if the filesystem could not be prepared to start the snapshot
+   */
+  private synchronized void prepareToTakeSnapshot(SnapshotDescription snapshot)
+      throws HBaseSnapshotException {
+    FileSystem fs = master.getMasterFileSystem().getFileSystem();
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
+
+    // make sure we aren't already running a snapshot
+    if (isTakingSnapshot()) {
+      throw new SnapshotCreationException("Already running another snapshot:"
+          + this.handler.getSnapshot(), snapshot);
+    }
+
+    try {
+      // delete the working directory, since we aren't running the snapshot
+      fs.delete(workingDir, true);
+
+      // recreate the working directory for the snapshot
+      if (!fs.mkdirs(workingDir)) {
+        throw new SnapshotCreationException("Couldn't create working directory (" + workingDir
+            + ") for snapshot.", snapshot);
+      }
+    } catch (HBaseSnapshotException e) {
+      throw e;
+    } catch (IOException e) {
+      throw new SnapshotCreationException(
+          "Exception while checking to see if snapshot could be started.", e, snapshot);
+    }
+  }
+
+  /**
+   * Take a snapshot of a disabled table.
+   * <p>
+   * Ensures the snapshot won't be started if there is another snapshot already running. Does
+   * <b>not</b> check to see if another snapshot of the same name already exists.
+   * @param snapshot description of the snapshot to take. Modified to be {@link Type#DISABLED}.
+   * @throws HBaseSnapshotException if the snapshot could not be started
+   */
+  public synchronized void snapshotDisabledTable(SnapshotDescription snapshot)
+      throws HBaseSnapshotException {
+    // setup the snapshot
+    prepareToTakeSnapshot(snapshot);
+
+    // set the snapshot to be a disabled snapshot, since the client doesn't know about that
+    snapshot = snapshot.toBuilder().setType(Type.DISABLED).build();
+
+    DisabledTableSnapshotHandler handler;
+    try {
+      handler = new DisabledTableSnapshotHandler(snapshot, this.master, this.master);
+      this.handler = handler;
+      this.pool.submit(handler);
+    } catch (IOException e) {
+      // cleanup the working directory
+      Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
+      try {
+        if (this.master.getMasterFileSystem().getFileSystem().delete(workingDir, true)) {
+          LOG.error("Couldn't delete working directory (" + workingDir + " for snapshot:"
+              + snapshot);
+        }
+      } catch (IOException e1) {
+        LOG.error("Couldn't delete working directory (" + workingDir + " for snapshot:" + snapshot);
+      }
+      // fail the snapshot
+      throw new SnapshotCreationException("Could not build snapshot handler", e, snapshot);
+    }
+  }
+
+  /**
+   * @return the current handler for the snapshot
+   */
+  public SnapshotSentinel getCurrentSnapshotSentinel() {
+    return this.handler;
+  }
+
+  @Override
+  public void stop(String why) {
+    // short circuit
+    if (this.stopped) return;
+    // make sure we get stop
+    this.stopped = true;
+    // pass the stop onto all the listeners
+    if (this.handler != null) this.handler.stop(why);
+  }
+
+  @Override
+  public boolean isStopped() {
+    return this.stopped;
+  }
+
+  /**
+   * Set the handler for the current snapshot
+   * <p>
+   * Exposed for TESTING
+   * @param handler handler the master should use
+   */
+  public void setSnapshotHandlerForTesting(SnapshotSentinel handler) {
+    this.handler = handler;
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 9540a0f..e5a8324 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -699,9 +699,32 @@ public class HRegion implements HeapSize { // , Writable{
    * @throws IOException
    */
   private void checkRegioninfoOnFilesystem() throws IOException {
-    Path regioninfoPath = new Path(this.regiondir, REGIONINFO_FILE);
-    if (this.fs.exists(regioninfoPath) &&
-        this.fs.getFileStatus(regioninfoPath).getLen() > 0) {
+    checkRegioninfoOnFilesystem(this.regiondir);
+  }
+
+  /**
+   * Write out an info file under the region directory. Useful recovering mangled regions.
+   * @param regiondir directory under which to write out the region info
+   * @throws IOException
+   */
+  private void checkRegioninfoOnFilesystem(Path regiondir) throws IOException {
+    writeRegioninfoOnFilesystem(regionInfo, regiondir, getFilesystem(), conf);
+  }
+
+  /**
+   * Write out an info file under the region directory. Useful recovering mangled regions. If the
+   * regioninfo already exists on disk and there is information in the file, then we fast exit.
+   * @param regionInfo information about the region
+   * @param regiondir directory under which to write out the region info
+   * @param fs {@link FileSystem} on which to write the region info
+   * @param conf {@link Configuration} from which to extract specific file locations
+   * @throws IOException on unexpected error.
+   */
+  public static void writeRegioninfoOnFilesystem(HRegionInfo regionInfo, Path regiondir,
+      FileSystem fs, Configuration conf) throws IOException {
+    Path regioninfoPath = new Path(regiondir, REGIONINFO_FILE);
+    if (fs.exists(regioninfoPath) &&
+        fs.getFileStatus(regioninfoPath).getLen() > 0) {
       return;
     }
     // Create in tmpdir and then move into place in case we crash after
@@ -714,7 +737,7 @@ public class HRegion implements HeapSize { // , Writable{
         HConstants.DATA_FILE_UMASK_KEY);
 
     // and then create the file
-    Path tmpPath = new Path(getTmpDir(), REGIONINFO_FILE);
+    Path tmpPath = new Path(getTmpDir(regiondir), REGIONINFO_FILE);
 
     // if datanode crashes or if the RS goes down just before the close is called while trying to
     // close the created regioninfo file in the .tmp directory then on next
@@ -727,10 +750,10 @@ public class HRegion implements HeapSize { // , Writable{
     FSDataOutputStream out = FSUtils.create(fs, tmpPath, perms);
 
     try {
-      this.regionInfo.write(out);
+      regionInfo.write(out);
       out.write('\n');
       out.write('\n');
-      out.write(Bytes.toBytes(this.regionInfo.toString()));
+      out.write(Bytes.toBytes(regionInfo.toString()));
     } finally {
       out.close();
     }
@@ -1142,7 +1165,11 @@ public class HRegion implements HeapSize { // , Writable{
    * will have its contents removed when the region is reopened.
    */
   Path getTmpDir() {
-    return new Path(getRegionDir(), REGION_TEMP_SUBDIR);
+    return getTmpDir(getRegionDir());
+  }
+
+  static Path getTmpDir(Path regionDir) {
+    return new Path(regionDir, REGION_TEMP_SUBDIR);
   }
 
   void triggerMajorCompaction() {
diff --git a/src/main/java/org/apache/hadoop/hbase/server/errorhandling/impl/ExceptionOrchestrator.java b/src/main/java/org/apache/hadoop/hbase/server/errorhandling/impl/ExceptionOrchestrator.java
index 8b610af..3b0f358 100644
--- a/src/main/java/org/apache/hadoop/hbase/server/errorhandling/impl/ExceptionOrchestrator.java
+++ b/src/main/java/org/apache/hadoop/hbase/server/errorhandling/impl/ExceptionOrchestrator.java
@@ -59,7 +59,7 @@ public class ExceptionOrchestrator<E extends Exception> implements ExceptionList
       .create();
 
   /** Error visitor for framework listeners */
-  final ForwardingErrorVisitor genericVisitor = new ForwardingErrorVisitor();
+  public final ForwardingErrorVisitor genericVisitor = new ForwardingErrorVisitor();
 
   public ExceptionOrchestrator() {
     this("generic-error-dispatcher");
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java
new file mode 100644
index 0000000..bf5838b
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/TakeSnapshotUtils.java
@@ -0,0 +1,325 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.Store;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.server.errorhandling.ExceptionListener;
+import org.apache.hadoop.hbase.server.errorhandling.OperationAttemptTimer;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.exception.CorruptedSnapshotException;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+
+import com.google.common.collect.HashMultimap;
+import com.google.common.collect.Multimap;
+
+/**
+ * Utilities for useful when taking a snapshot
+ */
+public class TakeSnapshotUtils {
+
+  private static final Log LOG = LogFactory.getLog(TakeSnapshotUtils.class);
+
+  private TakeSnapshotUtils() {
+    // private constructor for util class
+  }
+
+  /**
+   * Get the per-region snapshot description location.
+   * <p>
+   * Under the per-snapshot directory, specific files per-region are kept in a similar layout as per
+   * the current directory layout.
+   * @param desc description of the snapshot
+   * @param rootDir root directory for the hbase installation
+   * @param regionName encoded name of the region (see {@link HRegionInfo#encodeRegionName(byte[])})
+   * @return path to the per-region directory for the snapshot
+   */
+  public static Path getRegionSnapshotDirectory(SnapshotDescription desc, Path rootDir,
+      String regionName) {
+    Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(desc, rootDir);
+    return HRegion.getRegionDir(snapshotDir, regionName);
+  }
+
+  /**
+   * Get the home directory for store-level snapshot files.
+   * <p>
+   * Specific files per store are kept in a similar layout as per the current directory layout.
+   * @param regionDir snapshot directory for the parent region, <b>not</b> the standard region
+   *          directory. See {@link #getRegionSnapshotDirectory(SnapshotDescription, Path, String)}
+   * @param family name of the store to snapshot
+   * @return path to the snapshot home directory for the store/family
+   */
+  public static Path getStoreSnapshotDirectory(Path regionDir, String family) {
+    return Store.getStoreHomedir(regionDir, Bytes.toBytes(family));
+  }
+
+  /**
+   * Get the snapshot directory for each family to be added to the the snapshot
+   * @param snapshot description of the snapshot being take
+   * @param snapshotRegionDir directory in the snapshot where the region directory information
+   *          should be stored
+   * @param families families to be added (can be null)
+   * @return paths to the snapshot directory for each family, in the same order as the families
+   *         passed in
+   */
+  public static List<Path> getFamilySnapshotDirectories(SnapshotDescription snapshot,
+      Path snapshotRegionDir, FileStatus[] families) {
+    if (families == null || families.length == 0) return Collections.emptyList();
+
+    List<Path> familyDirs = new ArrayList<Path>(families.length);
+    for (FileStatus family : families) {
+      // build the reference directory name
+      familyDirs.add(getStoreSnapshotDirectory(snapshotRegionDir, family.getPath().getName()));
+    }
+    return familyDirs;
+  }
+
+  /**
+   * Create a snapshot timer for the master which notifies the monitor when an error occurs
+   * @param snapshot snapshot to monitor
+   * @param conf configuration to use when getting the max snapshot life
+   * @param monitor monitor to notify when the snapshot life expires
+   * @return the timer to use update to signal the start and end of the snapshot
+   */
+  @SuppressWarnings("rawtypes")
+  public static OperationAttemptTimer getMasterTimerAndBindToMonitor(SnapshotDescription snapshot,
+      Configuration conf, ExceptionListener monitor) {
+    long maxTime = SnapshotDescriptionUtils.getMaxMasterTimeout(conf, snapshot.getType(),
+      SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME);
+    return new OperationAttemptTimer(monitor, maxTime, snapshot);
+  }
+
+  /**
+   * Verify that all the expected logs got referenced
+   * @param fs filesystem where the logs live
+   * @param logsDir original logs directory
+   * @param serverNames names of the servers that involved in the snapshot
+   * @param snapshot description of the snapshot being taken
+   * @param snapshotLogDir directory for logs in the snapshot
+   * @throws IOException
+   */
+  public static void verifyAllLogsGotReferenced(FileSystem fs, Path logsDir,
+      Set<String> serverNames, SnapshotDescription snapshot, Path snapshotLogDir)
+      throws IOException {
+    assertTrue(snapshot, "Logs directory doesn't exist in snapshot", fs.exists(logsDir));
+    // for each of the server log dirs, make sure it matches the main directory
+    Multimap<String, String> snapshotLogs = getMapOfServersAndLogs(fs, snapshotLogDir, serverNames);
+    Multimap<String, String> realLogs = getMapOfServersAndLogs(fs, logsDir, serverNames);
+    if (realLogs != null) {
+      assertNotNull(snapshot, "No server logs added to snapshot", snapshotLogs);
+    } else if (realLogs == null) {
+      assertNull(snapshot, "Snapshotted server logs that don't exist", snapshotLogs);
+    }
+
+    // check the number of servers
+    Set<Entry<String, Collection<String>>> serverEntries = realLogs.asMap().entrySet();
+    Set<Entry<String, Collection<String>>> snapshotEntries = snapshotLogs.asMap().entrySet();
+    assertEquals(snapshot, "Not the same number of snapshot and original server logs directories",
+      serverEntries.size(), snapshotEntries.size());
+
+    // verify we snapshotted each of the log files
+    for (Entry<String, Collection<String>> serverLogs : serverEntries) {
+      // if the server is not the snapshot, skip checking its logs
+      if (!serverNames.contains(serverLogs.getKey())) continue;
+      Collection<String> snapshotServerLogs = snapshotLogs.get(serverLogs.getKey());
+      assertNotNull(snapshot, "Snapshots missing logs for server:" + serverLogs.getKey(),
+        snapshotServerLogs);
+
+      // check each of the log files
+      assertEquals(snapshot,
+        "Didn't reference all the log files for server:" + serverLogs.getKey(), serverLogs
+            .getValue().size(), snapshotServerLogs.size());
+      for (String log : serverLogs.getValue()) {
+        assertTrue(snapshot, "Snapshot logs didn't include " + log,
+          snapshotServerLogs.contains(log));
+      }
+    }
+  }
+
+  /**
+   * Verify one of a snapshot's region's recovered.edits, has been at the surface (file names,
+   * length), match the original directory.
+   * @param fs filesystem on which the snapshot had been taken
+   * @param rootDir full path to the root hbase directory
+   * @param regionInfo info for the region
+   * @param snapshot description of the snapshot that was taken
+   * @throws IOException if there is an unexpected error talking to the filesystem
+   */
+  public static void verifyRecoveredEdits(FileSystem fs, Path rootDir, HRegionInfo regionInfo,
+      SnapshotDescription snapshot) throws IOException {
+    Path regionDir = HRegion.getRegionDir(rootDir, regionInfo);
+    Path editsDir = HLog.getRegionDirRecoveredEditsDir(regionDir);
+    Path snapshotRegionDir = TakeSnapshotUtils.getRegionSnapshotDirectory(snapshot, rootDir,
+      regionInfo.getEncodedName());
+    Path snapshotEditsDir = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
+
+    FileStatus[] edits = FSUtils.listStatus(fs, editsDir);
+    FileStatus[] snapshotEdits = FSUtils.listStatus(fs, snapshotEditsDir);
+    if (edits == null) {
+      assertNull(snapshot, "Snapshot has edits but table doesn't", snapshotEdits);
+      return;
+    }
+
+    assertNotNull(snapshot, "Table has edits, but snapshot doesn't", snapshotEdits);
+
+    // check each of the files
+    assertEquals(snapshot, "Not same number of edits in snapshot as table", edits.length,
+      snapshotEdits.length);
+
+    // make sure we have a file with the same name as the original
+    // it would be really expensive to verify the content matches the original
+    for (FileStatus edit : edits) {
+      for (FileStatus sEdit : snapshotEdits) {
+        if (sEdit.getPath().equals(edit.getPath())) {
+          assertEquals(snapshot, "Snapshot file" + sEdit.getPath()
+              + " length not equal to the original: " + edit.getPath(), edit.getLen(),
+            sEdit.getLen());
+          break;
+        }
+      }
+      assertTrue(snapshot, "No edit in snapshot with name:" + edit.getPath(), false);
+    }
+  }
+
+  private static void assertNull(SnapshotDescription snapshot, String msg, Object isNull)
+      throws CorruptedSnapshotException {
+    if (isNull != null) {
+      throw new CorruptedSnapshotException(msg + ", Expected " + isNull + " to be null.", snapshot);
+    }
+  }
+
+  private static void assertNotNull(SnapshotDescription snapshot, String msg, Object notNull)
+      throws CorruptedSnapshotException {
+    if (notNull == null) {
+      throw new CorruptedSnapshotException(msg + ", Expected object to not be null, but was null.",
+          snapshot);
+    }
+  }
+
+  private static void assertTrue(SnapshotDescription snapshot, String msg, boolean isTrue)
+      throws CorruptedSnapshotException {
+    if (!isTrue) {
+      throw new CorruptedSnapshotException(msg + ", Expected true, but was false", snapshot);
+    }
+  }
+
+  /**
+   * Assert that the expect matches the gotten amount
+   * @param msg message to add the to exception
+   * @param expected
+   * @param gotten
+   * @throws CorruptedSnapshotException thrown if the two elements don't match
+   */
+  private static void assertEquals(SnapshotDescription snapshot, String msg, int expected,
+      int gotten) throws CorruptedSnapshotException {
+    if (expected != gotten) {
+      throw new CorruptedSnapshotException(msg + ". Expected:" + expected + ", got:" + gotten,
+          snapshot);
+    }
+  }
+
+  /**
+   * Assert that the expect matches the gotten amount
+   * @param msg message to add the to exception
+   * @param expected
+   * @param gotten
+   * @throws CorruptedSnapshotException thrown if the two elements don't match
+   */
+  private static void assertEquals(SnapshotDescription snapshot, String msg, long expected,
+      long gotten) throws CorruptedSnapshotException {
+    if (expected != gotten) {
+      throw new CorruptedSnapshotException(msg + ". Expected:" + expected + ", got:" + gotten,
+          snapshot);
+    }
+  }
+
+  /**
+   * @param logdir
+   * @param toInclude list of servers to include. If empty or null, returns all servers
+   * @return maps of servers to all their log files. If there is no log directory, returns
+   *         <tt>null</tt>
+   */
+  private static Multimap<String, String> getMapOfServersAndLogs(FileSystem fs, Path logdir,
+      Collection<String> toInclude) throws IOException {
+    // create a path filter based on the passed directories to include
+    PathFilter filter = toInclude == null || toInclude.size() == 0 ? null
+        : new MatchesDirectoryNames(toInclude);
+
+    // get all the expected directories
+    FileStatus[] serverLogDirs = FSUtils.listStatus(fs, logdir, filter);
+    if (serverLogDirs == null) return null;
+
+    // map those into a multimap of servername -> [log files]
+    Multimap<String, String> map = HashMultimap.create();
+    for (FileStatus server : serverLogDirs) {
+      FileStatus[] serverLogs = FSUtils.listStatus(fs, server.getPath(), null);
+      if (serverLogs == null) continue;
+      for (FileStatus log : serverLogs) {
+        map.put(server.getPath().getName(), log.getPath().getName());
+      }
+    }
+    return map;
+  }
+
+  /**
+   * Path filter that only accepts paths where that have a {@link Path#getName()} that is contained
+   * in the specified collection.
+   */
+  private static class MatchesDirectoryNames implements PathFilter {
+
+    Collection<String> paths;
+
+    public MatchesDirectoryNames(Collection<String> dirNames) {
+      this.paths = dirNames;
+    }
+
+    @Override
+    public boolean accept(Path path) {
+      return paths.contains(path.getName());
+    }
+  }
+
+  /**
+   * Get the log directory for a specific snapshot
+   * @param snapshotDir directory where the specific snapshot will be store
+   * @param serverName name of the parent regionserver for the log files
+   * @return path to the log home directory for the archive files.
+   */
+  public static Path getSnapshotHLogsDir(Path snapshotDir, String serverName) {
+    return new Path(snapshotDir, HLog.getHLogDirectoryName(serverName));
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotExceptionSnare.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotExceptionSnare.java
new file mode 100644
index 0000000..3c78fde
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotExceptionSnare.java
@@ -0,0 +1,71 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.error;
+
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.errorhandling.impl.ExceptionSnare;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.snapshot.exception.UnexpectedSnapshotException;
+
+/**
+ * {@link ExceptionSnare} for snapshot exceptions, ensuring that only the first exception is
+ * retained and always returned via {@link #failOnError()}.
+ * <p>
+ * Ensures that any generic exceptions received via
+ * {@link #receiveError(String, HBaseSnapshotException, Object...)} are in fact propagated as
+ * {@link HBaseSnapshotException}.
+ */
+public class SnapshotExceptionSnare extends ExceptionSnare<HBaseSnapshotException> implements
+    SnapshotFailureListener {
+
+  private SnapshotDescription snapshot;
+
+  /**
+   * Create a snare that expects errors for the passed snapshot. Any untyped exceptions passed to
+   * {@link #receiveError(String, HBaseSnapshotException, Object...)} are wrapped as an
+   * {@link UnexpectedSnapshotException} with the passed {@link SnapshotDescription}.
+   * @param snapshot
+   */
+  public SnapshotExceptionSnare(SnapshotDescription snapshot) {
+    this.snapshot = snapshot;
+  }
+
+  @Override
+  public void snapshotFailure(String reason, SnapshotDescription snapshot) {
+    this.receiveError(reason, null, snapshot);
+  }
+
+  @Override
+  public void snapshotFailure(String reason, SnapshotDescription snapshot, Exception t) {
+    this.receiveError(reason,
+      t instanceof HBaseSnapshotException ? (HBaseSnapshotException) t
+          : new UnexpectedSnapshotException(reason, t, snapshot), snapshot);
+  }
+
+  @Override
+  public void failOnError() throws HBaseSnapshotException {
+    try {
+      super.failOnError();
+    } catch (Exception e) {
+      if (e instanceof HBaseSnapshotException) {
+        throw (HBaseSnapshotException) e;
+      }
+      throw new UnexpectedSnapshotException(e.getMessage(), e, snapshot);
+    }
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotFailureListener.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotFailureListener.java
new file mode 100644
index 0000000..ccf0831
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/error/SnapshotFailureListener.java
@@ -0,0 +1,41 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.error;
+
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * Generic running snapshot failure listener
+ */
+public interface SnapshotFailureListener {
+
+  /**
+   * Notification that a given snapshot failed because of an error on the local server
+   * @param snapshot snapshot that failed
+   * @param reason explanation of why the snapshot failed
+   */
+  public void snapshotFailure(String reason, SnapshotDescription snapshot);
+
+  /**
+   * Notification that a given snapshot failed because of an error on the local server
+   * @param reason reason the snapshot failed
+   * @param snapshot the snapshot that failed
+   * @param t the exception that caused the failure
+   */
+  public void snapshotFailure(String reason, SnapshotDescription snapshot, Exception t);
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java
new file mode 100644
index 0000000..d205515
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/CopyRecoveredEditsTask.java
@@ -0,0 +1,89 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import java.io.IOException;
+import java.util.NavigableSet;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+
+/**
+ * Copy over each of the files in a region's recovered.edits directory to the region's snapshot
+ * directory.
+ * <p>
+ * This is a serial operation over each of the files in the recovered.edits directory and also
+ * streams all the bytes to the client and then back to the filesystem, so the files being copied
+ * should be <b>small</b> or it will (a) suck up a lot of bandwidth, and (b) take a long time.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class CopyRecoveredEditsTask extends SnapshotTask {
+
+  private static final Log LOG = LogFactory.getLog(CopyRecoveredEditsTask.class);
+  private final FileSystem fs;
+  private final Path regiondir;
+  private final Path outputDir;
+
+  /**
+   * @param snapshot Snapshot being taken
+   * @param monitor error monitor for the snapshot
+   * @param fs {@link FileSystem} where the snapshot is being taken
+   * @param regionDir directory for the region to examine for edits
+   * @param snapshotRegionDir directory for the region in the snapshot
+   */
+  public CopyRecoveredEditsTask(SnapshotDescription snapshot, SnapshotExceptionSnare monitor,
+      FileSystem fs, Path regionDir, Path snapshotRegionDir) {
+    super(snapshot, monitor, "Copy recovered.edits for region:" + regionDir.getName());
+    this.fs = fs;
+    this.regiondir = regionDir;
+    this.outputDir = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
+  }
+
+  @Override
+  public void process() throws IOException {
+    NavigableSet<Path> files = HLog.getSplitEditFilesSorted(this.fs, regiondir);
+    if (files == null || files.size() == 0) return;
+
+    // copy over each file.
+    // this is really inefficient (could be trivially parallelized), but is
+    // really simple to reason about.
+    for (Path source : files) {
+      // check to see if the file is zero length, in which case we can skip it
+      FileStatus stat = fs.getFileStatus(source);
+      if (stat.getLen() <= 0) continue;
+
+      // its not zero length, so copy over the file
+      Path out = new Path(outputDir, source.getName());
+      LOG.debug("Copying " + source + " to " + out);
+      FileUtil.copy(fs, source, fs, out, true, fs.getConf());
+
+      // check for errors to the running operation after each file
+      this.failOnError();
+    }
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java
new file mode 100644
index 0000000..abb87de
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceRegionHFilesTask.java
@@ -0,0 +1,126 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.util.FSUtils;
+
+/**
+ * Reference all the hfiles in a region for a snapshot.
+ * <p>
+ * Doesn't take into acccount if the hfiles are valid or not, just keeps track of what's in the
+ * region's directory.
+ */
+public class ReferenceRegionHFilesTask extends SnapshotTask {
+
+  public static final Log LOG = LogFactory.getLog(ReferenceRegionHFilesTask.class);
+  private final Path regiondir;
+  private final FileSystem fs;
+  private final PathFilter fileFilter;
+  private final Path snapshotDir;
+
+  /**
+   * Reference all the files in the given region directory
+   * @param snapshot snapshot for which to add references
+   * @param monitor to check/send error
+   * @param regionDir region directory to look for errors
+   * @param fs {@link FileSystem} where the snapshot/region live
+   * @param regionSnapshotDir directory in the snapshot to store region files
+   */
+  public ReferenceRegionHFilesTask(final SnapshotDescription snapshot,
+      SnapshotExceptionSnare monitor, Path regionDir, final FileSystem fs, Path regionSnapshotDir) {
+    super(snapshot, monitor, "Reference hfiles for region:" + regionDir.getName());
+    this.regiondir = regionDir;
+    this.fs = fs;
+
+    this.fileFilter = new PathFilter() {
+      @Override
+      public boolean accept(Path path) {
+        try {
+          return fs.isFile(path);
+        } catch (IOException e) {
+          LOG.error("Failed to reach fs to check file:" + path + ", marking as not file");
+          ReferenceRegionHFilesTask.this.snapshotFailure("Failed to reach fs to check file status",
+            e);
+          return false;
+        }
+      }
+    };
+    this.snapshotDir = regionSnapshotDir;
+  }
+
+  @Override
+  public void process() throws IOException {
+    FileStatus[] families = FSUtils.listStatus(fs, regiondir, new FSUtils.FamilyDirFilter(fs));
+
+    // if no families, then we are done again
+    if (families == null || families.length == 0) {
+      LOG.info("No families under region directory:" + regiondir
+          + ", not attempting to add references.");
+      return;
+    }
+
+    // snapshot directories to store the hfile reference
+    List<Path> snapshotFamilyDirs = TakeSnapshotUtils.getFamilySnapshotDirectories(snapshot,
+      snapshotDir, families);
+
+    LOG.debug("Add hfile references to snapshot directories:" + snapshotFamilyDirs);
+    for (int i = 0; i < families.length; i++) {
+      FileStatus family = families[i];
+      Path familyDir = family.getPath();
+      // get all the hfiles in the family
+      FileStatus[] hfiles = FSUtils.listStatus(fs, familyDir, fileFilter);
+
+      // if no hfiles, then we are done with this family
+      if (hfiles == null || hfiles.length == 0) {
+        LOG.debug("Not hfiles found for family: " + familyDir + ", skipping.");
+        continue;
+      }
+
+      // make the snapshot's family directory
+      Path snapshotFamilyDir = snapshotFamilyDirs.get(i);
+      fs.mkdirs(snapshotFamilyDir);
+
+      // create a reference for each hfile
+      for (FileStatus hfile : hfiles) {
+        Path referenceFile = new Path(snapshotFamilyDir, hfile.getPath().getName());
+        LOG.debug("Creating reference for:" + hfile.getPath() + " at " + referenceFile);
+        if (!fs.createNewFile(referenceFile)) {
+          throw new IOException("Failed to create reference file:" + referenceFile);
+        }
+      }
+    }
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Finished referencing hfiles, current region state:");
+      FSUtils.logFileSystemState(fs, regiondir, LOG);
+      LOG.debug("and the snapshot directory:");
+      FSUtils.logFileSystemState(fs, snapshotDir, LOG);
+    }
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java
new file mode 100644
index 0000000..6290bca
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/ReferenceServerWALsTask.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.util.FSUtils;
+
+/**
+ * Reference all the WAL files under a server's WAL directory
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class ReferenceServerWALsTask extends SnapshotTask {
+  private static final Log LOG = LogFactory.getLog(ReferenceServerWALsTask.class);
+  // XXX does this need to be HasThread?
+  private final FileSystem fs;
+  private final Configuration conf;
+  private final String serverName;
+  private Path logDir;
+
+  /**
+   * @param snapshot snapshot being run
+   * @param failureListener listener to check for errors while running the operation and to
+   *          propagate errors found while running the task
+   * @param logDir log directory for the server. Name of the directory is taken as the name of the
+   *          server
+   * @param conf {@link Configuration} to extract fileystem information
+   * @param fs filesystem where the log files are stored and should be referenced
+   * @throws IOException
+   */
+  public ReferenceServerWALsTask(SnapshotDescription snapshot,
+      SnapshotExceptionSnare failureListener, final Path logDir, final Configuration conf,
+      final FileSystem fs) throws IOException {
+    super(snapshot, failureListener, "Reference WALs for server:" + logDir.getName());
+    this.fs = fs;
+    this.conf = conf;
+    this.serverName = logDir.getName();
+    this.logDir = logDir;
+  }
+
+  @Override
+  public void process() throws IOException {
+    // TODO switch to using a single file to reference all required WAL files
+    // Iterate through each of the log files and add a reference to it.
+    // assumes that all the files under the server's logs directory is a log
+    FileStatus[] serverLogs = FSUtils.listStatus(fs, logDir, null);
+    if (serverLogs == null) LOG.info("No logs for server directory:" + logDir
+        + ", done referencing files.");
+
+    if (LOG.isDebugEnabled()) LOG.debug("Adding references for WAL files:"
+        + Arrays.toString(serverLogs));
+
+    for (FileStatus file : serverLogs) {
+      this.failOnError();
+
+      // TODO - switch to using MonitoredTask
+      // add the reference to the file
+      // 0. Build a reference path based on the file name
+      // get the current snapshot directory
+      Path rootDir = FSUtils.getRootDir(conf);
+      Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(this.snapshot, rootDir);
+      Path snapshotLogDir = TakeSnapshotUtils.getSnapshotHLogsDir(snapshotDir, serverName);
+      // actually store the reference on disk (small file)
+      Path ref = new Path(snapshotLogDir, file.getPath().getName());
+      if (!fs.createNewFile(ref)) {
+        if (!fs.exists(ref)) {
+          throw new IOException("Couldn't create reference for:" + file.getPath());
+        }
+      }
+      LOG.debug("Completed WAL referencing for: " + file.getPath() + " to " + ref);
+    }
+    LOG.debug("Successfully completed WAL referencing for ALL files");
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java
new file mode 100644
index 0000000..213a763
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/SnapshotTask.java
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.errorhandling.ExceptionCheckable;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+
+/**
+ * General snapshot operation taken on a regionserver
+ */
+public abstract class SnapshotTask implements ExceptionCheckable<HBaseSnapshotException>, Runnable {
+
+  private static final Log LOG = LogFactory.getLog(SnapshotTask.class);
+
+  private final SnapshotExceptionSnare errorMonitor;
+  private final String desc;
+
+  protected final SnapshotDescription snapshot;
+
+  /**
+   * @param snapshot Description of the snapshot we are going to operate on
+   * @param monitor listener interested in failures to the snapshot caused by this operation
+   * @param description description of the task being run, for logging
+   */
+  public SnapshotTask(SnapshotDescription snapshot, SnapshotExceptionSnare monitor,
+      String description) {
+    this.snapshot = snapshot;
+    this.errorMonitor = monitor;
+    this.desc = description;
+  }
+
+  protected final void snapshotFailure(String message, Exception e) {
+    this.errorMonitor.snapshotFailure(message, this.snapshot, e);
+  }
+
+  @Override
+  public void failOnError() throws HBaseSnapshotException {
+    this.errorMonitor.failOnError();
+  }
+
+  @Override
+  public boolean checkForError() {
+    return this.errorMonitor.checkForError();
+  }
+
+  @Override
+  public void run() {
+    try {
+      LOG.debug("Running: " + desc);
+      this.process();
+    } catch (Exception e) {
+      this.snapshotFailure("Failed to run " + this.desc, e);
+    }
+  }
+
+  /**
+   * Run the task for the snapshot.
+   * @throws Exception if the task fails. Will be propagated to any other tasks watching the same
+   *           {@link SnapshotErrorListener}.
+   */
+  protected abstract void process() throws Exception;
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java
new file mode 100644
index 0000000..d1d5f44
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/server/snapshot/task/TableInfoCopyTask.java
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
+
+/**
+ * Copy the table info into the snapshot directory
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class TableInfoCopyTask extends SnapshotTask {
+
+  public static final Log LOG = LogFactory.getLog(TableInfoCopyTask.class);
+  private final FileSystem fs;
+  private final Path rootDir;
+
+  /**
+   * Copy the table info for the given table into the snapshot
+   * @param failureListener listen for errors while running the snapshot
+   * @param snapshot snapshot for which we are copying the table info
+   * @param fs {@link FileSystem} where the tableinfo is stored (and where the copy will be written)
+   * @param rootDir root of the {@link FileSystem} where the tableinfo is stored
+   */
+  public TableInfoCopyTask(SnapshotExceptionSnare failureListener, SnapshotDescription snapshot,
+      FileSystem fs, Path rootDir) {
+    super(snapshot, failureListener, "Copy table info for table: " + snapshot.getTable());
+    this.rootDir = rootDir;
+    this.fs = fs;
+  }
+
+  @Override
+  public void process() throws IOException {
+    LOG.debug("Running table info copy.");
+    this.failOnError();
+    LOG.debug("Attempting to copy table info for snapshot:" + this.snapshot);
+    // get the HTable descriptor
+    HTableDescriptor orig = FSTableDescriptors.getTableDescriptor(fs, rootDir,
+      Bytes.toBytes(this.snapshot.getTable()));
+
+    this.failOnError();
+    // write a copy of descriptor to the snapshot directory
+    Path snapshotDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, rootDir);
+    FSTableDescriptors.createTableDescriptorForTableDirectory(fs, snapshotDir, orig, false);
+    LOG.debug("Finished copying tableinfo.");
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java
deleted file mode 100644
index 627b5a4..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot;
-
-import org.apache.hadoop.hbase.HBaseIOException;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * General exception when a snapshot fails.
- */
-@SuppressWarnings("serial")
-public class HBaseSnapshotException extends HBaseIOException {
-
-  private SnapshotDescription description;
-
-  public HBaseSnapshotException(String msg) {
-    super(msg);
-  }
-
-  public HBaseSnapshotException(String msg, Throwable cause) {
-    super(msg, cause);
-  }
-
-  public HBaseSnapshotException(Throwable cause) {
-    super(cause);
-  }
-
-  public HBaseSnapshotException(String msg, SnapshotDescription desc) {
-    super(msg);
-    this.description = desc;
-  }
-
-  public HBaseSnapshotException(Throwable cause, SnapshotDescription desc) {
-    super(cause);
-    this.description = desc;
-  }
-
-  public HBaseSnapshotException(String msg, Throwable cause, SnapshotDescription desc) {
-    super(msg, cause);
-    this.description = desc;
-  }
-
-  public SnapshotDescription getSnapshotDescription() {
-    return this.description;
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java
deleted file mode 100644
index c6cb71c..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot;
-
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-
-/**
- * Thrown when a snapshot could not be created due to a server-side error when taking the snapshot.
- */
-@SuppressWarnings("serial")
-public class SnapshotCreationException extends HBaseSnapshotException {
-
-  public SnapshotCreationException(String msg, SnapshotDescription desc) {
-    super(msg, desc);
-  }
-
-  public SnapshotCreationException(String msg, Throwable cause, SnapshotDescription desc) {
-    super(msg, cause, desc);
-  }
-
-  public SnapshotCreationException(String msg, Throwable cause) {
-    super(msg, cause);
-  }
-
-  public SnapshotCreationException(String msg) {
-    super(msg);
-  }
-
-  public SnapshotCreationException(Throwable cause, SnapshotDescription desc) {
-    super(cause, desc);
-  }
-
-  public SnapshotCreationException(Throwable cause) {
-    super(cause);
-  }
-}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java
index e331524..23eda40 100644
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java
@@ -17,19 +17,128 @@
  */
 package org.apache.hadoop.hbase.snapshot;
 
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.exception.CorruptedSnapshotException;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.FSUtils;
 
 /**
  * Utility class to help manage {@link SnapshotDescription SnapshotDesriptions}.
+ * <p>
+ * Snapshots are laid out on disk like this:
+ * 
+ * <pre>
+ * /hbase/.snapshots
+ *          /.tmp                <---- working directory
+ *          /[snapshot name]     <----- completed snapshot
+ * </pre>
+ * 
+ * A completed snapshot named 'completed' then looks like (multiple regions, servers, files, etc.
+ * signified by '...' on the same directory depth).
+ * 
+ * <pre>
+ * /hbase/.snapshots/completed
+ *                   .snapshotinfo          <--- Description of the snapshot
+ *                   .tableinfo             <--- Copy of the tableinfo
+ *                    /.logs
+ *                        /[server_name]
+ *                            /... [log files]
+ *                         ...
+ *                   /[region name]           <---- All the region's information
+ *                   .regioninfo              <---- Copy of the HRegionInfo
+ *                      /[column family name]
+ *                          /[hfile name]     <--- name of the hfile in the real region
+ *                          ...
+ *                      ...
+ *                    ...
+ * </pre>
+ * 
+ * Utility methods in this class are useful for getting the correct locations for different parts of
+ * the snapshot, as well as moving completed snapshots into place (see
+ * {@link #completeSnapshot(SnapshotDescription, Path, Path, FileSystem)}, and writing the
+ * {@link SnapshotDescription} to the working snapshot directory.
  */
 public class SnapshotDescriptionUtils {
 
+  /**
+   * Filter that only accepts completed snapshot directories
+   */
+  public static class CompletedSnaphotDirectoriesFilter extends FSUtils.DirFilter {
+
+    /**
+     * @param fs
+     */
+    public CompletedSnaphotDirectoriesFilter(FileSystem fs) {
+      super(fs);
+    }
+
+    @Override
+    public boolean accept(Path path) {
+      // only accept directories that aren't the tmp directory
+      if (super.accept(path)) {
+        return !path.getName().equals(SNAPSHOT_TMP_DIR_NAME);
+      }
+      return false;
+    }
+
+  }
+
+  private static final Log LOG = LogFactory.getLog(SnapshotDescriptionUtils.class);
+  /**
+   * Version of the fs layout for a snapshot. Future snapshots may have different file layouts,
+   * which we may need to read in differently.
+   */
+  public static final int SNAPSHOT_LAYOUT_VERSION = 0;
+
+  // snapshot directory constants
+  /**
+   * The file contains the snapshot basic information and it is under the directory of a snapshot.
+   */
+  public static final String SNAPSHOTINFO_FILE = ".snapshotinfo";
+
+  private static final String SNAPSHOT_TMP_DIR_NAME = ".tmp";
+  // snapshot operation values
+  /** Default value if no start time is specified */
+  public static final long NO_SNAPSHOT_START_TIME_SPECIFIED = 0;
+
+  public static final String MASTER_WAIT_TIME_GLOBAL_SNAPSHOT = "hbase.snapshot.global.master.timeout";
+  public static final String REGION_WAIT_TIME_GLOBAL_SNAPSHOT = "hbase.snapshot.global.region.timeout";
+  public static final String MASTER_WAIT_TIME_TIMESTAMP_SNAPSHOT = "hbase.snapshot.timestamp.master.timeout";
+  public static final String REGION_WAIT_TIME_TIMESTAMP_SNAPSHOT = "hbase.snapshot.timestamp.region.timeout";
+  public static final String MASTER_WAIT_TIME_DISABLED_SNAPSHOT = "hbase.snapshot.disabled.master.timeout";
+
+  /** Default timeout of 60 sec for a snapshot timeout on a region */
+  public static final long DEFAULT_REGION_SNAPSHOT_TIMEOUT = 60000;
+
+  /** By default, wait 60 seconds for a snapshot to complete */
+  public static final long DEFAULT_MAX_WAIT_TIME = 60000;
+
+  /**
+   * Conf key for amount of time the in the future a timestamp snapshot should be taken (ms).
+   * Defaults to {@value SnapshotDescriptionUtils#DEFAULT_TIMESTAMP_SNAPSHOT_SPLIT_IN_FUTURE}
+   */
+  public static final String TIMESTAMP_SNAPSHOT_SPLIT_POINT_ADDITION = "hbase.snapshot.timestamp.master.splittime";
+  /** Start 2 seconds in the future, if no start time given */
+  public static final long DEFAULT_TIMESTAMP_SNAPSHOT_SPLIT_IN_FUTURE = 2000;
+
   private SnapshotDescriptionUtils() {
     // private constructor for utility class
   }
-  
+
   /**
    * Check to make sure that the description of the snapshot requested is valid
    * @param snapshot description of the snapshot
@@ -44,4 +153,219 @@ public class SnapshotDescriptionUtils {
     // make sure the table name is valid
     HTableDescriptor.isLegalTableName(Bytes.toBytes(snapshot.getTable()));
   }
+
+  /**
+   * @param conf {@link Configuration} from which to check for the timeout
+   * @param type type of snapshot being taken
+   * @param defaultMaxWaitTime Default amount of time to wait, if none is in the configuration
+   * @return the max amount of time the master should wait for a snapshot to complete
+   */
+  public static long getMaxMasterTimeout(Configuration conf, SnapshotDescription.Type type,
+      long defaultMaxWaitTime) {
+    String confKey;
+    switch (type) {
+    case GLOBAL:
+      confKey = MASTER_WAIT_TIME_GLOBAL_SNAPSHOT;
+      break;
+    case TIMESTAMP:
+
+      confKey = MASTER_WAIT_TIME_TIMESTAMP_SNAPSHOT;
+    case DISABLED:
+    default:
+      confKey = MASTER_WAIT_TIME_DISABLED_SNAPSHOT;
+    }
+    return conf.getLong(confKey, defaultMaxWaitTime);
+  }
+
+  /**
+   * @param conf {@link Configuration} from which to check for the timeout
+   * @param type type of snapshot being taken
+   * @param defaultMaxWaitTime Default amount of time to wait, if none is in the configuration
+   * @return the max amount of time the region should wait for a snapshot to complete
+   */
+  public static long getMaxRegionTimeout(Configuration conf, SnapshotDescription.Type type,
+      long defaultMaxWaitTime) {
+    String confKey;
+    switch (type) {
+    case GLOBAL:
+      confKey = REGION_WAIT_TIME_GLOBAL_SNAPSHOT;
+      break;
+    case TIMESTAMP:
+    default:
+      confKey = REGION_WAIT_TIME_TIMESTAMP_SNAPSHOT;
+    }
+    return conf.getLong(confKey, defaultMaxWaitTime);
+  }
+
+  /**
+   * Get the snapshot root directory. All the snapshots are kept under this directory, i.e.
+   * ${hbase.rootdir}/.snapshot
+   * @param rootDir hbase root directory
+   * @return the base directory in which all snapshots are kept
+   */
+  public static Path getSnapshotRootDir(final Path rootDir) {
+    return new Path(rootDir, HConstants.SNAPSHOT_DIR_NAME);
+  }
+
+  /**
+   * Get the directory for a specified snapshot. This directory is a sub-directory of snapshot root
+   * directory and all the data files for a snapshot are kept under this directory.
+   * @param snapshot snapshot being taken
+   * @param rootDir hbase root directory
+   * @return the final directory for the completed snapshot
+   */
+  public static Path getCompletedSnapshotDir(final SnapshotDescription snapshot, final Path rootDir) {
+    return getCompletedSnapshotDir(snapshot.getName(), rootDir);
+  }
+
+  /**
+   * Get the directory for a completed snapshot. This directory is a sub-directory of snapshot root
+   * directory and all the data files for a snapshot are kept under this directory.
+   * @param snapshotName name of the snapshot being taken
+   * @param rootDir hbase root directory
+   * @return the final directory for the completed snapshot
+   */
+  public static Path getCompletedSnapshotDir(final String snapshotName, final Path rootDir) {
+    return getCompletedSnapshotDir(getSnapshotsDir(rootDir), snapshotName);
+  }
+
+  /**
+   * Get the directory to build a snapshot, before it is finalized
+   * @param snapshot snapshot that will be built
+   * @param rootDir root directory of the hbase installation
+   * @return {@link Path} where one can build a snapshot
+   */
+  public static Path getWorkingSnapshotDir(SnapshotDescription snapshot, final Path rootDir) {
+    return getCompletedSnapshotDir(new Path(getSnapshotsDir(rootDir), SNAPSHOT_TMP_DIR_NAME),
+      snapshot.getName());
+  }
+
+  /**
+   * Get the directory to store the snapshot instance
+   * @param snapshotsDir hbase-global directory for storing all snapshots
+   * @param snapshotName name of the snapshot to take
+   * @return
+   */
+  private static final Path getCompletedSnapshotDir(final Path snapshotsDir, String snapshotName) {
+    return new Path(snapshotsDir, snapshotName);
+  }
+
+  /**
+   * @param rootDir hbase root directory
+   * @return the directory for all completed snapshots;
+   */
+  public static final Path getSnapshotsDir(Path rootDir) {
+    return new Path(rootDir, HConstants.SNAPSHOT_DIR_NAME);
+  }
+
+  /**
+   * Convert the passed snapshot description into a 'full' snapshot description based on default
+   * parameters, if none have been supplied. This resolves any 'optional' parameters that aren't
+   * supplied to their default values.
+   * @param snapshot general snapshot descriptor
+   * @param conf Configuration to read configured snapshot defaults if snapshot is not complete
+   * @return a valid snapshot description
+   * @throws IllegalArgumentException if the {@link SnapshotDescription} is not a complete
+   *           {@link SnapshotDescription}.
+   */
+  public static SnapshotDescription validate(SnapshotDescription snapshot, Configuration conf)
+      throws IllegalArgumentException {
+    if (!snapshot.hasTable()) {
+      throw new IllegalArgumentException(
+        "Descriptor doesn't apply to a table, so we can't build it.");
+    }
+
+    // set the creation time, if one hasn't been set
+    long time = snapshot.getCreationTime();
+    if (time == SnapshotDescriptionUtils.NO_SNAPSHOT_START_TIME_SPECIFIED) {
+      time = EnvironmentEdgeManager.currentTimeMillis();
+      if (snapshot.getType().equals(SnapshotDescription.Type.TIMESTAMP)) {
+        long increment = conf.getLong(
+          SnapshotDescriptionUtils.TIMESTAMP_SNAPSHOT_SPLIT_POINT_ADDITION,
+          SnapshotDescriptionUtils.DEFAULT_TIMESTAMP_SNAPSHOT_SPLIT_IN_FUTURE);
+        LOG.debug("Setting timestamp snasphot in future by " + increment + " ms.");
+        time += increment;
+      }
+      LOG.debug("Creation time not specified, setting to:" + time + " (current time:"
+          + EnvironmentEdgeManager.currentTimeMillis() + ").");
+      SnapshotDescription.Builder builder = snapshot.toBuilder();
+      builder.setCreationTime(time);
+      snapshot = builder.build();
+    }
+    return snapshot;
+  }
+
+  /**
+   * Write the snapshot description into the working directory of a snapshot
+   * @param snapshot description of the snapshot being taken
+   * @param workingDir working directory of the snapshot
+   * @param fs {@link FileSystem} on which the snapshot should be taken
+   * @throws IOException if we can't reach the filesystem and the file cannot be cleaned up on
+   *           failure
+   */
+  public static void writeSnapshotInfo(SnapshotDescription snapshot, Path workingDir, FileSystem fs)
+      throws IOException {
+    FsPermission perms = FSUtils.getFilePermissions(fs, fs.getConf(),
+      HConstants.DATA_FILE_UMASK_KEY);
+    Path snapshotInfo = new Path(workingDir, SnapshotDescriptionUtils.SNAPSHOTINFO_FILE);
+    try {
+      FSDataOutputStream out = FSUtils.create(fs, snapshotInfo, perms, true);
+      try {
+        snapshot.writeTo(out);
+      } finally {
+        out.close();
+      }
+    } catch (IOException e) {
+      // if we get an exception, try to remove the snapshot info
+      if (!fs.delete(snapshotInfo, false)) {
+        String msg = "Couldn't delete snapshot info file: " + snapshotInfo;
+        LOG.error(msg);
+        throw new IOException(msg);
+      }
+    }
+  }
+
+  /**
+   * Read in the {@link SnapshotDescription} stored for the snapshot in the passed directory
+   * @param fs filesystem where the snapshot was taken
+   * @param snapshotDir directory where the snapshot was stored
+   * @return the stored snapshot description
+   * @throws CorruptedSnapshotException if the snapshot cannot be read
+   */
+  public static SnapshotDescription readSnapshotInfo(FileSystem fs, Path snapshotDir)
+      throws CorruptedSnapshotException {
+    Path snapshotInfo = new Path(snapshotDir, SNAPSHOTINFO_FILE);
+    try {
+      FSDataInputStream in = null;
+      try {
+        in = fs.open(snapshotInfo);
+        return SnapshotDescription.parseFrom(in);
+      } finally {
+        if (in != null) in.close();
+      }
+    } catch (IOException e) {
+      throw new CorruptedSnapshotException("Couldn't read snapshot info from:" + snapshotInfo, e);
+    }
+  }
+
+  /**
+   * Move the finished snapshot to its final, publicly visible directory - this marks the snapshot
+   * as 'complete'.
+   * @param snapshot description of the snapshot being tabken
+   * @param rootdir root directory of the hbase installation
+   * @param workingDir directory where the in progress snapshot was built
+   * @param fs {@link FileSystem} where the snapshot was built
+   * @throws SnapshotCreationException if the snapshot could not be moved
+   * @throws IOException the filesystem could not be reached
+   */
+  public static void completeSnapshot(SnapshotDescription snapshot, Path rootdir, Path workingDir,
+      FileSystem fs) throws SnapshotCreationException, IOException {
+    Path finishedDir = getCompletedSnapshotDir(snapshot, rootdir);
+    LOG.debug("Snapshot is done, just moving the snapshot from " + workingDir + " to "
+        + finishedDir);
+    if (!fs.rename(workingDir, finishedDir)) {
+      throw new SnapshotCreationException("Failed to move working directory(" + workingDir
+          + ") to completed directory(" + finishedDir + ").", snapshot);
+    }
+  }
 }
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java
deleted file mode 100644
index abfdea7..0000000
--- a/src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.snapshot;
-
-/**
- * Exception thrown when we get a snapshot error about a snapshot we don't know or recognize.
- */
-@SuppressWarnings("serial")
-public class UnknownSnapshotException extends SnapshotCreationException {
-
-  public UnknownSnapshotException(String msg) {
-    super(msg);
-  }
-}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java
new file mode 100644
index 0000000..602b965
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/CorruptedSnapshotException.java
@@ -0,0 +1,49 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+
+/**
+ * Exception thrown when the found snapshot info from the filesystem is not valid
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class CorruptedSnapshotException extends HBaseSnapshotException {
+
+  /**
+   * @param message message describing the exception
+   * @param e cause
+   */
+  public CorruptedSnapshotException(String message, Exception e) {
+    super(message, e);
+  }
+
+  /**
+   * Snapshot was corrupt for some reason
+   * @param message full description of the failure
+   * @param snapshot snapshot that was expected
+   */
+  public CorruptedSnapshotException(String message, SnapshotDescription snapshot) {
+    super(message, snapshot);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java
new file mode 100644
index 0000000..afb8c34
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/HBaseSnapshotException.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.HBaseIOException;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * General exception base class for when a snapshot fails
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public abstract class HBaseSnapshotException extends HBaseIOException {
+
+  private SnapshotDescription description;
+
+  /**
+   * Some exception happened for a snapshot and don't even know the snapshot that it was about
+   * @param msg Full description of the failure
+   */
+  public HBaseSnapshotException(String msg) {
+    super(msg);
+  }
+
+  /**
+   * Exception for the given snapshot that has no previous root cause
+   * @param msg reason why the snapshot failed
+   * @param desc description of the snapshot that is being failed
+   */
+  public HBaseSnapshotException(String msg, SnapshotDescription desc) {
+    super(msg);
+    this.description = desc;
+  }
+
+  /**
+   * Exception for the given snapshot due to another exception
+   * @param msg reason why the snapshot failed
+   * @param cause root cause of the failure
+   * @param desc description of the snapshot that is being failed
+   */
+  public HBaseSnapshotException(String msg, Throwable cause, SnapshotDescription desc) {
+    super(msg, cause);
+    this.description = desc;
+  }
+
+  /**
+   * Exception when the description of the snapshot cannot be determined, due to some root other
+   * root cause
+   * @param message description of what caused the failure
+   * @param e root cause
+   */
+  public HBaseSnapshotException(String message, Exception e) {
+    super(message, e);
+  }
+
+  public SnapshotDescription getSnapshotDescription() {
+    return this.description;
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java
new file mode 100644
index 0000000..9f25a0c
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotCreationException.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * Thrown when a snapshot could not be created due to a server-side error when taking the snapshot.
+ */
+@SuppressWarnings("serial")
+public class SnapshotCreationException extends HBaseSnapshotException {
+
+  /**
+   * Used internally by the RPC engine to pass the exception back to the client.
+   * @param msg error message to pass back
+   */
+  public SnapshotCreationException(String msg) {
+    super(msg);
+  }
+
+  /**
+   * Failure to create the specified snapshot
+   * @param msg reason why the snapshot couldn't be completed
+   * @param desc description of the snapshot attempted
+   */
+  public SnapshotCreationException(String msg, SnapshotDescription desc) {
+    super(msg, desc);
+  }
+
+  /**
+   * Failure to create the specified snapshot due to an external cause
+   * @param msg reason why the snapshot couldn't be completed
+   * @param cause root cause of the failure
+   * @param desc description of the snapshot attempted
+   */
+  public SnapshotCreationException(String msg, Throwable cause, SnapshotDescription desc) {
+    super(msg, cause, desc);
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java
new file mode 100644
index 0000000..68f1ef6
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotDoesNotExistException.java
@@ -0,0 +1,40 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+
+/**
+ * Thrown when the server is looking for a snapshot but can't find the snapshot on the filesystem
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class SnapshotDoesNotExistException extends HBaseSnapshotException {
+
+  /**
+   * @param desc expected snapshot to find
+   */
+  public SnapshotDoesNotExistException(SnapshotDescription desc) {
+    super("Snapshot doesn't exist on the filesystem", desc);
+  }
+
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java
new file mode 100644
index 0000000..1fb7496
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/SnapshotExistsException.java
@@ -0,0 +1,40 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * Thrown when a snapshot exists but should not
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class SnapshotExistsException extends HBaseSnapshotException {
+
+  /**
+   * Failure due to the snapshot already existing
+   * @param msg full description of the failure
+   * @param desc snapshot that was attempted
+   */
+  public SnapshotExistsException(String msg, SnapshotDescription desc) {
+    super(msg, desc);
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java
new file mode 100644
index 0000000..580d67e
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/TablePartiallyOpenException.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Thrown if a table should be online/offline but is partial open
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class TablePartiallyOpenException extends IOException {
+  private static final long serialVersionUID = 3571982660065058361L;
+
+  public TablePartiallyOpenException() {
+    super();
+  }
+
+  /**
+   * @param s message
+   */
+  public TablePartiallyOpenException(String s) {
+    super(s);
+  }
+
+  /**
+   * @param tableName Name of table that is partial open
+   */
+  public TablePartiallyOpenException(byte[] tableName) {
+    this(Bytes.toString(tableName));
+  }
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java
new file mode 100644
index 0000000..f729920
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnexpectedSnapshotException.java
@@ -0,0 +1,42 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+
+/**
+ * General exception when an unexpected error occurs while running a snapshot.
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class UnexpectedSnapshotException extends HBaseSnapshotException {
+
+  /**
+   * General exception for some cause
+   * @param msg reason why the snapshot couldn't be completed
+   * @param cause root cause of the failure
+   * @param snapshot description of the snapshot attempted
+   */
+  public UnexpectedSnapshotException(String msg, Exception cause, SnapshotDescription snapshot) {
+    super(msg, cause, snapshot);
+  }
+
+}
diff --git a/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java
new file mode 100644
index 0000000..698fd0c
--- /dev/null
+++ b/src/main/java/org/apache/hadoop/hbase/snapshot/exception/UnknownSnapshotException.java
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot.exception;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+/**
+ * Exception thrown when we get a request for a snapshot we don't recognize.
+ */
+@SuppressWarnings("serial")
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class UnknownSnapshotException extends HBaseSnapshotException {
+
+
+  /**
+   * @param msg full infomration about the failure
+   */
+  public UnknownSnapshotException(String msg) {
+    super(msg);
+  }
+}
\ No newline at end of file
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
index 5141121..786b334 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
@@ -597,8 +597,25 @@ public class FSTableDescriptors implements TableDescriptors {
   public static boolean createTableDescriptor(FileSystem fs, Path rootdir,
       HTableDescriptor htableDescriptor, boolean forceCreation)
   throws IOException {
-    FileStatus status =
-      getTableInfoPath(fs, rootdir, htableDescriptor.getNameAsString());
+    Path tabledir = FSUtils.getTablePath(rootdir, htableDescriptor.getNameAsString());
+    return createTableDescriptorForTableDirectory(fs, tabledir, htableDescriptor, forceCreation);
+  }
+
+  /**
+   * Create a new HTableDescriptor in HDFS in the specified table directory. Happens when we create
+   * a new table or snapshot a table.
+   * @param fs filesystem where the descriptor should be written
+   * @param tabledir directory under which we should write the file
+   * @param htableDescriptor description of the table to write
+   * @param forceCreation if <tt>true</tt>,then even if previous table descriptor is present it will
+   *          be overwritten
+   * @return <tt>true</tt> if the we successfully created the file, <tt>false</tt> if the file
+   *         already exists and we weren't forcing the descriptor creation.
+   * @throws IOException if a filesystem error occurs
+   */
+  public static boolean createTableDescriptorForTableDirectory(FileSystem fs, Path tabledir,
+      HTableDescriptor htableDescriptor, boolean forceCreation) throws IOException {
+    FileStatus status = getTableInfoPath(fs, tabledir);
     if (status != null) {
       LOG.info("Current tableInfoPath = " + status.getPath());
       if (!forceCreation) {
@@ -608,8 +625,7 @@ public class FSTableDescriptors implements TableDescriptors {
         }
       }
     }
-    Path p = writeTableDescriptor(fs, htableDescriptor,
-      FSUtils.getTablePath(rootdir, htableDescriptor.getNameAsString()), status);
+    Path p = writeTableDescriptor(fs, htableDescriptor, tabledir, status);
     return p != null;
   }
 }
diff --git a/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromAdmin.java b/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromAdmin.java
new file mode 100644
index 0000000..ced199e
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromAdmin.java
@@ -0,0 +1,151 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.ipc.HMasterInterface;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+import com.google.protobuf.RpcController;
+
+/**
+ * Test snapshot logic from the client
+ */
+@Category(SmallTests.class)
+public class TestSnapshotFromAdmin {
+
+  private static final Log LOG = LogFactory.getLog(TestSnapshotFromAdmin.class);
+
+  /**
+   * Test that the logic for doing 'correct' back-off based on exponential increase and the max-time
+   * passed from the server ensures the correct overall waiting for the snapshot to finish.
+   * @throws Exception
+   */
+  @Test(timeout = 10000)
+  public void testBackoffLogic() throws Exception {
+    final int maxWaitTime = 7500;
+    final int numRetries = 10;
+    final int pauseTime = 500;
+    // calculate the wait time, if we just do straight backoff (ignoring the expected time from
+    // master)
+    long ignoreExpectedTime = 0;
+    for (int i = 0; i < 6; i++) {
+      ignoreExpectedTime += HConstants.RETRY_BACKOFF[i] * pauseTime;
+    }
+    // the correct wait time, capping at the maxTime/tries + fudge room
+    final long time = pauseTime * 3 + ((maxWaitTime / numRetries) * 3) + 300;
+    assertTrue("Capped snapshot wait time isn't less that the uncapped backoff time "
+        + "- further testing won't prove anything.", time < ignoreExpectedTime);
+
+    // setup the mocks
+    HConnectionManager.HConnectionImplementation mockConnection = Mockito
+        .mock(HConnectionManager.HConnectionImplementation.class);
+    Configuration conf = HBaseConfiguration.create();
+    // setup the conf to match the expected properties
+    conf.setInt("hbase.client.retries.number", numRetries);
+    conf.setLong("hbase.client.pause", pauseTime);
+    // mock the master admin to our mock
+    HMasterInterface mockMaster = Mockito.mock(HMasterInterface.class);
+    Mockito.when(mockConnection.getConfiguration()).thenReturn(conf);
+    Mockito.when(mockConnection.getMaster()).thenReturn(mockMaster);
+    // set the max wait time for the snapshot to complete
+    Mockito
+        .when(
+          mockMaster.snapshot(
+            Mockito.any(HSnapshotDescription.class))).thenReturn((long)maxWaitTime);
+
+    // first five times, we return false, last we get success
+    Mockito.when(
+      mockMaster.isSnapshotDone(
+        Mockito.any(HSnapshotDescription.class))).thenReturn(false, false,
+      false, false, false, true);
+
+    // setup the admin and run the test
+    HBaseAdmin admin = new HBaseAdmin(mockConnection);
+    String snapshot = "snapshot";
+    String table = "table";
+    // get start time
+    long start = System.currentTimeMillis();
+    admin.snapshot(snapshot, table);
+    long finish = System.currentTimeMillis();
+    long elapsed = (finish - start);
+    assertTrue("Elapsed time:" + elapsed + " is more than expected max:" + time, elapsed <= time);
+    admin.close();
+  }
+
+  /**
+   * Make sure that we validate the snapshot name and the table name before we pass anything across
+   * the wire
+   * @throws Exception on failure
+   */
+  @Test
+  public void testValidateSnapshotName() throws Exception {
+    HConnectionManager.HConnectionImplementation mockConnection = Mockito
+        .mock(HConnectionManager.HConnectionImplementation.class);
+    Configuration conf = HBaseConfiguration.create();
+    Mockito.when(mockConnection.getConfiguration()).thenReturn(conf);
+    HBaseAdmin admin = new HBaseAdmin(mockConnection);
+    SnapshotDescription.Builder builder = SnapshotDescription.newBuilder();
+    // check that invalid snapshot names fail
+    failSnapshotStart(admin, builder.setName(".snapshot").build());
+    failSnapshotStart(admin, builder.setName("-snapshot").build());
+    failSnapshotStart(admin, builder.setName("snapshot fails").build());
+    failSnapshotStart(admin, builder.setName("snap$hot").build());
+    // check the table name also get verified
+    failSnapshotStart(admin, builder.setName("snapshot").setTable(".table").build());
+    failSnapshotStart(admin, builder.setName("snapshot").setTable("-table").build());
+    failSnapshotStart(admin, builder.setName("snapshot").setTable("table fails").build());
+    failSnapshotStart(admin, builder.setName("snapshot").setTable("tab%le").build());
+
+    // mock the master connection
+    HMasterInterface master = Mockito.mock(HMasterInterface.class);
+    Mockito.when(mockConnection.getMaster()).thenReturn(master);
+
+    Mockito.when(
+      master.snapshot(Mockito.any(HSnapshotDescription.class))).thenReturn((long)0);
+    Mockito.when(
+      master.isSnapshotDone(
+        Mockito.any(HSnapshotDescription.class))).thenReturn(true);
+
+      // make sure that we can use valid names
+    admin.snapshot(builder.setName("snapshot").setTable("table").build());
+  }
+
+  private void failSnapshotStart(HBaseAdmin admin, SnapshotDescription snapshot) throws IOException {
+    try {
+      admin.snapshot(snapshot);
+      fail("Snapshot should not have succeed with name:" + snapshot.getName());
+    } catch (IllegalArgumentException e) {
+      LOG.debug("Correctly failed to start snapshot:" + e.getMessage());
+    }
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java b/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
new file mode 100644
index 0000000..8e5940d
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
@@ -0,0 +1,200 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.TableNotFoundException;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+/**
+ * Test create/using/deleting snapshots from the client
+ * <p>
+ * This is an end-to-end test for the snapshot utility
+ */
+@Category(LargeTests.class)
+public class TestSnapshotFromClient {
+  private static final Log LOG = LogFactory.getLog(TestSnapshotFromClient.class);
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+  private static final int NUM_RS = 2;
+  private static final String STRING_TABLE_NAME = "test";
+  private static final byte[] TEST_FAM = Bytes.toBytes("fam");
+  private static final byte[] TABLE_NAME = Bytes.toBytes(STRING_TABLE_NAME);
+
+  /**
+   * Setup the config for the cluster
+   * @throws Exception on failure
+   */
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    setupConf(UTIL.getConfiguration());
+    UTIL.startMiniCluster(NUM_RS);
+  }
+
+  private static void setupConf(Configuration conf) {
+    // disable the ui
+    conf.setInt("hbase.regionsever.info.port", -1);
+    // change the flush size to a small amount, regulating number of store files
+    conf.setInt("hbase.hregion.memstore.flush.size", 25000);
+    // so make sure we get a compaction when doing a load, but keep around some
+    // files in the store
+    conf.setInt("hbase.hstore.compaction.min", 10);
+    conf.setInt("hbase.hstore.compactionThreshold", 10);
+    // block writes if we get to 12 store files
+    conf.setInt("hbase.hstore.blockingStoreFiles", 12);
+    // drop the number of attempts for the hbase admin
+    conf.setInt("hbase.client.retries.number", 1);
+  }
+
+  @Before
+  public void setup() throws Exception {
+    UTIL.createTable(TABLE_NAME, TEST_FAM);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    UTIL.deleteTable(TABLE_NAME);
+    // and cleanup the archive directory
+    try {
+      UTIL.getTestFileSystem().delete(new Path(UTIL.getDefaultRootDirPath(), ".archive"), true);
+    } catch (IOException e) {
+      LOG.warn("Failure to delete archive directory", e);
+    }
+  }
+
+  @AfterClass
+  public static void cleanupTest() throws Exception {
+    try {
+      UTIL.shutdownMiniCluster();
+    } catch (Exception e) {
+      LOG.warn("failure shutting down cluster", e);
+    }
+  }
+
+  /**
+   * Test snapshotting a table that is offline
+   * @throws Exception
+   */
+  @Test
+  public void testOfflineTableSnapshot() throws Exception {
+    HBaseAdmin admin = UTIL.getHBaseAdmin();
+    // make sure we don't fail on listing snapshots
+    SnapshotTestingUtils.assertNoSnapshots(admin);
+
+    // put some stuff in the table
+    HTable table = new HTable(UTIL.getConfiguration(), TABLE_NAME);
+    UTIL.loadTable(table, TEST_FAM);
+
+    // get the name of all the regionservers hosting the snapshotted table
+    Set<String> snapshotServers = new HashSet<String>();
+    List<RegionServerThread> servers = UTIL.getMiniHBaseCluster().getLiveRegionServerThreads();
+    for (RegionServerThread server : servers) {
+      if (server.getRegionServer().getOnlineRegions(TABLE_NAME).size() > 0) {
+        snapshotServers.add(server.getRegionServer().getServerName().toString());
+      }
+    }
+
+    LOG.debug("FS state before disable:");
+    FSUtils.logFileSystemState(UTIL.getTestFileSystem(),
+      FSUtils.getRootDir(UTIL.getConfiguration()), LOG);
+    // XXX if this is flakey, might want to consider using the async version and looping as
+    // disableTable can succeed and still timeout.
+    admin.disableTable(TABLE_NAME);
+
+    LOG.debug("FS state before snapshot:");
+    FSUtils.logFileSystemState(UTIL.getTestFileSystem(),
+      FSUtils.getRootDir(UTIL.getConfiguration()), LOG);
+
+    // take a snapshot of the disabled table
+    byte[] snapshot = Bytes.toBytes("offlineTableSnapshot");
+    admin.snapshot(snapshot, TABLE_NAME);
+    LOG.debug("Snapshot completed.");
+
+    // make sure we have the snapshot
+    List<SnapshotDescription> snapshots = SnapshotTestingUtils.assertOneSnapshotThatMatches(admin,
+      snapshot, TABLE_NAME);
+
+    // make sure its a valid snapshot
+    FileSystem fs = UTIL.getHBaseCluster().getMaster().getMasterFileSystem().getFileSystem();
+    Path rootDir = UTIL.getHBaseCluster().getMaster().getMasterFileSystem().getRootDir();
+    LOG.debug("FS state after snapshot:");
+    FSUtils.logFileSystemState(UTIL.getTestFileSystem(),
+      FSUtils.getRootDir(UTIL.getConfiguration()), LOG);
+
+    SnapshotTestingUtils.confirmSnapshotValid(snapshots.get(0), TABLE_NAME, TEST_FAM, rootDir,
+      admin, fs, false, new Path(rootDir, HConstants.HREGION_LOGDIR_NAME), snapshotServers);
+
+    admin.deleteSnapshot(snapshot);
+    snapshots = admin.listSnapshots();
+    SnapshotTestingUtils.assertNoSnapshots(admin);
+  }
+
+  @Test
+  public void testSnapshotFailsOnNonExistantTable() throws Exception {
+    HBaseAdmin admin = UTIL.getHBaseAdmin();
+    // make sure we don't fail on listing snapshots
+    SnapshotTestingUtils.assertNoSnapshots(admin);
+    String tableName = "_not_a_table";
+
+    // make sure the table doesn't exist
+    boolean fail = false;
+    do {
+    try {
+    admin.getTableDescriptor(Bytes.toBytes(tableName));
+    fail = true;
+        LOG.error("Table:" + tableName + " already exists, checking a new name");
+    tableName = tableName+"!";
+    }catch(TableNotFoundException e) {
+      fail = false;
+      }
+    } while (fail);
+
+    // snapshot the non-existant table
+    try {
+      admin.snapshot("fail", tableName);
+      fail("Snapshot succeeded even though there is not table.");
+    } catch (SnapshotCreationException e) {
+      LOG.info("Correctly failed to snapshot a non-existant table:" + e.getMessage());
+    }
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java b/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
new file mode 100644
index 0000000..bba7d37
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
@@ -0,0 +1,380 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.cleaner;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate;
+import org.apache.hadoop.hbase.master.cleaner.HFileCleaner;
+import org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler;
+import org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
+import org.apache.hadoop.hbase.snapshot.exception.UnknownSnapshotException;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.HFileArchiveUtil;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+import com.google.common.collect.Lists;
+
+/**
+ * Test the master-related aspects of a snapshot
+ */
+@Category(MediumTests.class)
+public class TestSnapshotFromMaster {
+
+  private static final Log LOG = LogFactory.getLog(TestSnapshotFromMaster.class);
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+  private static final int NUM_RS = 2;
+  private static Path rootDir;
+  private static Path snapshots;
+  private static Path archiveDir;
+  private static FileSystem fs;
+  private static HMaster master;
+
+  private static final String STRING_TABLE_NAME = "test";
+  private static final byte[] TEST_FAM = Bytes.toBytes("fam");
+  private static final byte[] TABLE_NAME = Bytes.toBytes(STRING_TABLE_NAME);
+  // refresh the cache every 1/2 second
+  private static final long cacheRefreshPeriod = 500;
+
+  /**
+   * Setup the config for the cluster
+   */
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    setupConf(UTIL.getConfiguration());
+    UTIL.startMiniCluster(NUM_RS);
+    fs = UTIL.getDFSCluster().getFileSystem();
+    rootDir = FSUtils.getRootDir(UTIL.getConfiguration());
+    snapshots = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);
+    archiveDir = new Path(rootDir, ".archive");
+    master = UTIL.getMiniHBaseCluster().getMaster();
+  }
+
+  private static void setupConf(Configuration conf) {
+    // disable the ui
+    conf.setInt("hbase.regionsever.info.port", -1);
+    // change the flush size to a small amount, regulating number of store files
+    conf.setInt("hbase.hregion.memstore.flush.size", 25000);
+    // so make sure we get a compaction when doing a load, but keep around some
+    // files in the store
+    conf.setInt("hbase.hstore.compaction.min", 5);
+    conf.setInt("hbase.hstore.compactionThreshold", 5);
+    // block writes if we get to 12 store files
+    conf.setInt("hbase.hstore.blockingStoreFiles", 12);
+    // drop the number of attempts for the hbase admin
+    conf.setInt("hbase.client.retries.number", 1);
+    // set the only HFile cleaner as the snapshot cleaner
+    conf.setStrings(HFileCleaner.MASTER_HFILE_CLEANER_PLUGINS,
+      SnapshotHFileCleaner.class.getCanonicalName());
+    conf.setLong(SnapshotHFileCleaner.HFILE_CACHE_REFRESH_PERIOD_CONF_KEY, cacheRefreshPeriod);
+  }
+
+  @Before
+  public void setup() throws Exception {
+    UTIL.createTable(TABLE_NAME, TEST_FAM);
+    master.getSnapshotManagerForTesting().setSnapshotHandlerForTesting(null);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+
+    UTIL.deleteTable(TABLE_NAME);
+
+    // delete the archive directory, if its exists
+    if (fs.exists(archiveDir)) {
+      if (!fs.delete(archiveDir, true)) {
+        throw new IOException("Couldn't delete archive directory (" + archiveDir
+            + " for an unknown reason");
+      }
+    }
+
+    // delete the snapshot directory, if its exists
+    if (fs.exists(snapshots)) {
+      if (!fs.delete(snapshots, true)) {
+        throw new IOException("Couldn't delete snapshots directory (" + snapshots
+            + " for an unknown reason");
+      }
+    }
+  }
+
+  @AfterClass
+  public static void cleanupTest() throws Exception {
+    try {
+      UTIL.shutdownMiniCluster();
+    } catch (Exception e) {
+      // NOOP;
+    }
+  }
+
+  /**
+   * Test that the contract from the master for checking on a snapshot are valid.
+   * <p>
+   * <ol>
+   * <li>If a snapshot fails with an error, we expect to get the source error.</li>
+   * <li>If there is no snapshot name supplied, we should get an error.</li>
+   * <li>If asking about a snapshot has hasn't occurred, you should get an error.</li>
+   * </ol>
+   */
+  @Test(timeout = 15000)
+  public void testIsDoneContract() throws Exception {
+
+    String snapshotName = "asyncExpectedFailureTest";
+
+    // check that we get an exception when looking up snapshot where one hasn't happened
+    SnapshotTestingUtils.expectSnapshotDoneException(master, new HSnapshotDescription(),
+      UnknownSnapshotException.class);
+
+    // and that we get the same issue, even if we specify a name
+    SnapshotDescription desc = SnapshotDescription.newBuilder()
+      .setName(snapshotName).build();
+    SnapshotTestingUtils.expectSnapshotDoneException(master, new HSnapshotDescription(desc),
+      UnknownSnapshotException.class);
+
+    // set a mock handler to simulate a snapshot
+    DisabledTableSnapshotHandler mockHandler = Mockito.mock(DisabledTableSnapshotHandler.class);
+    Mockito.when(mockHandler.getExceptionIfFailed()).thenReturn(null);
+    Mockito.when(mockHandler.getSnapshot()).thenReturn(desc);
+    Mockito.when(mockHandler.isFinished()).thenReturn(new Boolean(true));
+
+    master.getSnapshotManagerForTesting().setSnapshotHandlerForTesting(mockHandler);
+
+    // if we do a lookup without a snapshot name, we should fail - you should always know your name
+    SnapshotTestingUtils.expectSnapshotDoneException(master, new HSnapshotDescription(),
+      UnknownSnapshotException.class);
+
+    // then do the lookup for the snapshot that it is done
+    boolean isDone = master.isSnapshotDone(new HSnapshotDescription(desc));
+    assertTrue("Snapshot didn't complete when it should have.", isDone);
+
+    // now try the case where we are looking for a snapshot we didn't take
+    desc = SnapshotDescription.newBuilder().setName("Not A Snapshot").build();
+    SnapshotTestingUtils.expectSnapshotDoneException(master, new HSnapshotDescription(desc),
+      UnknownSnapshotException.class);
+
+    // then create a snapshot to the fs and make sure that we can find it when checking done
+    snapshotName = "completed";
+    FileSystem fs = master.getMasterFileSystem().getFileSystem();
+    Path root = master.getMasterFileSystem().getRootDir();
+    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, root);
+    desc = desc.toBuilder().setName(snapshotName).build();
+    SnapshotDescriptionUtils.writeSnapshotInfo(desc, snapshotDir, fs);
+
+    isDone = master.isSnapshotDone(new HSnapshotDescription(desc));
+    assertTrue("Completed, on-disk snapshot not found", isDone);
+
+    HBaseSnapshotException testException = new SnapshotCreationException("test fail", desc);
+    Mockito.when(mockHandler.getExceptionIfFailed()).thenReturn(testException);
+    try {
+      master.isSnapshotDone(new HSnapshotDescription(desc));
+      fail("Master should have passed along snapshot error, but didn't");
+    } catch (IOException e) {
+      LOG.debug("Correctly got exception back from the master on failure: " + e.getMessage());
+    }
+  }
+
+  @Test
+  public void testListSnapshots() throws Exception {
+    // first check when there are no snapshots
+    List<HSnapshotDescription> snapshots = master.listSnapshots();
+    assertEquals("Found unexpected number of snapshots", 0, snapshots.size());
+
+    // write one snapshot to the fs
+    String snapshotName = "completed";
+    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName(snapshotName).build();
+    SnapshotDescriptionUtils.writeSnapshotInfo(snapshot, snapshotDir, fs);
+
+    // check that we get one snapshot
+    snapshots = master.listSnapshots();
+    assertEquals("Found unexpected number of snapshots", 1, snapshots.size());
+    List<HSnapshotDescription> expected = Lists.newArrayList(new HSnapshotDescription(snapshot));
+    assertEquals("Returned snapshots don't match created snapshots", expected, snapshots);
+
+    // write a second snapshot
+    snapshotName = "completed_two";
+    snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
+    snapshot = SnapshotDescription.newBuilder().setName(snapshotName).build();
+    SnapshotDescriptionUtils.writeSnapshotInfo(snapshot, snapshotDir, fs);
+    expected.add(new HSnapshotDescription(snapshot));
+
+    // check that we get one snapshot
+    snapshots = master.listSnapshots();
+    assertEquals("Found unexpected number of snapshots", 2, snapshots.size());
+    assertEquals("Returned snapshots don't match created snapshots", expected, snapshots);
+  }
+
+  @Test
+  public void testDeleteSnapshot() throws Exception {
+
+    String snapshotName = "completed";
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName(snapshotName).build();
+
+    try {
+      master.deleteSnapshot(new HSnapshotDescription(snapshot));
+      fail("Master didn't throw exception when attempting to delete snapshot that doesn't exist");
+    } catch (IOException e) {
+      LOG.debug("Correctly failed delete of non-existant snapshot:" + e.getMessage());
+    }
+
+    // write one snapshot to the fs
+    Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
+    SnapshotDescriptionUtils.writeSnapshotInfo(snapshot, snapshotDir, fs);
+
+    // then delete the existing snapshot,which shouldn't cause an exception to be thrown
+    master.deleteSnapshot(new HSnapshotDescription(snapshot));
+  }
+
+  /**
+   * Test that the snapshot hfile archive cleaner works correctly. HFiles that are in snapshots
+   * should be retained, while those that are not in a snapshot should be deleted.
+   * @throws Exception on failure
+   */
+  @Test
+  public void testSnapshotHFileArchiving() throws Exception {
+    HBaseAdmin admin = UTIL.getHBaseAdmin();
+    // make sure we don't fail on listing snapshots
+    SnapshotTestingUtils.assertNoSnapshots(admin);
+    // load the table
+    UTIL.loadTable(new HTable(UTIL.getConfiguration(), TABLE_NAME), TEST_FAM);
+
+    // disable the table so we can take a snapshot
+    admin.disableTable(TABLE_NAME);
+
+    // take a snapshot of the table
+    String snapshotName = "snapshot";
+    byte[] snapshotNameBytes = Bytes.toBytes(snapshotName);
+    admin.snapshot(snapshotNameBytes, TABLE_NAME);
+
+    Configuration conf = UTIL.getConfiguration();
+    Path rootDir = FSUtils.getRootDir(conf);
+    FileSystem fs = FSUtils.getCurrentFileSystem(conf);
+
+    FSUtils.logFileSystemState(fs, rootDir, LOG);
+
+    // ensure we only have one snapshot
+    SnapshotTestingUtils.assertOneSnapshotThatMatches(admin, snapshotNameBytes, TABLE_NAME);
+
+    // renable the table so we can compact the regions
+    admin.enableTable(TABLE_NAME);
+
+    // compact the files so we get some archived files for the table we just snapshotted
+    List<HRegion> regions = UTIL.getHBaseCluster().getRegions(TABLE_NAME);
+    for (HRegion region : regions) {
+      region.compactStores();
+    }
+
+    // make sure the cleaner has run
+    LOG.debug("Running hfile cleaners");
+    ensureHFileCleanersRun();
+
+    // get the snapshot files for the table
+    Path snapshotTable = SnapshotDescriptionUtils.getCompletedSnapshotDir(snapshotName, rootDir);
+    FileStatus[] snapshotHFiles = SnapshotTestingUtils.listHFiles(fs, snapshotTable);
+    // check that the files in the archive contain the ones that we need for the snapshot
+    LOG.debug("Have snapshot hfiles:");
+    for (FileStatus file : snapshotHFiles) {
+      LOG.debug(file.getPath());
+    }
+    // get the archived files for the table
+    Collection<String> files = getArchivedHFiles(conf, rootDir, fs, STRING_TABLE_NAME);
+
+    // and make sure that there is a proper subset
+    for (FileStatus file : snapshotHFiles) {
+      assertTrue("Archived hfiles " + files + " is missing snapshot file:" + file.getPath(),
+        files.contains(file.getPath().getName()));
+    }
+
+    // delete the existing snapshot
+    admin.deleteSnapshot(snapshotNameBytes);
+    SnapshotTestingUtils.assertNoSnapshots(admin);
+
+    // make sure that we don't keep around the hfiles that aren't in a snapshot
+    // make sure we wait long enough to refresh the snapshot hfile
+    List<BaseHFileCleanerDelegate> delegates = UTIL.getMiniHBaseCluster().getMaster()
+        .getHFileCleaner().cleanersChain;
+    ((SnapshotHFileCleaner) delegates.get(0)).getFileCacheForTesting()
+        .triggerCacheRefreshForTesting();
+    // run the cleaner again
+    LOG.debug("Running hfile cleaners");
+    ensureHFileCleanersRun();
+
+    files = getArchivedHFiles(conf, rootDir, fs, STRING_TABLE_NAME);
+    assertEquals("Still have some hfiles in the archive, when their snapshot has been deleted.", 0,
+      files.size());
+  }
+
+  /**
+   * @return all the HFiles for a given table that have been archived
+   * @throws IOException on expected failure
+   */
+  private final Collection<String> getArchivedHFiles(Configuration conf, Path rootDir,
+      FileSystem fs, String tableName) throws IOException {
+    Path tableArchive = HFileArchiveUtil.getTableArchivePath(new Path(rootDir, tableName));
+    FileStatus[] archivedHFiles = SnapshotTestingUtils.listHFiles(fs, tableArchive);
+    List<String> files = new ArrayList<String>(archivedHFiles.length);
+    LOG.debug("Have archived hfiles:");
+    for (FileStatus file : archivedHFiles) {
+      LOG.debug(file.getPath());
+      files.add(file.getPath().getName());
+    }
+    // sort the archived files
+
+    Collections.sort(files);
+    return files;
+  }
+
+  /**
+   * Make sure the {@link HFileCleaner HFileCleaners} run at least once
+   */
+  private static void ensureHFileCleanersRun() {
+    UTIL.getHBaseCluster().getMaster().getHFileCleaner().chore();
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java b/src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java
new file mode 100644
index 0000000..83b8cc7
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/master/snapshot/manage/TestSnapshotManager.java
@@ -0,0 +1,132 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.snapshot.manage;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.TableDescriptors;
+import org.apache.hadoop.hbase.executor.ExecutorService;
+import org.apache.hadoop.hbase.master.MasterFileSystem;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.SnapshotSentinel;
+import org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.exception.SnapshotCreationException;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+/**
+ * Test basic snapshot manager functionality
+ */
+@Category(SmallTests.class)
+public class TestSnapshotManager {
+  private static final Log LOG = LogFactory.getLog(TestSnapshotManager.class);
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  MasterServices services = Mockito.mock(MasterServices.class);
+  ZooKeeperWatcher watcher = Mockito.mock(ZooKeeperWatcher.class);
+  ExecutorService pool = Mockito.mock(ExecutorService.class);
+  MasterFileSystem mfs = Mockito.mock(MasterFileSystem.class);
+  FileSystem fs;
+  {
+    try {
+      fs = UTIL.getTestFileSystem();
+    } catch (IOException e) {
+      throw new RuntimeException("Couldn't get test filesystem", e);
+    }
+
+  }
+
+  private SnapshotManager getNewManager() throws KeeperException {
+    Mockito.reset(services, watcher, pool);
+    Mockito.when(services.getMasterFileSystem()).thenReturn(mfs);
+    Mockito.when(mfs.getFileSystem()).thenReturn(fs);
+    Mockito.when(mfs.getRootDir()).thenReturn(UTIL.getDataTestDir());
+    return new SnapshotManager(services, watcher, pool);
+  }
+
+
+
+  @Test
+  public void testInProcess() throws KeeperException, SnapshotCreationException {
+    SnapshotManager manager = getNewManager();
+    SnapshotSentinel handler = Mockito.mock(SnapshotSentinel.class);
+    assertFalse("Manager is in process when there is no current handler", manager.isTakingSnapshot());
+    manager.setSnapshotHandlerForTesting(handler);
+    Mockito.when(handler.isFinished()).thenReturn(false);
+    assertTrue("Manager isn't in process when handler is running", manager.isTakingSnapshot());
+    Mockito.when(handler.isFinished()).thenReturn(true);
+    assertFalse("Manager is process when handler isn't running", manager.isTakingSnapshot());
+  }
+
+  /**
+   * Test that we stop the running disabled table snapshot by passing along an error to the error
+   * handler.
+   * @throws Exception
+   */
+  @Test
+  public void testStopPropagation() throws Exception {
+    // create a new orchestrator and hook up a listener
+    SnapshotManager manager = getNewManager();
+    FSUtils.setRootDir(UTIL.getConfiguration(), UTIL.getDataTestDir());
+
+    // setup a mock snapshot to run
+    String tableName = "some table";
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("testAbort")
+        .setTable(tableName).build();
+    // mock out all the expected call to the master services
+    // this allows us to run must faster and without using a minicluster
+
+    // ensure the table exists when we ask for it
+    TableDescriptors tables = Mockito.mock(TableDescriptors.class);
+    Mockito.when(services.getTableDescriptors()).thenReturn(tables);
+    HTableDescriptor descriptor = Mockito.mock(HTableDescriptor.class);
+    Mockito.when(tables.get(Mockito.anyString())).thenReturn(descriptor);
+
+    // return the local file system as the backing to the MasterFileSystem
+    MasterFileSystem mfs = Mockito.mock(MasterFileSystem.class);
+    Mockito.when(mfs.getFileSystem()).thenReturn(UTIL.getTestFileSystem());
+    Mockito.when(services.getMasterFileSystem()).thenReturn(mfs);
+    Mockito.when(services.getConfiguration()).thenReturn(UTIL.getConfiguration());
+
+    // create a new handler that we will check for errors
+    manager.snapshotDisabledTable(snapshot);
+    // make sure we submitted the handler, but because its mocked, it doesn't run it.
+    Mockito.verify(pool, Mockito.times(1)).submit(Mockito.any(DisabledTableSnapshotHandler.class));
+
+    // pass along the stop notification
+    manager.stop("stopping for test");
+    SnapshotSentinel handler = manager.getCurrentSnapshotSentinel();
+    assertNotNull("Snare didn't receive error notification from snapshot manager.",
+      handler.getExceptionIfFailed());
+  }
+}
\ No newline at end of file
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/error/TestSnapshotExceptionSnare.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/error/TestSnapshotExceptionSnare.java
new file mode 100644
index 0000000..0699057
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/server/snapshot/error/TestSnapshotExceptionSnare.java
@@ -0,0 +1,74 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.error;
+
+import static org.junit.Assert.fail;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.errorhandling.ExceptionListener;
+import org.apache.hadoop.hbase.server.errorhandling.OperationAttemptTimer;
+import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.junit.Test;
+
+/**
+ * Test the exception snare propagates errors as expected
+ */
+public class TestSnapshotExceptionSnare {
+
+  private static final Log LOG = LogFactory.getLog(TestSnapshotExceptionSnare.class);
+
+  /**
+   * This test ensures that we only propagate snapshot exceptions, even if we don't get a snapshot
+   * exception
+   */
+  @Test
+  @SuppressWarnings({ "rawtypes", "unchecked" })
+  public void testPropagatesOnlySnapshotException() {
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
+    ExceptionListener snare = new SnapshotExceptionSnare(snapshot);
+    snare.receiveError("Some message", new Exception());
+    try {
+      ((SnapshotExceptionSnare) snare).failOnError();
+      fail("Snare didn't throw an exception");
+    } catch (HBaseSnapshotException e) {
+      LOG.error("Correctly got a snapshot exception" + e);
+    }
+  }
+
+  @Test
+  public void testPropatesTimerError() {
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
+    SnapshotExceptionSnare snare = new SnapshotExceptionSnare(snapshot);
+    Configuration conf = new Configuration();
+    // don't let the timer count down before we fire it off
+    conf.setLong(SnapshotDescriptionUtils.MASTER_WAIT_TIME_DISABLED_SNAPSHOT, Long.MAX_VALUE);
+    OperationAttemptTimer timer = TakeSnapshotUtils.getMasterTimerAndBindToMonitor(snapshot, conf,
+      snare);
+    timer.trigger();
+    try {
+      snare.failOnError();
+    } catch (HBaseSnapshotException e) {
+      LOG.info("Correctly failed from timer:" + e.getMessage());
+    }
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java
new file mode 100644
index 0000000..cf9ac9e
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestCopyRecoveredEditsTask.java
@@ -0,0 +1,128 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+/**
+ * Test that we correctly copy the recovered edits from a directory
+ */
+@Category(SmallTests.class)
+public class TestCopyRecoveredEditsTask {
+
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  @Test
+  public void testCopyFiles() throws Exception {
+
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
+    SnapshotExceptionSnare monitor = Mockito.mock(SnapshotExceptionSnare.class);
+    FileSystem fs = UTIL.getTestFileSystem();
+    Path root = UTIL.getDataTestDir();
+    String regionName = "regionA";
+    Path regionDir = new Path(root, regionName);
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, root);
+
+    try {
+      // doesn't really matter where the region's snapshot directory is, but this is pretty close
+      Path snapshotRegionDir = new Path(workingDir, regionName);
+      fs.mkdirs(snapshotRegionDir);
+
+      // put some stuff in the recovered.edits directory
+      Path edits = HLog.getRegionDirRecoveredEditsDir(regionDir);
+      fs.mkdirs(edits);
+      // make a file with some data
+      Path file1 = new Path(edits, "0000000000000002352");
+      FSDataOutputStream out = fs.create(file1);
+      byte[] data = new byte[] { 1, 2, 3, 4 };
+      out.write(data);
+      out.close();
+      // make an empty file
+      Path empty = new Path(edits, "empty");
+      fs.createNewFile(empty);
+
+      CopyRecoveredEditsTask task = new CopyRecoveredEditsTask(snapshot, monitor, fs, regionDir,
+          snapshotRegionDir);
+      task.run();
+
+      Path snapshotEdits = HLog.getRegionDirRecoveredEditsDir(snapshotRegionDir);
+      FileStatus[] snapshotEditFiles = FSUtils.listStatus(fs, snapshotEdits);
+      assertEquals("Got wrong number of files in the snapshot edits", 1, snapshotEditFiles.length);
+      FileStatus file = snapshotEditFiles[0];
+      assertEquals("Didn't copy expected file", file1.getName(), file.getPath().getName());
+
+      Mockito.verify(monitor, Mockito.never()).receiveError(Mockito.anyString(),
+        Mockito.any(HBaseSnapshotException.class));
+      Mockito.verify(monitor, Mockito.never()).snapshotFailure(Mockito.anyString(),
+        Mockito.any(SnapshotDescription.class));
+      Mockito.verify(monitor, Mockito.never()).snapshotFailure(Mockito.anyString(),
+        Mockito.any(SnapshotDescription.class), Mockito.any(Exception.class));
+    } finally {
+      // cleanup the working directory
+      FSUtils.delete(fs, regionDir, true);
+      FSUtils.delete(fs, workingDir, true);
+    }
+  }
+
+  /**
+   * Check that we don't get an exception if there is no recovered edits directory to copy
+   * @throws Exception on failure
+   */
+  @Test
+  public void testNoEditsDir() throws Exception {
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
+    SnapshotExceptionSnare monitor = Mockito.mock(SnapshotExceptionSnare.class);
+    FileSystem fs = UTIL.getTestFileSystem();
+    Path root = UTIL.getDataTestDir();
+    String regionName = "regionA";
+    Path regionDir = new Path(root, regionName);
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, root);
+    try {
+      // doesn't really matter where the region's snapshot directory is, but this is pretty close
+      Path snapshotRegionDir = new Path(workingDir, regionName);
+      fs.mkdirs(snapshotRegionDir);
+      Path regionEdits = HLog.getRegionDirRecoveredEditsDir(regionDir);
+      assertFalse("Edits dir exists already - it shouldn't", fs.exists(regionEdits));
+
+      CopyRecoveredEditsTask task = new CopyRecoveredEditsTask(snapshot, monitor, fs, regionDir,
+          snapshotRegionDir);
+      task.run();
+    } finally {
+      // cleanup the working directory
+      FSUtils.delete(fs, regionDir, true);
+      FSUtils.delete(fs, workingDir, true);
+    }
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java
new file mode 100644
index 0000000..aa652ba
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestReferenceRegionHFilesTask.java
@@ -0,0 +1,92 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+@Category(SmallTests.class)
+public class TestReferenceRegionHFilesTask {
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  @Test
+  public void testRun() throws IOException {
+    FileSystem fs = UTIL.getTestFileSystem();
+    // setup the region internals
+    Path testdir = UTIL.getDataTestDir();
+    Path regionDir = new Path(testdir, "region");
+    Path family1 = new Path(regionDir, "fam1");
+    // make an empty family
+    Path family2 = new Path(regionDir, "fam2");
+    fs.mkdirs(family2);
+
+    // add some files to family 1
+    Path file1 = new Path(family1, "05f99689ae254693836613d1884c6b63");
+    fs.createNewFile(file1);
+    Path file2 = new Path(family1, "7ac9898bf41d445aa0003e3d699d5d26");
+    fs.createNewFile(file2);
+
+    // create the snapshot directory
+    Path snapshotRegionDir = new Path(testdir, HConstants.SNAPSHOT_DIR_NAME);
+    fs.mkdirs(snapshotRegionDir);
+
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("name")
+        .setTable("table").build();
+    SnapshotExceptionSnare monitor = Mockito.mock(SnapshotExceptionSnare.class);
+    ReferenceRegionHFilesTask task = new ReferenceRegionHFilesTask(snapshot, monitor, regionDir,
+        fs, snapshotRegionDir);
+    task.run();
+
+    // make sure we never get an error
+    Mockito.verify(monitor, Mockito.never()).snapshotFailure(Mockito.anyString(),
+      Mockito.eq(snapshot));
+    Mockito.verify(monitor, Mockito.never()).snapshotFailure(Mockito.anyString(),
+      Mockito.eq(snapshot), Mockito.any(Exception.class));
+
+    // verify that all the hfiles get referenced
+    List<String> hfiles = new ArrayList<String>(2);
+    FileStatus[] regions = FSUtils.listStatus(fs, snapshotRegionDir);
+    for (FileStatus region : regions) {
+      FileStatus[] fams = FSUtils.listStatus(fs, region.getPath());
+      for (FileStatus fam : fams) {
+        FileStatus[] files = FSUtils.listStatus(fs, fam.getPath());
+        for (FileStatus file : files) {
+          hfiles.add(file.getPath().getName());
+        }
+      }
+    }
+    assertTrue("Didn't reference :" + file1, hfiles.contains(file1.getName()));
+    assertTrue("Didn't reference :" + file1, hfiles.contains(file2.getName()));
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java
new file mode 100644
index 0000000..6a6ffc7
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestSnapshotTask.java
@@ -0,0 +1,52 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+@Category(SmallTests.class)
+public class TestSnapshotTask {
+
+  /**
+   * Check that errors from running the task get propagated back to the error listener.
+   */
+  @Test
+  public void testErrorPropagationg() {
+    SnapshotExceptionSnare error = Mockito.mock(SnapshotExceptionSnare.class);
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot")
+        .setTable("table").build();
+    final Exception thrown = new Exception("Failed!");
+    SnapshotTask fail = new SnapshotTask(snapshot, error, "always fails") {
+
+      @Override
+      protected void process() throws Exception {
+        throw thrown;
+      }
+    };
+    fail.run();
+
+    Mockito.verify(error, Mockito.times(1)).snapshotFailure(Mockito.anyString(),
+      Mockito.eq(snapshot), Mockito.eq(thrown));
+  }
+
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java
new file mode 100644
index 0000000..dfc8f93
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/server/snapshot/task/TestWALReferenceTask.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.server.snapshot.task;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.server.snapshot.error.SnapshotExceptionSnare;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.junit.Test;
+import org.mockito.Mockito;
+
+/**
+ * Test that the WAL reference task works as expected
+ */
+public class TestWALReferenceTask {
+
+  private static final Log LOG = LogFactory.getLog(TestWALReferenceTask.class);
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+  @Test
+  public void testRun() throws IOException {
+    Configuration conf = UTIL.getConfiguration();
+    FileSystem fs = UTIL.getTestFileSystem();
+    // setup the log dir
+    Path testDir = UTIL.getDataTestDir();
+    Set<String> servers = new HashSet<String>();
+    Path logDir = new Path(testDir, ".logs");
+    Path server1Dir = new Path(logDir, "Server1");
+    servers.add(server1Dir.getName());
+    Path server2Dir = new Path(logDir, "me.hbase.com,56073,1348618509968");
+    servers.add(server2Dir.getName());
+    // logs under server 1
+    Path log1_1 = new Path(server1Dir, "me.hbase.com%2C56073%2C1348618509968.1348618520536");
+    Path log1_2 = new Path(server1Dir, "me.hbase.com%2C56073%2C1348618509968.1234567890123");
+    // logs under server 2
+    Path log2_1 = new Path(server2Dir, "me.hbase.com%2C56074%2C1348618509998.1348618515589");
+    Path log2_2 = new Path(server2Dir, "me.hbase.com%2C56073%2C1348618509968.1234567890123");
+
+    // create all the log files
+    fs.createNewFile(log1_1);
+    fs.createNewFile(log1_2);
+    fs.createNewFile(log2_1);
+    fs.createNewFile(log2_2);
+
+    FSUtils.logFileSystemState(fs, testDir, LOG);
+    FSUtils.setRootDir(conf, testDir);
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder()
+        .setName("testWALReferenceSnapshot").build();
+    SnapshotExceptionSnare listener = Mockito.mock(SnapshotExceptionSnare.class);
+
+    // reference all the files in the first server directory
+    ReferenceServerWALsTask task = new ReferenceServerWALsTask(snapshot, listener, server1Dir,
+        conf, fs);
+    task.run();
+
+    // reference all the files in the first server directory
+    task = new ReferenceServerWALsTask(snapshot, listener, server2Dir, conf, fs);
+    task.run();
+
+    // verify that we got everything
+    FSUtils.logFileSystemState(fs, testDir, LOG);
+    Path workingDir = SnapshotDescriptionUtils.getWorkingSnapshotDir(snapshot, testDir);
+    Path snapshotLogDir = new Path(workingDir, HConstants.HREGION_LOGDIR_NAME);
+
+    // make sure we reference the all the wal files
+    TakeSnapshotUtils.verifyAllLogsGotReferenced(fs, logDir, servers, snapshot, snapshotLogDir);
+
+    // make sure we never got an error
+    Mockito.verify(listener, Mockito.atLeastOnce()).failOnError();
+    Mockito.verifyNoMoreInteractions(listener);
+  }
+}
\ No newline at end of file
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java b/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
new file mode 100644
index 0000000..91a9f6e
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
@@ -0,0 +1,187 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils;
+import org.apache.hadoop.hbase.snapshot.HSnapshotDescription;
+import org.apache.hadoop.hbase.snapshot.exception.HBaseSnapshotException;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSTableDescriptors;
+import org.junit.Assert;
+
+/**
+ * Utilities class for snapshots
+ */
+public class SnapshotTestingUtils {
+
+  private static final Log LOG = LogFactory.getLog(SnapshotTestingUtils.class);
+
+  /**
+   * Assert that we don't have any snapshots lists
+   * @throws IOException if the admin operation fails
+   */
+  public static void assertNoSnapshots(HBaseAdmin admin) throws IOException {
+    assertEquals("Have some previous snapshots", 0, admin.listSnapshots().size());
+  }
+
+  /**
+   * Make sure that there is only one snapshot returned from the master and its name and table match
+   * the passed in parameters.
+   */
+  public static void assertOneSnapshotThatMatches(HBaseAdmin admin, HSnapshotDescription snapshot)
+      throws IOException {
+    assertOneSnapshotThatMatches(admin, snapshot.getName(), snapshot.getTable());
+  }
+
+  /**
+   * Make sure that there is only one snapshot returned from the master and its name and table match
+   * the passed in parameters.
+   */
+  public static void assertOneSnapshotThatMatches(HBaseAdmin admin, SnapshotDescription snapshot)
+      throws IOException {
+    assertOneSnapshotThatMatches(admin, snapshot.getName(), snapshot.getTable());
+  }
+
+  /**
+   * Make sure that there is only one snapshot returned from the master and its name and table match
+   * the passed in parameters.
+   */
+  public static List<SnapshotDescription> assertOneSnapshotThatMatches(HBaseAdmin admin,
+      String snapshotName, String tableName) throws IOException {
+    // list the snapshot
+    List<SnapshotDescription> snapshots = admin.listSnapshots();
+
+    assertEquals("Should only have 1 snapshot", 1, snapshots.size());
+    assertEquals(snapshotName, snapshots.get(0).getName());
+    assertEquals(tableName, snapshots.get(0).getTable());
+
+    return snapshots;
+  }
+
+  /**
+   * Make sure that there is only one snapshot returned from the master and its name and table match
+   * the passed in parameters.
+   */
+  public static List<SnapshotDescription> assertOneSnapshotThatMatches(HBaseAdmin admin,
+      byte[] snapshot, byte[] tableName) throws IOException {
+    return assertOneSnapshotThatMatches(admin, Bytes.toString(snapshot), Bytes.toString(tableName));
+  }
+
+  /**
+   * Confirm that the snapshot contains references to all the files that should be in the snapshot
+   */
+  public static void confirmSnapshotValid(SnapshotDescription snapshotDescriptor,
+      byte[] tableName, byte[] testFamily, Path rootDir, HBaseAdmin admin, FileSystem fs,
+      boolean requireLogs, Path logsDir, Set<String> snapshotServers) throws IOException {
+    Path snapshotDir = SnapshotDescriptionUtils
+        .getCompletedSnapshotDir(snapshotDescriptor, rootDir);
+    assertTrue(fs.exists(snapshotDir));
+    Path snapshotinfo = new Path(snapshotDir, SnapshotDescriptionUtils.SNAPSHOTINFO_FILE);
+    assertTrue(fs.exists(snapshotinfo));
+    // check the logs dir
+    if (requireLogs) {
+      TakeSnapshotUtils.verifyAllLogsGotReferenced(fs, logsDir, snapshotServers,
+        snapshotDescriptor, new Path(snapshotDir, HConstants.HREGION_LOGDIR_NAME));
+    }
+    // check the table info
+    HTableDescriptor desc = FSTableDescriptors.getTableDescriptor(fs, rootDir, tableName);
+    HTableDescriptor snapshotDesc = FSTableDescriptors.getTableDescriptor(fs, snapshotDir);
+    assertEquals(desc, snapshotDesc);
+
+    // check the region snapshot for all the regions
+    List<HRegionInfo> regions = admin.getTableRegions(tableName);
+    for (HRegionInfo info : regions) {
+      String regionName = info.getEncodedName();
+      Path regionDir = new Path(snapshotDir, regionName);
+      HRegionInfo snapshotRegionInfo = HRegion.loadDotRegionInfoFileContent(fs, regionDir);
+      assertEquals(info, snapshotRegionInfo);
+      // check to make sure we have the family
+      Path familyDir = new Path(regionDir, Bytes.toString(testFamily));
+      assertTrue("Expected to find: " + familyDir + ", but it doesn't exist", fs.exists(familyDir));
+      // make sure we have some files references
+      assertTrue(fs.listStatus(familyDir).length > 0);
+    }
+  }
+
+  /**
+   * Helper method for testing async snapshot operations. Just waits for the given snapshot to
+   * complete on the server by repeatedly checking the master.
+   * @param master running the snapshot
+   * @param snapshot to check
+   * @param sleep amount to sleep between checks to see if the snapshot is done
+   * @throws IOException if the snapshot fails
+   */
+  public static void waitForSnapshotToComplete(HMaster master, HSnapshotDescription snapshot,
+      long sleep) throws IOException {
+    boolean done = false;
+    while (!done) {
+      done = master.isSnapshotDone(snapshot);
+      try {
+        Thread.sleep(sleep);
+      } catch (InterruptedException e) {
+        throw new IOException(e);
+      }
+    }
+  }
+
+  public static void cleanupSnapshot(HBaseAdmin admin, byte[] tableName) throws IOException {
+    SnapshotTestingUtils.cleanupSnapshot(admin, Bytes.toString(tableName));
+  }
+
+  public static void cleanupSnapshot(HBaseAdmin admin, String snapshotName) throws IOException {
+    // delete the taken snapshot
+    admin.deleteSnapshot(snapshotName);
+    assertNoSnapshots(admin);
+  }
+
+  /**
+   * Expect the snapshot to throw an error when checking if the snapshot is complete
+   * @param master master to check
+   * @param snapshot the {@link HSnapshotDescription} request to pass to the master
+   * @param clazz expected exception from the master
+   */
+  public static void expectSnapshotDoneException(HMaster master, HSnapshotDescription snapshot,
+      Class<? extends HBaseSnapshotException> clazz) {
+    try {
+      boolean res = master.isSnapshotDone(snapshot);
+      Assert.fail("didn't fail to lookup a snapshot: res=" + res);
+    } catch (HBaseSnapshotException e) {
+      assertEquals("Threw wrong snapshot exception!", clazz, e.getClass());
+    } catch (Throwable t) {
+      Assert.fail("Threw an unexpected exception:" + t);
+    }
+  }
+}
diff --git a/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotDescriptionUtils.java b/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotDescriptionUtils.java
new file mode 100644
index 0000000..778bcf0
--- /dev/null
+++ b/src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotDescriptionUtils.java
@@ -0,0 +1,138 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.snapshot;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription.Type;
+import org.apache.hadoop.hbase.util.EnvironmentEdge;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManagerTestHelper;
+import org.junit.After;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+/**
+ * Test that the {@link SnapshotDescription} helper is helping correctly.
+ */
+@Category(SmallTests.class)
+public class TestSnapshotDescriptionUtils {
+  private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+  private static FileSystem fs;
+  private static Path root;
+
+  @BeforeClass
+  public static void setupFS() throws Exception {
+    fs = UTIL.getTestFileSystem();
+    root = new Path(UTIL.getDataTestDir(), "hbase");
+  }
+
+  @After
+  public void cleanupFS() throws Exception {
+    if (fs.exists(root)) {
+    if (!fs.delete(root, true)) {
+      throw new IOException("Failed to delete root test dir: " + root);
+    }
+    if (!fs.mkdirs(root)) {
+      throw new IOException("Failed to create root test dir: " + root);
+    }
+    }
+  }
+
+  private static final Log LOG = LogFactory.getLog(TestSnapshotDescriptionUtils.class);
+
+  @Test
+  public void testValidateDescriptor() {
+    EnvironmentEdge edge = new EnvironmentEdge() {
+      @Override
+      public long currentTimeMillis() {
+        return 0;
+      }
+    };
+    EnvironmentEdgeManagerTestHelper.injectEdge(edge);
+
+    // check a basic snapshot description
+    SnapshotDescription.Builder builder = SnapshotDescription.newBuilder();
+    builder.setName("snapshot");
+    builder.setTable("table");
+
+    // check that time is to an amount in the future
+    Configuration conf = new Configuration(false);
+    conf.setLong(SnapshotDescriptionUtils.TIMESTAMP_SNAPSHOT_SPLIT_POINT_ADDITION, 1);
+    SnapshotDescription desc = SnapshotDescriptionUtils.validate(builder.build(), conf);
+    assertEquals("Description creation time wasn't set correctly", 1, desc.getCreationTime());
+
+    // test a global snapshot
+    edge = new EnvironmentEdge() {
+      @Override
+      public long currentTimeMillis() {
+        return 2;
+      }
+    };
+    EnvironmentEdgeManagerTestHelper.injectEdge(edge);
+    builder.setType(Type.GLOBAL);
+    desc = SnapshotDescriptionUtils.validate(builder.build(), conf);
+    assertEquals("Description creation time wasn't set correctly", 2, desc.getCreationTime());
+
+    // test that we don't override a given value
+    builder.setCreationTime(10);
+    desc = SnapshotDescriptionUtils.validate(builder.build(), conf);
+    assertEquals("Description creation time wasn't set correctly", 10, desc.getCreationTime());
+
+    try {
+      SnapshotDescriptionUtils.validate(SnapshotDescription.newBuilder().setName("fail").build(),
+        conf);
+      fail("Snapshot was considered valid without a table name");
+    } catch (IllegalArgumentException e) {
+      LOG.debug("Correctly failed when snapshot doesn't have a tablename");
+    }
+  }
+
+  /**
+   * Test that we throw an exception if there is no working snapshot directory when we attempt to
+   * 'complete' the snapshot
+   * @throws Exception on failure
+   */
+  @Test
+  public void testCompleteSnapshotWithNoSnapshotDirectoryFailure() throws Exception {
+    Path snapshotDir = new Path(root, ".snapshot");
+    Path tmpDir = new Path(snapshotDir, ".tmp");
+    Path workingDir = new Path(tmpDir, "not_a_snapshot");
+    assertFalse("Already have working snapshot dir: " + workingDir
+        + " but shouldn't. Test file leak?", fs.exists(workingDir));
+    SnapshotDescription snapshot = SnapshotDescription.newBuilder().setName("snapshot").build();
+    try {
+      SnapshotDescriptionUtils.completeSnapshot(snapshot, root, workingDir, fs);
+      fail("Shouldn't successfully complete move of a non-existent directory.");
+    } catch (IOException e) {
+      LOG.info("Correctly failed to move non-existant directory: " + e.getMessage());
+    }
+  }
+}
\ No newline at end of file
-- 
1.7.0.4

