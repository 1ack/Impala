From 6160fba81963441a70b878d5c9fb2aa27e0077c2 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Wed, 3 Jul 2013 15:52:56 +0000
Subject: [PATCH 149/218] CDH-11925:Reinvestigate HIVE-337; Magic number limit of 8 separators for SerDe should be reconsidered today

---
 data/files/nested_complex.txt                      |    2 +
 .../hadoop/hive/hbase/TestLazyHBaseObject.java     |   18 +-
 .../org/apache/hadoop/hive/ql/plan/PlanUtils.java  |    4 +
 .../queries/clientnegative/nested_complex_neg.q    |   15 +
 .../clientpositive/nested_complex_cdh11925.q       |   22 +
 .../clientnegative/nested_complex_neg.q.out        |    9 +
 .../results/clientpositive/bucket_map_join_1.q.out |    1 +
 .../results/clientpositive/bucket_map_join_2.q.out |    1 +
 .../results/clientpositive/bucketcontext_1.q.out   |    2 +
 .../results/clientpositive/bucketcontext_2.q.out   |    2 +
 .../results/clientpositive/bucketcontext_3.q.out   |    2 +
 .../results/clientpositive/bucketcontext_4.q.out   |    2 +
 .../results/clientpositive/bucketcontext_5.q.out   |    2 +
 .../results/clientpositive/bucketcontext_6.q.out   |    2 +
 .../results/clientpositive/bucketcontext_7.q.out   |    2 +
 .../results/clientpositive/bucketcontext_8.q.out   |    2 +
 .../results/clientpositive/bucketmapjoin1.q.out    |    2 +
 .../results/clientpositive/bucketmapjoin10.q.out   |    1 +
 .../results/clientpositive/bucketmapjoin11.q.out   |    2 +
 .../results/clientpositive/bucketmapjoin12.q.out   |    2 +
 .../results/clientpositive/bucketmapjoin13.q.out   |    4 +
 .../results/clientpositive/bucketmapjoin7.q.out    |    1 +
 .../results/clientpositive/bucketmapjoin8.q.out    |    2 +
 .../results/clientpositive/bucketmapjoin9.q.out    |    2 +
 .../clientpositive/bucketmapjoin_negative3.q.out   |    9 +
 .../clientpositive/columnstats_partlvl.q.out       |    2 +
 .../clientpositive/columnstats_tbllvl.q.out        |    1 +
 ql/src/test/results/clientpositive/combine2.q.out  |    1 +
 .../results/clientpositive/combine2_hadoop20.q.out |    1 +
 .../clientpositive/filter_join_breaktask.q.out     |    1 +
 .../results/clientpositive/groupby_sort_1.q.out    |    1 +
 .../clientpositive/groupby_sort_skew_1.q.out       |    1 +
 ql/src/test/results/clientpositive/input23.q.out   |    1 +
 ql/src/test/results/clientpositive/input42.q.out   |    3 +
 .../test/results/clientpositive/input_part7.q.out  |    1 +
 .../test/results/clientpositive/input_part9.q.out  |    1 +
 .../clientpositive/join_filters_overlap.q.out      |    5 +
 .../list_bucket_query_multiskew_1.q.out            |    4 +
 .../list_bucket_query_multiskew_2.q.out            |    3 +
 .../list_bucket_query_multiskew_3.q.out            |  984 ++++++++------------
 .../list_bucket_query_oneskew_1.q.out              |    3 +
 .../list_bucket_query_oneskew_2.q.out              |  162 ++--
 .../list_bucket_query_oneskew_3.q.out              |   81 +-
 .../results/clientpositive/louter_join_ppr.q.out   |    4 +
 .../results/clientpositive/metadataonly1.q.out     |    8 +
 .../clientpositive/nested_complex_cdh11925.q.out   |   63 ++
 .../results/clientpositive/outer_join_ppr.q.out    |    2 +
 ql/src/test/results/clientpositive/pcr.q.out       |   17 +
 .../results/clientpositive/ppd_join_filter.q.out   |    4 +
 .../results/clientpositive/ppd_union_view.q.out    |    4 +
 .../clientpositive/ppr_allchildsarenull.q.out      |    2 +
 .../clientpositive/rand_partitionpruner1.q.out     |    1 +
 .../clientpositive/rand_partitionpruner3.q.out     |    2 +
 .../results/clientpositive/regexp_extract.q.out    |    2 +
 .../results/clientpositive/router_join_ppr.q.out   |    4 +
 ql/src/test/results/clientpositive/sample10.q.out  |    1 +
 ql/src/test/results/clientpositive/sample6.q.out   |    7 +
 ql/src/test/results/clientpositive/sample8.q.out   |    1 +
 ql/src/test/results/clientpositive/sample9.q.out   |    1 +
 .../test/results/clientpositive/smb_mapjoin9.q.out |    2 +
 .../results/clientpositive/smb_mapjoin_13.q.out    |    2 +
 .../clientpositive/sort_merge_join_desc_5.q.out    |    1 +
 .../clientpositive/sort_merge_join_desc_6.q.out    |    1 +
 .../clientpositive/sort_merge_join_desc_7.q.out    |    1 +
 .../results/clientpositive/transform_ppr1.q.out    |    1 +
 .../results/clientpositive/transform_ppr2.q.out    |    1 +
 .../test/results/clientpositive/udf_explode.q.out  |    4 +
 .../results/clientpositive/udf_java_method.q.out   |    1 +
 .../test/results/clientpositive/udf_reflect.q.out  |    1 +
 .../test/results/clientpositive/udtf_explode.q.out |    3 +
 ql/src/test/results/clientpositive/union24.q.out   |    3 +
 ql/src/test/results/clientpositive/union_ppr.q.out |    1 +
 ql/src/test/results/compiler/plan/cast1.q.xml      |    4 +
 ql/src/test/results/compiler/plan/groupby2.q.xml   |    4 +
 ql/src/test/results/compiler/plan/groupby3.q.xml   |    4 +
 ql/src/test/results/compiler/plan/groupby4.q.xml   |    4 +
 ql/src/test/results/compiler/plan/groupby5.q.xml   |    4 +
 ql/src/test/results/compiler/plan/groupby6.q.xml   |    4 +
 ql/src/test/results/compiler/plan/input20.q.xml    |    4 +
 ql/src/test/results/compiler/plan/input8.q.xml     |    4 +
 .../test/results/compiler/plan/input_part1.q.xml   |    4 +
 .../results/compiler/plan/input_testxpath.q.xml    |    4 +
 .../results/compiler/plan/input_testxpath2.q.xml   |    4 +
 ql/src/test/results/compiler/plan/join4.q.xml      |    4 +
 ql/src/test/results/compiler/plan/join5.q.xml      |    4 +
 ql/src/test/results/compiler/plan/join6.q.xml      |    4 +
 ql/src/test/results/compiler/plan/join7.q.xml      |    4 +
 ql/src/test/results/compiler/plan/join8.q.xml      |    4 +
 ql/src/test/results/compiler/plan/sample1.q.xml    |    4 +
 ql/src/test/results/compiler/plan/udf1.q.xml       |    4 +
 ql/src/test/results/compiler/plan/udf4.q.xml       |    4 +
 ql/src/test/results/compiler/plan/udf6.q.xml       |    4 +
 ql/src/test/results/compiler/plan/udf_case.q.xml   |    4 +
 ql/src/test/results/compiler/plan/udf_when.q.xml   |    4 +
 .../hadoop/hive/serde2/lazy/LazyFactory.java       |   23 +-
 .../hadoop/hive/serde2/lazy/LazySimpleSerDe.java   |   83 ++-
 .../apache/hadoop/hive/serde2/lazy/LazyUtils.java  |   24 +
 .../hive/serde2/lazy/TestLazyArrayMapStruct.java   |  295 ++++++
 98 files changed, 1312 insertions(+), 715 deletions(-)
 create mode 100644 data/files/nested_complex.txt
 create mode 100644 ql/src/test/queries/clientnegative/nested_complex_neg.q
 create mode 100644 ql/src/test/queries/clientpositive/nested_complex_cdh11925.q
 create mode 100644 ql/src/test/results/clientnegative/nested_complex_neg.q.out
 create mode 100644 ql/src/test/results/clientpositive/nested_complex_cdh11925.q.out

diff --git a/src/data/files/nested_complex.txt b/src/data/files/nested_complex.txt
new file mode 100644
index 0000000..fee462c
--- /dev/null
+++ b/src/data/files/nested_complex.txt
@@ -0,0 +1,2 @@
+3012k1v1k2v2a102
+2032k1v1k3v3b102
diff --git a/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java b/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java
index 3bd0919..db69ae5 100644
--- a/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java
+++ b/src/hbase-handler/src/test/org/apache/hadoop/hive/hbase/TestLazyHBaseObject.java
@@ -57,8 +57,9 @@ import org.apache.hadoop.io.Writable;
 public class TestLazyHBaseObject extends TestCase {
   /**
    * Test the LazyMap class with Integer-to-String.
+   * @throws SerDeException
    */
-  public void testLazyHBaseCellMap1() {
+  public void testLazyHBaseCellMap1() throws SerDeException {
     // Map of Integer to String
     Text nullSequence = new Text("\\N");
     ObjectInspector oi = LazyFactory.createLazyObjectInspector(
@@ -118,8 +119,9 @@ public class TestLazyHBaseObject extends TestCase {
 
   /**
    * Test the LazyMap class with String-to-String.
+   * @throws SerDeException
    */
-  public void testLazyHBaseCellMap2() {
+  public void testLazyHBaseCellMap2() throws SerDeException {
     // Map of String to String
     Text nullSequence = new Text("\\N");
     ObjectInspector oi = LazyFactory.createLazyObjectInspector(
@@ -180,8 +182,9 @@ public class TestLazyHBaseObject extends TestCase {
   /**
    * Test the LazyHBaseCellMap class for the case where both the key and the value in the family
    * map are stored in binary format using the appropriate LazyPrimitive objects.
+   * @throws SerDeException
    */
-  public void testLazyHBaseCellMap3() {
+  public void testLazyHBaseCellMap3() throws SerDeException {
 
     Text nullSequence = new Text("\\N");
     TypeInfo mapBinaryIntKeyValue = TypeInfoUtils.getTypeInfoFromTypeString("map<int,int>");
@@ -450,8 +453,9 @@ public class TestLazyHBaseObject extends TestCase {
   /**
    * Test the LazyHBaseRow class with one-for-one mappings between
    * Hive fields and HBase columns.
+   * @throws SerDeException
    */
-  public void testLazyHBaseRow1() {
+  public void testLazyHBaseRow1() throws SerDeException {
     List<TypeInfo> fieldTypeInfos =
       TypeInfoUtils.getTypeInfosFromTypeString(
           "string,int,array<string>,map<string,string>,string");
@@ -573,8 +577,9 @@ public class TestLazyHBaseObject extends TestCase {
   /**
    * Test the LazyHBaseRow class with a mapping from a Hive field to
    * an HBase column family.
+   * @throws SerDeException
    */
-  public void testLazyHBaseRow2() {
+  public void testLazyHBaseRow2() throws SerDeException {
     // column family is mapped to Map<string,string>
     List<TypeInfo> fieldTypeInfos =
       TypeInfoUtils.getTypeInfosFromTypeString(
@@ -695,8 +700,9 @@ public class TestLazyHBaseObject extends TestCase {
    * Test the LazyHBaseRow class with a one-to-one/onto mapping between Hive columns and
    * HBase column family/column qualifier pairs. The column types are primitive and fields
    * are stored in binary format in HBase.
+   * @throws SerDeException
    */
-  public void testLazyHBaseRow3() {
+  public void testLazyHBaseRow3() throws SerDeException {
 
     List<TypeInfo> fieldTypeInfos = TypeInfoUtils.getTypeInfosFromTypeString(
         "string,int,tinyint,smallint,bigint,float,double,string,boolean");
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
index aa10c91..1e9ef48 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
@@ -218,7 +218,11 @@ public final class PlanUtils {
       String fileFormat) {
     TableDesc tblDesc = getTableDesc(LazySimpleSerDe.class, "" + Utilities.ctrlaCode, cols, colTypes,
         false, false, fileFormat);
+    //enable escaping
     tblDesc.getProperties().setProperty(serdeConstants.ESCAPE_CHAR, "\\");
+    //enable extended nesting levels
+    tblDesc.getProperties().setProperty(
+        LazySimpleSerDe.SERIALIZATION_EXTEND_NESTING_LEVELS, "true");    
     return tblDesc;
   }
 
diff --git a/src/ql/src/test/queries/clientnegative/nested_complex_neg.q b/src/ql/src/test/queries/clientnegative/nested_complex_neg.q
new file mode 100644
index 0000000..ac6c4ee
--- /dev/null
+++ b/src/ql/src/test/queries/clientnegative/nested_complex_neg.q
@@ -0,0 +1,15 @@
+
+create table nestedcomplex (
+simple_int int,
+max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
+simple_string string)
+
+;
+
+
+-- This should fail in as extended nesting levels are not enabled using the serdeproperty hive.serialization.extend.nesting.levels
+load data local inpath '../data/files/nested_complex.txt' overwrite into table nestedcomplex;
+
+select * from nestedcomplex sort by simple_int;
diff --git a/src/ql/src/test/queries/clientpositive/nested_complex_cdh11925.q b/src/ql/src/test/queries/clientpositive/nested_complex_cdh11925.q
new file mode 100644
index 0000000..b94fbb7
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/nested_complex_cdh11925.q
@@ -0,0 +1,22 @@
+
+create table nestedcomplex (
+simple_int int,
+max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
+simple_string string)
+ROW FORMAT SERDE
+   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
+WITH SERDEPROPERTIES (
+   'hive.serialization.extend.nesting.levels'='true',
+   'line.delim'='\n'
+)
+;
+
+describe nestedcomplex;
+describe extended nestedcomplex;
+
+
+load data local inpath '../data/files/nested_complex.txt' overwrite into table nestedcomplex;
+
+select * from nestedcomplex sort by simple_int;
diff --git a/src/ql/src/test/results/clientnegative/nested_complex_neg.q.out b/src/ql/src/test/results/clientnegative/nested_complex_neg.q.out
new file mode 100644
index 0000000..c046299
--- /dev/null
+++ b/src/ql/src/test/results/clientnegative/nested_complex_neg.q.out
@@ -0,0 +1,9 @@
+PREHOOK: query: create table nestedcomplex (
+simple_int int,
+max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
+simple_string string)
+PREHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
diff --git a/src/ql/src/test/results/clientpositive/bucket_map_join_1.q.out b/src/ql/src/test/results/clientpositive/bucket_map_join_1.q.out
index 739442d..45c9c6e 100644
--- a/src/ql/src/test/results/clientpositive/bucket_map_join_1.q.out
+++ b/src/ql/src/test/results/clientpositive/bucket_map_join_1.q.out
@@ -228,6 +228,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucket_map_join_2.q.out b/src/ql/src/test/results/clientpositive/bucket_map_join_2.q.out
index 2ea7f90..e579167 100644
--- a/src/ql/src/test/results/clientpositive/bucket_map_join_2.q.out
+++ b/src/ql/src/test/results/clientpositive/bucket_map_join_2.q.out
@@ -228,6 +228,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out
index b58aa3f..393f0b0 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_1.q.out
@@ -317,6 +317,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -559,6 +560,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out
index 8eabe0c..cd51adb 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_2.q.out
@@ -305,6 +305,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -547,6 +548,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out
index 5605e33..70acbd4 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_3.q.out
@@ -256,6 +256,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -449,6 +450,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out
index 1707b89..e91ada7 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_4.q.out
@@ -268,6 +268,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -461,6 +462,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_5.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_5.q.out
index 95a8274..6515dfd 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_5.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_5.q.out
@@ -239,6 +239,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -424,6 +425,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out
index 2417bea..6ec9b8e 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_6.q.out
@@ -304,6 +304,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -544,6 +545,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out
index f3d0f08..42c6832 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_7.q.out
@@ -330,6 +330,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -574,6 +575,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out b/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out
index 365b718..5807bf3 100644
--- a/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketcontext_8.q.out
@@ -330,6 +330,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -574,6 +575,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
index fdb1233..34d9e41 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin1.q.out
@@ -117,6 +117,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2
                           columns.types int:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -246,6 +247,7 @@ STAGE PLANS:
                             columns _col0,_col1,_col2
                             columns.types int:string:string
                             escape.delim \
+                            hive.serialization.extend.nesting.levels true
                             serialization.format 1
                       TotalFiles: 1
                       GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out
index 10192ca..f47ab16 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin10.q.out
@@ -345,6 +345,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out
index ab62300..375f7f6 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin11.q.out
@@ -358,6 +358,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -641,6 +642,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out
index 2a2c80a..c4c88b4 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin12.q.out
@@ -278,6 +278,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -506,6 +507,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out
index 5a90ce3..f2c8059 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin13.q.out
@@ -325,6 +325,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -574,6 +575,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -835,6 +837,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -1098,6 +1101,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out
index 5f37d10..60e8568 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin7.q.out
@@ -136,6 +136,7 @@ STAGE PLANS:
                             columns _col0,_col1
                             columns.types int:string
                             escape.delim \
+                            hive.serialization.extend.nesting.levels true
                             serialization.format 1
                       TotalFiles: 1
                       GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out
index 7a11805..4dfc688 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin8.q.out
@@ -250,6 +250,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -493,6 +494,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out
index a30a5b6..da93a41 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin9.q.out
@@ -249,6 +249,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -514,6 +515,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/bucketmapjoin_negative3.q.out b/src/ql/src/test/results/clientpositive/bucketmapjoin_negative3.q.out
index 3a087fa..6acc6b5 100644
--- a/src/ql/src/test/results/clientpositive/bucketmapjoin_negative3.q.out
+++ b/src/ql/src/test/results/clientpositive/bucketmapjoin_negative3.q.out
@@ -203,6 +203,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -362,6 +363,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -516,6 +518,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -668,6 +671,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -820,6 +824,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -972,6 +977,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -1124,6 +1130,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -1276,6 +1283,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -1428,6 +1436,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2,_col3
                           columns.types string:string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out b/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
index 2c32730..9330b25 100644
--- a/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
+++ b/src/ql/src/test/results/clientpositive/columnstats_partlvl.q.out
@@ -202,6 +202,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -408,6 +409,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out b/src/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out
index 007bc31..ab411fc 100644
--- a/src/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out
+++ b/src/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out
@@ -240,6 +240,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2
                     columns.types struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>:struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>:struct<columntype:string,min:double,max:double,countnulls:bigint,numdistinctvalues:bigint>
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/combine2.q.out b/src/ql/src/test/results/clientpositive/combine2.q.out
index e13da6b..6cd74bc 100644
--- a/src/ql/src/test/results/clientpositive/combine2.q.out
+++ b/src/ql/src/test/results/clientpositive/combine2.q.out
@@ -629,6 +629,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out b/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out
index 5152ade..5b11edd 100644
--- a/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out
+++ b/src/ql/src/test/results/clientpositive/combine2_hadoop20.q.out
@@ -621,6 +621,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out b/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
index 3a75bcf..475d685 100644
--- a/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
+++ b/src/ql/src/test/results/clientpositive/filter_join_breaktask.q.out
@@ -290,6 +290,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types int:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out b/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out
index 2f997e3..153e1a6 100644
--- a/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_sort_1.q.out
@@ -3617,6 +3617,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3,_col4
                     columns.types string:bigint:string:string:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/groupby_sort_skew_1.q.out b/src/ql/src/test/results/clientpositive/groupby_sort_skew_1.q.out
index b4e798c..807d18a 100644
--- a/src/ql/src/test/results/clientpositive/groupby_sort_skew_1.q.out
+++ b/src/ql/src/test/results/clientpositive/groupby_sort_skew_1.q.out
@@ -4027,6 +4027,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3,_col4
                     columns.types string:bigint:string:string:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/input23.q.out b/src/ql/src/test/results/clientpositive/input23.q.out
index f71a43f..76b9a07 100644
--- a/src/ql/src/test/results/clientpositive/input23.q.out
+++ b/src/ql/src/test/results/clientpositive/input23.q.out
@@ -144,6 +144,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7
                       columns.types string:string:string:string:string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/input42.q.out b/src/ql/src/test/results/clientpositive/input42.q.out
index 67679af..2ccc32c 100644
--- a/src/ql/src/test/results/clientpositive/input42.q.out
+++ b/src/ql/src/test/results/clientpositive/input42.q.out
@@ -158,6 +158,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1348,6 +1349,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1916,6 +1918,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/input_part7.q.out b/src/ql/src/test/results/clientpositive/input_part7.q.out
index 538a742..300173d 100644
--- a/src/ql/src/test/results/clientpositive/input_part7.q.out
+++ b/src/ql/src/test/results/clientpositive/input_part7.q.out
@@ -242,6 +242,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/input_part9.q.out b/src/ql/src/test/results/clientpositive/input_part9.q.out
index 91d1794..e343c9c 100644
--- a/src/ql/src/test/results/clientpositive/input_part9.q.out
+++ b/src/ql/src/test/results/clientpositive/input_part9.q.out
@@ -163,6 +163,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/join_filters_overlap.q.out b/src/ql/src/test/results/clientpositive/join_filters_overlap.q.out
index e9481b6..48fcfc6 100644
--- a/src/ql/src/test/results/clientpositive/join_filters_overlap.q.out
+++ b/src/ql/src/test/results/clientpositive/join_filters_overlap.q.out
@@ -181,6 +181,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3,_col4,_col5
                     columns.types int:int:int:int:int:int
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -387,6 +388,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3,_col4,_col5
                     columns.types int:int:int:int:int:int
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -593,6 +595,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3,_col4,_col5
                     columns.types int:int:int:int:int:int
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -825,6 +828,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7
                     columns.types int:int:int:int:int:int:int:int
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -1062,6 +1066,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7
                     columns.types int:int:int:int:int:int:int:int
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out b/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out
index f86babf..ef0606b 100644
--- a/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out
+++ b/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out
@@ -262,6 +262,7 @@ STAGE PLANS:
                         columns _col0
                         columns.types int
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -396,6 +397,7 @@ STAGE PLANS:
                         columns _col0,_col1
                         columns.types int:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -529,6 +531,7 @@ STAGE PLANS:
                         columns _col0
                         columns.types int
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -662,6 +665,7 @@ STAGE PLANS:
                         columns _col0,_col1
                         columns.types int:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out b/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out
index 5938e48..4773185 100644
--- a/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out
+++ b/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out
@@ -264,6 +264,7 @@ STAGE PLANS:
                         columns _col0,_col1
                         columns.types int:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -444,6 +445,7 @@ STAGE PLANS:
                         columns _col0
                         columns.types int
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -581,6 +583,7 @@ STAGE PLANS:
                         columns _col0,_col1
                         columns.types int:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out b/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out
index 2d1bccf..433bc5d 100644
--- a/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out
+++ b/src/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out
@@ -12,11 +12,9 @@ PREHOOK: query: -- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
 -- 1. pruner only pick up right directory
 -- 2. query result is right
 
--- create 2 tables: fact_daily and fact_daily
--- fact_daily will be used for list bucketing query
--- fact_daily is a table used to prepare data and test directories		
-CREATE TABLE fact_daily(x int, y STRING, z STRING) PARTITIONED BY (ds STRING, hr STRING)	
-#### A masked pattern was here ####
+-- create a skewed table
+create table fact_daily (key String, value String) 
+partitioned by (ds String, hr String)
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: -- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)	
 
@@ -32,239 +30,104 @@ POSTHOOK: query: -- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
 -- 1. pruner only pick up right directory
 -- 2. query result is right
 
--- create 2 tables: fact_daily and fact_daily
--- fact_daily will be used for list bucketing query
--- fact_daily is a table used to prepare data and test directories		
-CREATE TABLE fact_daily(x int, y STRING, z STRING) PARTITIONED BY (ds STRING, hr STRING)	
-#### A masked pattern was here ####
+-- create a skewed table
+create table fact_daily (key String, value String) 
+partitioned by (ds String, hr String)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@fact_daily
-PREHOOK: query: -- create /fact_daily/ds=1/hr=1 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='1')	
-SELECT key, value, value FROM src WHERE key=484
+PREHOOK: query: -- partition no skew
+insert overwrite table fact_daily partition (ds = '1', hr = '1')
+select key, value from src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 PREHOOK: Output: default@fact_daily@ds=1/hr=1
-POSTHOOK: query: -- create /fact_daily/ds=1/hr=1 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='1')	
-SELECT key, value, value FROM src WHERE key=484
+POSTHOOK: query: -- partition no skew
+insert overwrite table fact_daily partition (ds = '1', hr = '1')
+select key, value from src
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@fact_daily@ds=1/hr=1
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: -- create /fact_daily/ds=1/hr=2 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='2')	
-SELECT key+11, value, value FROM src WHERE key=484
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@fact_daily@ds=1/hr=2
-POSTHOOK: query: -- create /fact_daily/ds=1/hr=2 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='2')	
-SELECT key+11, value, value FROM src WHERE key=484
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@fact_daily@ds=1/hr=2
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: -- create /fact_daily/ds=1/hr=3 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='3')	
-SELECT key, value, value FROM src WHERE key=238
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@fact_daily@ds=1/hr=3
-POSTHOOK: query: -- create /fact_daily/ds=1/hr=3 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='3')	
-SELECT key, value, value FROM src WHERE key=238
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@fact_daily@ds=1/hr=3
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: -- create /fact_daily/ds=1/hr=4 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='4')	
-SELECT key, value, value FROM src WHERE key=98
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@fact_daily@ds=1/hr=4
-POSTHOOK: query: -- create /fact_daily/ds=1/hr=4 directory	
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='1', hr='4')	
-SELECT key, value, value FROM src WHERE key=98
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@fact_daily@ds=1/hr=4
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-#### A masked pattern was here ####
-PREHOOK: query: -- create a non-skewed partition ds=200 and hr =1 in fact_daily table
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='200', hr='1') SELECT key, value, value FROM src WHERE key=145 or key=406 or key=429
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@fact_daily@ds=200/hr=1
-POSTHOOK: query: -- create a non-skewed partition ds=200 and hr =1 in fact_daily table
-INSERT OVERWRITE TABLE fact_daily PARTITION (ds='200', hr='1') SELECT key, value, value FROM src WHERE key=145 or key=406 or key=429
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@fact_daily@ds=200/hr=1
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: -- switch fact_daily to skewed table, create partition ds=1 and hr=5 and point its location to /fact_daily/ds=1
-alter table fact_daily skewed by (x,y) on ((484,'val_484'),(238,'val_238'))
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='1')
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='1')
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+# col_name            	data_type           	comment             
+	 	 
+key                 	string              	None                
+value               	string              	None                
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ds                  	string              	None                
+hr                  	string              	None                
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[1, 1]              	 
+Database:           	default             	 
+Table:              	fact_daily          	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	numFiles            	1                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: -- partition. skewed value is 484/238
+alter table fact_daily skewed by (key, value) on (('484','val_484'),('238','val_238')) stored as DIRECTORIES
 PREHOOK: type: ALTERTABLE_SKEWED
 PREHOOK: Input: default@fact_daily
 PREHOOK: Output: default@fact_daily
-POSTHOOK: query: -- switch fact_daily to skewed table, create partition ds=1 and hr=5 and point its location to /fact_daily/ds=1
-alter table fact_daily skewed by (x,y) on ((484,'val_484'),(238,'val_238'))
+POSTHOOK: query: -- partition. skewed value is 484/238
+alter table fact_daily skewed by (key, value) on (('484','val_484'),('238','val_238')) stored as DIRECTORIES
 POSTHOOK: type: ALTERTABLE_SKEWED
 POSTHOOK: Input: default@fact_daily
 POSTHOOK: Output: default@fact_daily
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: ALTER TABLE fact_daily SET TBLPROPERTIES('EXTERNAL'='TRUE')
-PREHOOK: type: ALTERTABLE_PROPERTIES
-PREHOOK: Input: default@fact_daily
-PREHOOK: Output: default@fact_daily
-POSTHOOK: query: ALTER TABLE fact_daily SET TBLPROPERTIES('EXTERNAL'='TRUE')
-POSTHOOK: type: ALTERTABLE_PROPERTIES
-POSTHOOK: Input: default@fact_daily
-POSTHOOK: Output: default@fact_daily
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: ALTER TABLE fact_daily ADD PARTITION (ds='1', hr='5')	
-#### A masked pattern was here ####
-PREHOOK: type: ALTERTABLE_ADDPARTS
-PREHOOK: Input: default@fact_daily
-POSTHOOK: query: ALTER TABLE fact_daily ADD PARTITION (ds='1', hr='5')	
-#### A masked pattern was here ####
-POSTHOOK: type: ALTERTABLE_ADDPARTS
-POSTHOOK: Input: default@fact_daily
-POSTHOOK: Output: default@fact_daily@ds=1/hr=5
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: -- set List Bucketing location map
-#### A masked pattern was here ####
-PREHOOK: type: ALTERTBLPART_SKEWED_LOCATION
-PREHOOK: Input: default@fact_daily
-PREHOOK: Output: default@fact_daily@ds=1/hr=5
-POSTHOOK: query: -- set List Bucketing location map
-#### A masked pattern was here ####
-POSTHOOK: type: ALTERTBLPART_SKEWED_LOCATION
-POSTHOOK: Input: default@fact_daily
-POSTHOOK: Input: default@fact_daily@ds=1/hr=5
-POSTHOOK: Output: default@fact_daily@ds=1/hr=5
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='5')
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table fact_daily partition (ds = '1', hr = '2')
+select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@fact_daily@ds=1/hr=2
+POSTHOOK: query: insert overwrite table fact_daily partition (ds = '1', hr = '2')
+select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@fact_daily@ds=1/hr=2
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='2')
 PREHOOK: type: DESCTABLE
-POSTHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='5')
+POSTHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='2')
 POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 # col_name            	data_type           	comment             
 	 	 
-x                   	int                 	None                
-y                   	string              	None                
-z                   	string              	None                
+key                 	string              	None                
+value               	string              	None                
 	 	 
 # Partition Information	 	 
 # col_name            	data_type           	comment             
@@ -273,13 +136,17 @@ ds                  	string              	None
 hr                  	string              	None                
 	 	 
 # Detailed Partition Information	 	 
-Partition Value:    	[1, 5]              	 
+Partition Value:    	[1, 2]              	 
 Database:           	default             	 
 Table:              	fact_daily          	 
 #### A masked pattern was here ####
 Protect Mode:       	None                	 
 #### A masked pattern was here ####
 Partition Parameters:	 	 
+	numFiles            	3                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -290,108 +157,57 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
-Skewed Columns:     	[x, y]              	 
+Stored As SubDirectories:	Yes                 	 
+Skewed Columns:     	[key, value]        	 
 Skewed Values:      	[[484, val_484], [238, val_238]]	 
 #### A masked pattern was here ####
+Skewed Value to Truncated Path:	{[484, val_484]=/fact_daily/ds=1/hr=2/key=484/value=val_484, [238, val_238]=/fact_daily/ds=1/hr=2/key=238/value=val_238}	 
 Storage Desc Params:	 	 
 	serialization.format	1                   
-PREHOOK: query: -- alter skewed information and create partition ds=100 and hr=1
-alter table fact_daily skewed by (x,y) on ((495,'val_484'))
+PREHOOK: query: -- another partition. skewed value is 327
+alter table fact_daily skewed by (key, value) on (('327','val_327')) stored as DIRECTORIES
 PREHOOK: type: ALTERTABLE_SKEWED
 PREHOOK: Input: default@fact_daily
 PREHOOK: Output: default@fact_daily
-POSTHOOK: query: -- alter skewed information and create partition ds=100 and hr=1
-alter table fact_daily skewed by (x,y) on ((495,'val_484'))
+POSTHOOK: query: -- another partition. skewed value is 327
+alter table fact_daily skewed by (key, value) on (('327','val_327')) stored as DIRECTORIES
 POSTHOOK: type: ALTERTABLE_SKEWED
 POSTHOOK: Input: default@fact_daily
 POSTHOOK: Output: default@fact_daily
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: ALTER TABLE fact_daily ADD PARTITION (ds='100', hr='1')	
-#### A masked pattern was here ####
-PREHOOK: type: ALTERTABLE_ADDPARTS
-PREHOOK: Input: default@fact_daily
-POSTHOOK: query: ALTER TABLE fact_daily ADD PARTITION (ds='100', hr='1')	
-#### A masked pattern was here ####
-POSTHOOK: type: ALTERTABLE_ADDPARTS
-POSTHOOK: Input: default@fact_daily
-POSTHOOK: Output: default@fact_daily@ds=100/hr=1
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-#### A masked pattern was here ####
-PREHOOK: type: ALTERTBLPART_SKEWED_LOCATION
-PREHOOK: Input: default@fact_daily
-PREHOOK: Output: default@fact_daily@ds=100/hr=1
-#### A masked pattern was here ####
-POSTHOOK: type: ALTERTBLPART_SKEWED_LOCATION
-POSTHOOK: Input: default@fact_daily
-POSTHOOK: Input: default@fact_daily@ds=100/hr=1
-POSTHOOK: Output: default@fact_daily@ds=100/hr=1
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-PREHOOK: query: describe formatted fact_daily PARTITION (ds = '100', hr='1')
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table fact_daily partition (ds = '1', hr = '3')
+select key, value from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@fact_daily@ds=1/hr=3
+POSTHOOK: query: insert overwrite table fact_daily partition (ds = '1', hr = '3')
+select key, value from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@fact_daily@ds=1/hr=3
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='3')
 PREHOOK: type: DESCTABLE
-POSTHOOK: query: describe formatted fact_daily PARTITION (ds = '100', hr='1')
+POSTHOOK: query: describe formatted fact_daily PARTITION (ds = '1', hr='3')
 POSTHOOK: type: DESCTABLE
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 # col_name            	data_type           	comment             
 	 	 
-x                   	int                 	None                
-y                   	string              	None                
-z                   	string              	None                
+key                 	string              	None                
+value               	string              	None                
 	 	 
 # Partition Information	 	 
 # col_name            	data_type           	comment             
@@ -400,13 +216,17 @@ ds                  	string              	None
 hr                  	string              	None                
 	 	 
 # Detailed Partition Information	 	 
-Partition Value:    	[100, 1]            	 
+Partition Value:    	[1, 3]              	 
 Database:           	default             	 
 Table:              	fact_daily          	 
 #### A masked pattern was here ####
 Protect Mode:       	None                	 
 #### A masked pattern was here ####
 Partition Parameters:	 	 
+	numFiles            	2                   
+	numRows             	500                 
+	rawDataSize         	5312                
+	totalSize           	5812                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -417,36 +237,29 @@ Compressed:         	No
 Num Buckets:        	-1                  	 
 Bucket Columns:     	[]                  	 
 Sort Columns:       	[]                  	 
-Skewed Columns:     	[x, y]              	 
-Skewed Values:      	[[495, val_484]]    	 
+Stored As SubDirectories:	Yes                 	 
+Skewed Columns:     	[key, value]        	 
+Skewed Values:      	[[327, val_327]]    	 
 #### A masked pattern was here ####
+Skewed Value to Truncated Path:	{[327, val_327]=/fact_daily/ds=1/hr=3/key=327/value=val_327}	 
 Storage Desc Params:	 	 
 	serialization.format	1                   
 PREHOOK: query: -- query non-skewed partition
 explain extended
-select * from fact_daily where ds='200' and  hr='1' and x=145
+select * from fact_daily where ds = '1' and  hr='1' and key='145'
 PREHOOK: type: QUERY
 POSTHOOK: query: -- query non-skewed partition
 explain extended
-select * from fact_daily where ds='200' and  hr='1' and x=145
+select * from fact_daily where ds = '1' and  hr='1' and key='145'
 POSTHOOK: type: QUERY
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) '200') (= (TOK_TABLE_OR_COL hr) '1')) (= (TOK_TABLE_OR_COL x) 145)))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) '1') (= (TOK_TABLE_OR_COL hr) '1')) (= (TOK_TABLE_OR_COL key) '145')))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -463,21 +276,19 @@ STAGE PLANS:
             Filter Operator
               isSamplingPred: false
               predicate:
-                  expr: (x = 145)
+                  expr: (key = '145')
                   type: boolean
               Select Operator
                 expressions:
-                      expr: x
-                      type: int
-                      expr: y
+                      expr: key
                       type: string
-                      expr: z
+                      expr: value
                       type: string
                       expr: ds
                       type: string
                       expr: hr
                       type: string
-                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                outputColumnNames: _col0, _col1, _col2, _col3
                 File Output Operator
                   compressed: false
                   GlobalTableId: 0
@@ -488,9 +299,10 @@ STAGE PLANS:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       properties:
-                        columns _col0,_col1,_col2,_col3,_col4
-                        columns.types int:string:string:string:string
+                        columns _col0,_col1,_col2,_col3
+                        columns.types string:string:string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -505,187 +317,232 @@ STAGE PLANS:
             input format: org.apache.hadoop.mapred.TextInputFormat
             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
             partition values:
-              ds 200
+              ds 1
               hr 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
-              columns x,y,z
-              columns.types int:string:string
+              columns key,value
+              columns.types string:string
 #### A masked pattern was here ####
               name default.fact_daily
               numFiles 1
-              numPartitions 5
-              numRows 7
+              numRows 500
               partition_columns ds/hr
-              rawDataSize 133
-              serialization.ddl struct fact_daily { i32 x, string y, string z}
+              rawDataSize 5312
+              serialization.ddl struct fact_daily { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 140
+              totalSize 5812
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
               properties:
-                EXTERNAL TRUE
                 bucket_count -1
-                columns x,y,z
-                columns.types int:string:string
+                columns key,value
+                columns.types string:string
 #### A masked pattern was here ####
                 name default.fact_daily
-                numFiles 5
-                numPartitions 5
-                numRows 13
+                numFiles 6
+                numPartitions 3
+                numRows 1500
                 partition_columns ds/hr
-                rawDataSize 241
-                serialization.ddl struct fact_daily { i32 x, string y, string z}
+                rawDataSize 15936
+                serialization.ddl struct fact_daily { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 254
+                totalSize 17436
 #### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.fact_daily
             name: default.fact_daily
       Truncated Path -> Alias:
-        /fact_daily/ds=200/hr=1 [fact_daily]
+        /fact_daily/ds=1/hr=1 [fact_daily]
 
   Stage: Stage-0
     Fetch Operator
       limit: -1
 
 
-PREHOOK: query: select * from fact_daily where ds='200' and  hr='1' and x=145
+PREHOOK: query: select * from fact_daily where ds = '1' and  hr='1' and key='145'
 PREHOOK: type: QUERY
-PREHOOK: Input: default@fact_daily@ds=200/hr=1
+PREHOOK: Input: default@fact_daily
+PREHOOK: Input: default@fact_daily@ds=1/hr=1
 #### A masked pattern was here ####
-POSTHOOK: query: select * from fact_daily where ds='200' and  hr='1' and x=145
+POSTHOOK: query: select * from fact_daily where ds = '1' and  hr='1' and key='145'
 POSTHOOK: type: QUERY
-POSTHOOK: Input: default@fact_daily@ds=200/hr=1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-145	val_145	val_145	200	1
+POSTHOOK: Input: default@fact_daily
+POSTHOOK: Input: default@fact_daily@ds=1/hr=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+145	val_145	1	1
 PREHOOK: query: explain extended
-select * from fact_daily where ds='200' and  hr='1'
+select count(*) from fact_daily where ds = '1' and  hr='1'
 PREHOOK: type: QUERY
 POSTHOOK: query: explain extended
-select * from fact_daily where ds='200' and  hr='1'
+select count(*) from fact_daily where ds = '1' and  hr='1'
 POSTHOOK: type: QUERY
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '200') (= (TOK_TABLE_OR_COL hr) '1')))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '1') (= (TOK_TABLE_OR_COL hr) '1')))))
 
 STAGE DEPENDENCIES:
+  Stage-1 is a root stage
   Stage-0 is a root stage
 
 STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        fact_daily 
+          TableScan
+            alias: fact_daily
+            GatherStats: false
+            Select Operator
+              Group By Operator
+                aggregations:
+                      expr: count()
+                bucketGroup: false
+                mode: hash
+                outputColumnNames: _col0
+                Reduce Output Operator
+                  sort order: 
+                  tag: -1
+                  value expressions:
+                        expr: _col0
+                        type: bigint
+      Needs Tagging: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: hr=1
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds 1
+              hr 1
+            properties:
+              bucket_count -1
+              columns key,value
+              columns.types string:string
+#### A masked pattern was here ####
+              name default.fact_daily
+              numFiles 1
+              numRows 500
+              partition_columns ds/hr
+              rawDataSize 5312
+              serialization.ddl struct fact_daily { string key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 5812
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count -1
+                columns key,value
+                columns.types string:string
+#### A masked pattern was here ####
+                name default.fact_daily
+                numFiles 6
+                numPartitions 3
+                numRows 1500
+                partition_columns ds/hr
+                rawDataSize 15936
+                serialization.ddl struct fact_daily { string key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                totalSize 17436
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.fact_daily
+            name: default.fact_daily
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          mode: mergepartial
+          outputColumnNames: _col0
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: bigint
+            outputColumnNames: _col0
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+#### A masked pattern was here ####
+              NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  properties:
+                    columns _col0
+                    columns.types bigint
+                    escape.delim \
+                    hive.serialization.extend.nesting.levels true
+                    serialization.format 1
+              TotalFiles: 1
+              GatherStats: false
+              MultiFileSpray: false
+      Truncated Path -> Alias:
+        /fact_daily/ds=1/hr=1 [fact_daily]
+
   Stage: Stage-0
     Fetch Operator
       limit: -1
-      Processor Tree:
-        TableScan
-          alias: fact_daily
-          GatherStats: false
-          Select Operator
-            expressions:
-                  expr: x
-                  type: int
-                  expr: y
-                  type: string
-                  expr: z
-                  type: string
-                  expr: ds
-                  type: string
-                  expr: hr
-                  type: string
-            outputColumnNames: _col0, _col1, _col2, _col3, _col4
-            ListSink
 
 
-PREHOOK: query: select * from fact_daily where ds='200' and  hr='1'
+PREHOOK: query: select count(*) from fact_daily where ds = '1' and  hr='1'
 PREHOOK: type: QUERY
-PREHOOK: Input: default@fact_daily@ds=200/hr=1
+PREHOOK: Input: default@fact_daily
+PREHOOK: Input: default@fact_daily@ds=1/hr=1
 #### A masked pattern was here ####
-POSTHOOK: query: select * from fact_daily where ds='200' and  hr='1'
+POSTHOOK: query: select count(*) from fact_daily where ds = '1' and  hr='1'
 POSTHOOK: type: QUERY
-POSTHOOK: Input: default@fact_daily@ds=200/hr=1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-406	val_406	val_406	200	1
-429	val_429	val_429	200	1
-145	val_145	val_145	200	1
-406	val_406	val_406	200	1
-406	val_406	val_406	200	1
-429	val_429	val_429	200	1
-406	val_406	val_406	200	1
+POSTHOOK: Input: default@fact_daily
+POSTHOOK: Input: default@fact_daily@ds=1/hr=1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+500
 PREHOOK: query: -- query skewed partition
 explain extended
-SELECT * FROM fact_daily WHERE ds='1' and hr='5' and (x=484 and y ='val_484')
+SELECT * FROM fact_daily WHERE ds='1' and hr='2' and (key='484' and value='val_484')
 PREHOOK: type: QUERY
 POSTHOOK: query: -- query skewed partition
 explain extended
-SELECT * FROM fact_daily WHERE ds='1' and hr='5' and (x=484 and y ='val_484')
+SELECT * FROM fact_daily WHERE ds='1' and hr='2' and (key='484' and value='val_484')
 POSTHOOK: type: QUERY
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) '1') (= (TOK_TABLE_OR_COL hr) '5')) (and (= (TOK_TABLE_OR_COL x) 484) (= (TOK_TABLE_OR_COL y) 'val_484'))))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) '1') (= (TOK_TABLE_OR_COL hr) '2')) (and (= (TOK_TABLE_OR_COL key) '484') (= (TOK_TABLE_OR_COL value) 'val_484'))))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -702,21 +559,19 @@ STAGE PLANS:
             Filter Operator
               isSamplingPred: false
               predicate:
-                  expr: ((x = 484) and (y = 'val_484'))
+                  expr: ((key = '484') and (value = 'val_484'))
                   type: boolean
               Select Operator
                 expressions:
-                      expr: x
-                      type: int
-                      expr: y
+                      expr: key
                       type: string
-                      expr: z
+                      expr: value
                       type: string
                       expr: ds
                       type: string
                       expr: hr
                       type: string
-                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                outputColumnNames: _col0, _col1, _col2, _col3
                 File Output Operator
                   compressed: false
                   GlobalTableId: 0
@@ -727,9 +582,10 @@ STAGE PLANS:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       properties:
-                        columns _col0,_col1,_col2,_col3,_col4
-                        columns.types int:string:string:string:string
+                        columns _col0,_col1,_col2,_col3
+                        columns.types string:string:string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -740,110 +596,91 @@ STAGE PLANS:
       Path -> Partition:
 #### A masked pattern was here ####
           Partition
-            base file name: y=val_484
+            base file name: value=val_484
             input format: org.apache.hadoop.mapred.TextInputFormat
             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
             partition values:
               ds 1
-              hr 5
+              hr 2
             properties:
-              EXTERNAL TRUE
               bucket_count -1
-              columns x,y,z
-              columns.types int:string:string
+              columns key,value
+              columns.types string:string
 #### A masked pattern was here ####
               name default.fact_daily
-              numFiles 5
-              numPartitions 5
-              numRows 13
+              numFiles 3
+              numRows 500
               partition_columns ds/hr
-              rawDataSize 241
-              serialization.ddl struct fact_daily { i32 x, string y, string z}
+              rawDataSize 5312
+              serialization.ddl struct fact_daily { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 254
+              totalSize 5812
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
               properties:
-                EXTERNAL TRUE
                 bucket_count -1
-                columns x,y,z
-                columns.types int:string:string
+                columns key,value
+                columns.types string:string
 #### A masked pattern was here ####
                 name default.fact_daily
-                numFiles 5
-                numPartitions 5
-                numRows 13
+                numFiles 6
+                numPartitions 3
+                numRows 1500
                 partition_columns ds/hr
-                rawDataSize 241
-                serialization.ddl struct fact_daily { i32 x, string y, string z}
+                rawDataSize 15936
+                serialization.ddl struct fact_daily { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 254
+                totalSize 17436
 #### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.fact_daily
             name: default.fact_daily
       Truncated Path -> Alias:
-        /fact_daily/ds=1/hr=5/x=484/y=val_484 [fact_daily]
+        /fact_daily/ds=1/hr=2/key=484/value=val_484 [fact_daily]
 
   Stage: Stage-0
     Fetch Operator
       limit: -1
 
 
-PREHOOK: query: SELECT * FROM fact_daily WHERE ds='1' and hr='5' and (x=484 and y ='val_484')
+PREHOOK: query: SELECT * FROM fact_daily WHERE ds='1' and hr='2' and (key='484' and value='val_484')
 PREHOOK: type: QUERY
-PREHOOK: Input: default@fact_daily@ds=1/hr=5
+PREHOOK: Input: default@fact_daily
+PREHOOK: Input: default@fact_daily@ds=1/hr=2
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='1' and hr='5' and (x=484 and y ='val_484')
+POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='1' and hr='2' and (key='484' and value='val_484')
 POSTHOOK: type: QUERY
-POSTHOOK: Input: default@fact_daily@ds=1/hr=5
-#### A masked pattern was here ####
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-484	val_484	val_484	1	5
+POSTHOOK: Input: default@fact_daily
+POSTHOOK: Input: default@fact_daily@ds=1/hr=2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+484	val_484	1	2
 PREHOOK: query: -- query another skewed partition
 explain extended
-SELECT * FROM fact_daily WHERE ds='100' and hr='1' and (x=495 and y ='val_484')
+SELECT * FROM fact_daily WHERE ds='1' and hr='3' and (key='327' and value='val_327')
 PREHOOK: type: QUERY
 POSTHOOK: query: -- query another skewed partition
 explain extended
-SELECT * FROM fact_daily WHERE ds='100' and hr='1' and (x=495 and y ='val_484')
+SELECT * FROM fact_daily WHERE ds='1' and hr='3' and (key='327' and value='val_327')
 POSTHOOK: type: QUERY
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) '100') (= (TOK_TABLE_OR_COL hr) '1')) (and (= (TOK_TABLE_OR_COL x) 495) (= (TOK_TABLE_OR_COL y) 'val_484'))))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (and (and (= (TOK_TABLE_OR_COL ds) '1') (= (TOK_TABLE_OR_COL hr) '3')) (and (= (TOK_TABLE_OR_COL key) '327') (= (TOK_TABLE_OR_COL value) 'val_327'))))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -860,21 +697,19 @@ STAGE PLANS:
             Filter Operator
               isSamplingPred: false
               predicate:
-                  expr: ((x = 495) and (y = 'val_484'))
+                  expr: ((key = '327') and (value = 'val_327'))
                   type: boolean
               Select Operator
                 expressions:
-                      expr: x
-                      type: int
-                      expr: y
+                      expr: key
                       type: string
-                      expr: z
+                      expr: value
                       type: string
                       expr: ds
                       type: string
                       expr: hr
                       type: string
-                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                outputColumnNames: _col0, _col1, _col2, _col3
                 File Output Operator
                   compressed: false
                   GlobalTableId: 0
@@ -885,9 +720,10 @@ STAGE PLANS:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       properties:
-                        columns _col0,_col1,_col2,_col3,_col4
-                        columns.types int:string:string:string:string
+                        columns _col0,_col1,_col2,_col3
+                        columns.types string:string:string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -898,82 +734,74 @@ STAGE PLANS:
       Path -> Partition:
 #### A masked pattern was here ####
           Partition
-            base file name: y=val_484
+            base file name: value=val_327
             input format: org.apache.hadoop.mapred.TextInputFormat
             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
             partition values:
-              ds 100
-              hr 1
+              ds 1
+              hr 3
             properties:
-              EXTERNAL TRUE
               bucket_count -1
-              columns x,y,z
-              columns.types int:string:string
+              columns key,value
+              columns.types string:string
 #### A masked pattern was here ####
               name default.fact_daily
-              numFiles 5
-              numPartitions 5
-              numRows 13
+              numFiles 2
+              numRows 500
               partition_columns ds/hr
-              rawDataSize 241
-              serialization.ddl struct fact_daily { i32 x, string y, string z}
+              rawDataSize 5312
+              serialization.ddl struct fact_daily { string key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 254
+              totalSize 5812
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
           
               input format: org.apache.hadoop.mapred.TextInputFormat
               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
               properties:
-                EXTERNAL TRUE
                 bucket_count -1
-                columns x,y,z
-                columns.types int:string:string
+                columns key,value
+                columns.types string:string
 #### A masked pattern was here ####
                 name default.fact_daily
-                numFiles 5
-                numPartitions 5
-                numRows 13
+                numFiles 6
+                numPartitions 3
+                numRows 1500
                 partition_columns ds/hr
-                rawDataSize 241
-                serialization.ddl struct fact_daily { i32 x, string y, string z}
+                rawDataSize 15936
+                serialization.ddl struct fact_daily { string key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                totalSize 254
+                totalSize 17436
 #### A masked pattern was here ####
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.fact_daily
             name: default.fact_daily
       Truncated Path -> Alias:
-        /fact_daily/ds=1/hr=5/x=495/y=val_484 [fact_daily]
+        /fact_daily/ds=1/hr=3/key=327/value=val_327 [fact_daily]
 
   Stage: Stage-0
     Fetch Operator
       limit: -1
 
 
-PREHOOK: query: SELECT * FROM fact_daily WHERE ds='100' and hr='1' and (x=495 and y ='val_484')
+PREHOOK: query: SELECT * FROM fact_daily WHERE ds='1' and hr='3' and (key='327' and value='val_327')
 PREHOOK: type: QUERY
-PREHOOK: Input: default@fact_daily@ds=100/hr=1
+PREHOOK: Input: default@fact_daily
+PREHOOK: Input: default@fact_daily@ds=1/hr=3
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='100' and hr='1' and (x=495 and y ='val_484')
+POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='1' and hr='3' and (key='327' and value='val_327')
 POSTHOOK: type: QUERY
-POSTHOOK: Input: default@fact_daily@ds=100/hr=1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=4).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: fact_daily PARTITION(ds=200,hr=1).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-495	val_484	val_484	100	1
+POSTHOOK: Input: default@fact_daily
+POSTHOOK: Input: default@fact_daily@ds=1/hr=3
+#### A masked pattern was here ####
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: fact_daily PARTITION(ds=1,hr=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+327	val_327	1	3
+327	val_327	1	3
+327	val_327	1	3
diff --git a/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_1.q.out b/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_1.q.out
index 5ff59a5..a3a1f5f 100644
--- a/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_1.q.out
+++ b/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_1.q.out
@@ -210,6 +210,7 @@ STAGE PLANS:
                         columns _col0
                         columns.types int
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -325,6 +326,7 @@ STAGE PLANS:
                         columns _col0
                         columns.types int
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -436,6 +438,7 @@ STAGE PLANS:
                         columns _col0
                         columns.types int
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_2.q.out b/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_2.q.out
index 5379c3c..8d3ae97 100644
--- a/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_2.q.out
+++ b/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_2.q.out
@@ -166,20 +166,22 @@ Skewed Values:      	[[484]]
 #### A masked pattern was here ####
 Storage Desc Params:	 	 
 	serialization.format	1                   
-PREHOOK: query: SELECT * FROM fact_daily WHERE ds='1'
+PREHOOK: query: SELECT * FROM fact_daily WHERE ds='1' ORDER BY x, y
 PREHOOK: type: QUERY
+PREHOOK: Input: default@fact_daily
 PREHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='1'
+POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='1' ORDER BY x, y
 POSTHOOK: type: QUERY
+POSTHOOK: Input: default@fact_daily
 POSTHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-495	val_484	1
 484	val_484	1
+495	val_484	1
 PREHOOK: query: -- The first subquery
 -- explain plan shows which directory selected: Truncated Path -> Alias
 explain extended select x from (select x from fact_daily where ds = '1') subq where x = 484
@@ -217,28 +219,24 @@ STAGE PLANS:
                       expr: x
                       type: int
                 outputColumnNames: _col0
-                Select Operator
-                  expressions:
-                        expr: _col0
-                        type: int
-                  outputColumnNames: _col0
-                  File Output Operator
-                    compressed: false
-                    GlobalTableId: 0
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
-                    table:
-                        input format: org.apache.hadoop.mapred.TextInputFormat
-                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          columns _col0
-                          columns.types int
-                          escape.delim \
-                          serialization.format 1
-                    TotalFiles: 1
-                    GatherStats: false
-                    MultiFileSpray: false
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+#### A masked pattern was here ####
+                  NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      properties:
+                        columns _col0
+                        columns.types int
+                        escape.delim \
+                        hive.serialization.extend.nesting.levels true
+                        serialization.format 1
+                  TotalFiles: 1
+                  GatherStats: false
+                  MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
 #### A masked pattern was here ####
@@ -251,7 +249,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
               columns x,y
               columns.types int:string
@@ -292,11 +289,13 @@ STAGE PLANS:
 PREHOOK: query: -- List Bucketing Query
 select x from (select * from fact_daily where ds = '1') subq where x = 484
 PREHOOK: type: QUERY
+PREHOOK: Input: default@fact_daily
 PREHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- List Bucketing Query
 select x from (select * from fact_daily where ds = '1') subq where x = 484
 POSTHOOK: type: QUERY
+POSTHOOK: Input: default@fact_daily
 POSTHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -306,18 +305,18 @@ POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(n
 484
 PREHOOK: query: -- The second subquery
 -- explain plan shows which directory selected: Truncated Path -> Alias
-explain extended select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484
+explain extended select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484 ORDER BY x1, y1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- The second subquery
 -- explain plan shows which directory selected: Truncated Path -> Alias
-explain extended select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484
+explain extended select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484 ORDER BY x1, y1
 POSTHOOK: type: QUERY
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=2).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=2).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL x) x1) (TOK_SELEXPR (TOK_TABLE_OR_COL y) y1)) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '1')))) subq)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL x1)) (TOK_SELEXPR (TOK_TABLE_OR_COL y1))) (TOK_WHERE (= (TOK_TABLE_OR_COL x1) 484))))
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL x) x1) (TOK_SELEXPR (TOK_TABLE_OR_COL y) y1)) (TOK_WHERE (= (TOK_TABLE_OR_COL ds) '1')))) subq)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL x1)) (TOK_SELEXPR (TOK_TABLE_OR_COL y1))) (TOK_WHERE (= (TOK_TABLE_OR_COL x1) 484)) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL x1)) (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL y1)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -343,30 +342,19 @@ STAGE PLANS:
                       expr: y
                       type: string
                 outputColumnNames: _col0, _col1
-                Select Operator
-                  expressions:
+                Reduce Output Operator
+                  key expressions:
+                        expr: _col0
+                        type: int
+                        expr: _col1
+                        type: string
+                  sort order: ++
+                  tag: -1
+                  value expressions:
                         expr: _col0
                         type: int
                         expr: _col1
                         type: string
-                  outputColumnNames: _col0, _col1
-                  File Output Operator
-                    compressed: false
-                    GlobalTableId: 0
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
-#### A masked pattern was here ####
-                    table:
-                        input format: org.apache.hadoop.mapred.TextInputFormat
-                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          columns _col0,_col1
-                          columns.types int:string
-                          escape.delim \
-                          serialization.format 1
-                    TotalFiles: 1
-                    GatherStats: false
-                    MultiFileSpray: false
       Needs Tagging: false
       Path -> Alias:
 #### A masked pattern was here ####
@@ -379,7 +367,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
               columns x,y
               columns.types int:string
@@ -409,6 +396,26 @@ STAGE PLANS:
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.fact_daily
             name: default.fact_daily
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            GlobalTableId: 0
+#### A masked pattern was here ####
+            NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                properties:
+                  columns _col0,_col1
+                  columns.types int:string
+                  escape.delim \
+                  hive.serialization.extend.nesting.levels true
+                  serialization.format 1
+            TotalFiles: 1
+            GatherStats: false
+            MultiFileSpray: false
       Truncated Path -> Alias:
         /fact_tz/ds=1/x=484 [subq:fact_daily]
 
@@ -418,13 +425,15 @@ STAGE PLANS:
 
 
 PREHOOK: query: -- List Bucketing Query
-select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484
+select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484 ORDER BY x1, y1
 PREHOOK: type: QUERY
+PREHOOK: Input: default@fact_daily
 PREHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- List Bucketing Query
-select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484
+select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484 ORDER BY x1, y1
 POSTHOOK: type: QUERY
+POSTHOOK: Input: default@fact_daily
 POSTHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -502,7 +511,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
               columns x,y
               columns.types int:string
@@ -562,6 +570,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -577,11 +586,13 @@ STAGE PLANS:
 PREHOOK: query: -- List Bucketing Query
 select y, count(1) from fact_daily where ds ='1' and x = 484 group by y
 PREHOOK: type: QUERY
+PREHOOK: Input: default@fact_daily
 PREHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- List Bucketing Query
 select y, count(1) from fact_daily where ds ='1' and x = 484 group by y
 POSTHOOK: type: QUERY
+POSTHOOK: Input: default@fact_daily
 POSTHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -659,7 +670,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
               columns x,y
               columns.types int:string
@@ -706,30 +716,24 @@ STAGE PLANS:
                   expr: _col1
                   type: bigint
             outputColumnNames: _col0, _col1
-            Select Operator
-              expressions:
-                    expr: _col0
-                    type: int
-                    expr: _col1
-                    type: bigint
-              outputColumnNames: _col0, _col1
-              File Output Operator
-                compressed: false
-                GlobalTableId: 0
-#### A masked pattern was here ####
-                NumFilesPerFileSink: 1
-#### A masked pattern was here ####
-                table:
-                    input format: org.apache.hadoop.mapred.TextInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    properties:
-                      columns _col0,_col1
-                      columns.types int:bigint
-                      escape.delim \
-                      serialization.format 1
-                TotalFiles: 1
-                GatherStats: false
-                MultiFileSpray: false
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+#### A masked pattern was here ####
+              NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  properties:
+                    columns _col0,_col1
+                    columns.types int:bigint
+                    escape.delim \
+                    hive.serialization.extend.nesting.levels true
+                    serialization.format 1
+              TotalFiles: 1
+              GatherStats: false
+              MultiFileSpray: false
       Truncated Path -> Alias:
         /fact_tz/ds=1/x=484 [subq:fact_daily]
 
@@ -741,11 +745,13 @@ STAGE PLANS:
 PREHOOK: query: -- List Bucketing Query
 select x, c from (select x, count(1) as c from fact_daily where ds = '1' group by x) subq where x = 484
 PREHOOK: type: QUERY
+PREHOOK: Input: default@fact_daily
 PREHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- List Bucketing Query
 select x, c from (select x, count(1) as c from fact_daily where ds = '1' group by x) subq where x = 484
 POSTHOOK: type: QUERY
+POSTHOOK: Input: default@fact_daily
 POSTHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
diff --git a/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_3.q.out b/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_3.q.out
index 76ef67d..0d6d168 100644
--- a/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_3.q.out
+++ b/src/ql/src/test/results/clientpositive/list_bucket_query_oneskew_3.q.out
@@ -216,12 +216,14 @@ Skewed Values:      	[[484], [238]]
 #### A masked pattern was here ####
 Storage Desc Params:	 	 
 	serialization.format	1                   
-PREHOOK: query: SELECT * FROM fact_daily WHERE ds='1'
+PREHOOK: query: SELECT * FROM fact_daily WHERE ds='1' ORDER BY x, y, z
 PREHOOK: type: QUERY
+PREHOOK: Input: default@fact_daily
 PREHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
-POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='1'
+POSTHOOK: query: SELECT * FROM fact_daily WHERE ds='1' ORDER BY x, y, z
 POSTHOOK: type: QUERY
+POSTHOOK: Input: default@fact_daily
 POSTHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -234,18 +236,18 @@ POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSche
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 86	val_86	val_86	1
-278	val_278	val_278	1
-278	val_278	val_278	1
 238	val_238	val_238	1
 238	val_238	val_238	1
+278	val_278	val_278	1
+278	val_278	val_278	1
 484	val_484	val_484	1
 PREHOOK: query: -- pruner  pick up right directory
 -- explain plan shows which directory selected: Truncated Path -> Alias
-explain extended SELECT x FROM fact_daily WHERE ds='1' and not (x = 86)
+explain extended SELECT x FROM fact_daily WHERE ds='1' and not (x = 86) ORDER BY x
 PREHOOK: type: QUERY
 POSTHOOK: query: -- pruner  pick up right directory
 -- explain plan shows which directory selected: Truncated Path -> Alias
-explain extended SELECT x FROM fact_daily WHERE ds='1' and not (x = 86)
+explain extended SELECT x FROM fact_daily WHERE ds='1' and not (x = 86) ORDER BY x
 POSTHOOK: type: QUERY
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
@@ -257,7 +259,7 @@ POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSche
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL x))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '1') (not (= (TOK_TABLE_OR_COL x) 86))))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME fact_daily))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL x))) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '1') (not (= (TOK_TABLE_OR_COL x) 86)))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL x)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -281,23 +283,15 @@ STAGE PLANS:
                       expr: x
                       type: int
                 outputColumnNames: _col0
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-#### A masked pattern was here ####
-                  NumFilesPerFileSink: 1
-#### A masked pattern was here ####
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      properties:
-                        columns _col0
-                        columns.types int
-                        escape.delim \
-                        serialization.format 1
-                  TotalFiles: 1
-                  GatherStats: false
-                  MultiFileSpray: false
+                Reduce Output Operator
+                  key expressions:
+                        expr: _col0
+                        type: int
+                  sort order: +
+                  tag: -1
+                  value expressions:
+                        expr: _col0
+                        type: int
       Needs Tagging: false
       Path -> Alias:
 #### A masked pattern was here ####
@@ -310,7 +304,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
               columns x,y,z
               columns.types int:string:string
@@ -348,7 +341,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
               columns x,y,z
               columns.types int:string:string
@@ -386,7 +378,6 @@ STAGE PLANS:
             partition values:
               ds 1
             properties:
-              EXTERNAL TRUE
               bucket_count -1
               columns x,y,z
               columns.types int:string:string
@@ -416,6 +407,26 @@ STAGE PLANS:
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.fact_daily
             name: default.fact_daily
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            GlobalTableId: 0
+#### A masked pattern was here ####
+            NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                properties:
+                  columns _col0
+                  columns.types int
+                  escape.delim \
+                  hive.serialization.extend.nesting.levels true
+                  serialization.format 1
+            TotalFiles: 1
+            GatherStats: false
+            MultiFileSpray: false
       Truncated Path -> Alias:
         /fact_tz/ds=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME [fact_daily]
         /fact_tz/ds=1/x=238 [fact_daily]
@@ -427,13 +438,15 @@ STAGE PLANS:
 
 
 PREHOOK: query: -- List Bucketing Query
-SELECT x,y,z FROM fact_daily WHERE ds='1' and not (x = 86)
+SELECT x FROM fact_daily WHERE ds='1' and not (x = 86) ORDER BY x
 PREHOOK: type: QUERY
+PREHOOK: Input: default@fact_daily
 PREHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- List Bucketing Query
-SELECT x,y,z FROM fact_daily WHERE ds='1' and not (x = 86)
+SELECT x FROM fact_daily WHERE ds='1' and not (x = 86) ORDER BY x
 POSTHOOK: type: QUERY
+POSTHOOK: Input: default@fact_daily
 POSTHOOK: Input: default@fact_daily@ds=1
 #### A masked pattern was here ####
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=1).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
@@ -445,8 +458,8 @@ POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=2).z SIMPLE [(src)src.FieldSchema(n
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).x EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).y SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 POSTHOOK: Lineage: fact_tz PARTITION(ds=1,hr=3).z SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-278	val_278	val_278
-278	val_278	val_278
-238	val_238	val_238
-238	val_238	val_238
-484	val_484	val_484
+238
+238
+278
+278
+484
diff --git a/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out b/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out
index 94936d8..ef6a7db 100644
--- a/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/louter_join_ppr.q.out
@@ -249,6 +249,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
@@ -651,6 +652,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
@@ -1054,6 +1056,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
@@ -1365,6 +1368,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/metadataonly1.q.out b/src/ql/src/test/results/clientpositive/metadataonly1.q.out
index b3836d5..3c5bfe9 100644
--- a/src/ql/src/test/results/clientpositive/metadataonly1.q.out
+++ b/src/ql/src/test/results/clientpositive/metadataonly1.q.out
@@ -65,6 +65,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -192,6 +193,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -322,6 +324,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -446,6 +449,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -832,6 +836,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -1081,6 +1086,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -1302,6 +1308,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -1468,6 +1475,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/nested_complex_cdh11925.q.out b/src/ql/src/test/results/clientpositive/nested_complex_cdh11925.q.out
new file mode 100644
index 0000000..c0b68c0
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/nested_complex_cdh11925.q.out
@@ -0,0 +1,63 @@
+PREHOOK: query: create table nestedcomplex (
+simple_int int,
+max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
+simple_string string)
+ROW FORMAT SERDE
+   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
+WITH SERDEPROPERTIES (
+   'hive.serialization.extend.nesting.levels'='true',
+   'line.delim'='\n'
+)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table nestedcomplex (
+simple_int int,
+max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
+max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
+simple_string string)
+ROW FORMAT SERDE
+   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
+WITH SERDEPROPERTIES (
+   'hive.serialization.extend.nesting.levels'='true',
+   'line.delim'='\n'
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@nestedcomplex
+PREHOOK: query: describe nestedcomplex
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: describe nestedcomplex
+POSTHOOK: type: DESCTABLE
+simple_int	int	
+max_nested_array	array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>	
+max_nested_map	array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>	
+max_nested_struct	array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string,i:bigint>>>>>>>>>>>>>>>>>>>>>>>	
+simple_string	string	
+PREHOOK: query: describe extended nestedcomplex
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: describe extended nestedcomplex
+POSTHOOK: type: DESCTABLE
+simple_int	int	
+max_nested_array	array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>	
+max_nested_map	array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>	
+max_nested_struct	array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string,i:bigint>>>>>>>>>>>>>>>>>>>>>>>	
+simple_string	string	
+	 	 
+#### A masked pattern was here ####
+PREHOOK: query: load data local inpath '../data/files/nested_complex.txt' overwrite into table nestedcomplex
+PREHOOK: type: LOAD
+PREHOOK: Output: default@nestedcomplex
+POSTHOOK: query: load data local inpath '../data/files/nested_complex.txt' overwrite into table nestedcomplex
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@nestedcomplex
+PREHOOK: query: select * from nestedcomplex sort by simple_int
+PREHOOK: type: QUERY
+PREHOOK: Input: default@nestedcomplex
+#### A masked pattern was here ####
+POSTHOOK: query: select * from nestedcomplex sort by simple_int
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@nestedcomplex
+#### A masked pattern was here ####
+2	[[[[[[[[[[[[[[[[[[[[[[[0,3,2]]]]]]]]]]]]]]]]]]]]]]]	[[[[[[[[[[[[[[[[[[[[[{"k1":"v1","k3":"v3"}]]]]]]]]]]]]]]]]]]]]]	[[[[[[[[[[[[[[[[[[[[[[{"s":"b","i":10}]]]]]]]]]]]]]]]]]]]]]]	2
+3	[[[[[[[[[[[[[[[[[[[[[[[0,1,2]]]]]]]]]]]]]]]]]]]]]]]	[[[[[[[[[[[[[[[[[[[[[{"k1":"v1","k2":"v2"}]]]]]]]]]]]]]]]]]]]]]	[[[[[[[[[[[[[[[[[[[[[[{"s":"a","i":10}]]]]]]]]]]]]]]]]]]]]]]	2
diff --git a/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out b/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out
index f311cce..5c79a4a 100644
--- a/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/outer_join_ppr.q.out
@@ -338,6 +338,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
@@ -731,6 +732,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/pcr.q.out b/src/ql/src/test/results/clientpositive/pcr.q.out
index cd3caff..b2ca247 100644
--- a/src/ql/src/test/results/clientpositive/pcr.q.out
+++ b/src/ql/src/test/results/clientpositive/pcr.q.out
@@ -214,6 +214,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -457,6 +458,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -699,6 +701,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -905,6 +908,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1158,6 +1162,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1422,6 +1427,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1645,6 +1651,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1830,6 +1837,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -2100,6 +2108,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -2359,6 +2368,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -2602,6 +2612,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3,_col4,_col5
                   columns.types int:string:string:int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -2925,6 +2936,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3,_col4,_col5
                   columns.types int:string:string:int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -3266,6 +3278,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -3559,6 +3572,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2
                   columns.types int:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -4949,6 +4963,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -5166,6 +5181,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -5390,6 +5406,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/ppd_join_filter.q.out b/src/ql/src/test/results/clientpositive/ppd_join_filter.q.out
index d76d5bd..46124f9 100644
--- a/src/ql/src/test/results/clientpositive/ppd_join_filter.q.out
+++ b/src/ql/src/test/results/clientpositive/ppd_join_filter.q.out
@@ -283,6 +283,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2
                     columns.types string:double:double
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -615,6 +616,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2
                     columns.types string:double:double
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -947,6 +949,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2
                     columns.types string:double:double
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -1279,6 +1282,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2
                     columns.types string:double:double
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/ppd_union_view.q.out b/src/ql/src/test/results/clientpositive/ppd_union_view.q.out
index 6663943..78b2c07 100644
--- a/src/ql/src/test/results/clientpositive/ppd_union_view.q.out
+++ b/src/ql/src/test/results/clientpositive/ppd_union_view.q.out
@@ -411,6 +411,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2
                           columns.types string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -465,6 +466,7 @@ STAGE PLANS:
                               columns _col0,_col1,_col2
                               columns.types string:string:string
                               escape.delim \
+                              hive.serialization.extend.nesting.levels true
                               serialization.format 1
                         TotalFiles: 1
                         GatherStats: false
@@ -714,6 +716,7 @@ STAGE PLANS:
                           columns _col0,_col1,_col2
                           columns.types string:string:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -763,6 +766,7 @@ STAGE PLANS:
                             columns _col0,_col1,_col2
                             columns.types string:string:string
                             escape.delim \
+                            hive.serialization.extend.nesting.levels true
                             serialization.format 1
                       TotalFiles: 1
                       GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out b/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out
index 2136a33..f8cce85 100644
--- a/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out
+++ b/src/ql/src/test/results/clientpositive/ppr_allchildsarenull.q.out
@@ -56,6 +56,7 @@ STAGE PLANS:
                         columns _col0,_col1
                         columns.types int:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -247,6 +248,7 @@ STAGE PLANS:
                         columns _col0,_col1
                         columns.types int:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/rand_partitionpruner1.q.out b/src/ql/src/test/results/clientpositive/rand_partitionpruner1.q.out
index 2f006c6..47b9447 100644
--- a/src/ql/src/test/results/clientpositive/rand_partitionpruner1.q.out
+++ b/src/ql/src/test/results/clientpositive/rand_partitionpruner1.q.out
@@ -44,6 +44,7 @@ STAGE PLANS:
                         columns _col0,_col1
                         columns.types string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out b/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out
index 600a834..a39cdba 100644
--- a/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out
+++ b/src/ql/src/test/results/clientpositive/rand_partitionpruner3.q.out
@@ -50,6 +50,7 @@ STAGE PLANS:
                         columns _col0,_col1,_col2,_col3
                         columns.types string:string:string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
@@ -177,6 +178,7 @@ STAGE PLANS:
                         columns _col0,_col1,_col2,_col3
                         columns.types string:string:string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/regexp_extract.q.out b/src/ql/src/test/results/clientpositive/regexp_extract.q.out
index 361d8ed..c666cd6 100644
--- a/src/ql/src/test/results/clientpositive/regexp_extract.q.out
+++ b/src/ql/src/test/results/clientpositive/regexp_extract.q.out
@@ -140,6 +140,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -398,6 +399,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/router_join_ppr.q.out b/src/ql/src/test/results/clientpositive/router_join_ppr.q.out
index 189e795..470f4a9 100644
--- a/src/ql/src/test/results/clientpositive/router_join_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/router_join_ppr.q.out
@@ -348,6 +348,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
@@ -657,6 +658,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
@@ -962,6 +964,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
@@ -1359,6 +1362,7 @@ STAGE PLANS:
                       columns _col0,_col1,_col2,_col3
                       columns.types string:string:string:string
                       escape.delim \
+                      hive.serialization.extend.nesting.levels true
                       serialization.format 1
                 TotalFiles: 1
                 GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/sample10.q.out b/src/ql/src/test/results/clientpositive/sample10.q.out
index e4fecbe..b61ef64 100644
--- a/src/ql/src/test/results/clientpositive/sample10.q.out
+++ b/src/ql/src/test/results/clientpositive/sample10.q.out
@@ -379,6 +379,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types string:bigint
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/sample6.q.out b/src/ql/src/test/results/clientpositive/sample6.q.out
index cd78d8b..9fa8774 100644
--- a/src/ql/src/test/results/clientpositive/sample6.q.out
+++ b/src/ql/src/test/results/clientpositive/sample6.q.out
@@ -677,6 +677,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1051,6 +1052,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1679,6 +1681,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -2150,6 +2153,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -2651,6 +2655,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -2909,6 +2914,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -3042,6 +3048,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types int:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/sample8.q.out b/src/ql/src/test/results/clientpositive/sample8.q.out
index 8f26dc8..792af23 100644
--- a/src/ql/src/test/results/clientpositive/sample8.q.out
+++ b/src/ql/src/test/results/clientpositive/sample8.q.out
@@ -361,6 +361,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/sample9.q.out b/src/ql/src/test/results/clientpositive/sample9.q.out
index 86e75dd..b4ead3d 100644
--- a/src/ql/src/test/results/clientpositive/sample9.q.out
+++ b/src/ql/src/test/results/clientpositive/sample9.q.out
@@ -53,6 +53,7 @@ STAGE PLANS:
                           columns _col0,_col1
                           columns.types int:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/smb_mapjoin9.q.out b/src/ql/src/test/results/clientpositive/smb_mapjoin9.q.out
index 2b5b4d8..79fa12d 100644
--- a/src/ql/src/test/results/clientpositive/smb_mapjoin9.q.out
+++ b/src/ql/src/test/results/clientpositive/smb_mapjoin9.q.out
@@ -91,6 +91,7 @@ STAGE PLANS:
                             columns _col0,_col1,_col2,_col3
                             columns.types int:string:string:int
                             escape.delim \
+                            hive.serialization.extend.nesting.levels true
                             serialization.format 1
                       TotalFiles: 1
                       GatherStats: false
@@ -199,6 +200,7 @@ STAGE PLANS:
                             columns _col0,_col1,_col2,_col3
                             columns.types int:string:string:int
                             escape.delim \
+                            hive.serialization.extend.nesting.levels true
                             serialization.format 1
                       TotalFiles: 1
                       GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/smb_mapjoin_13.q.out b/src/ql/src/test/results/clientpositive/smb_mapjoin_13.q.out
index 1e5e553..1faf1c1 100644
--- a/src/ql/src/test/results/clientpositive/smb_mapjoin_13.q.out
+++ b/src/ql/src/test/results/clientpositive/smb_mapjoin_13.q.out
@@ -245,6 +245,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3
                     columns.types int:string:int:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -500,6 +501,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2,_col3
                     columns.types int:string:int:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out
index 9c8368d..feba554 100644
--- a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out
+++ b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_5.q.out
@@ -226,6 +226,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out
index 0394c7d..40420b0 100644
--- a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out
+++ b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_6.q.out
@@ -257,6 +257,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out
index 493867f..f0d721d 100644
--- a/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out
+++ b/src/ql/src/test/results/clientpositive/sort_merge_join_desc_7.q.out
@@ -361,6 +361,7 @@ STAGE PLANS:
                     columns _col0
                     columns.types bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/transform_ppr1.q.out b/src/ql/src/test/results/clientpositive/transform_ppr1.q.out
index 740a931..10748c9 100644
--- a/src/ql/src/test/results/clientpositive/transform_ppr1.q.out
+++ b/src/ql/src/test/results/clientpositive/transform_ppr1.q.out
@@ -281,6 +281,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/transform_ppr2.q.out b/src/ql/src/test/results/clientpositive/transform_ppr2.q.out
index fb8e039..1a16702 100644
--- a/src/ql/src/test/results/clientpositive/transform_ppr2.q.out
+++ b/src/ql/src/test/results/clientpositive/transform_ppr2.q.out
@@ -191,6 +191,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types string:string
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/udf_explode.q.out b/src/ql/src/test/results/clientpositive/udf_explode.q.out
index dc6a513..8809375 100644
--- a/src/ql/src/test/results/clientpositive/udf_explode.q.out
+++ b/src/ql/src/test/results/clientpositive/udf_explode.q.out
@@ -48,6 +48,7 @@ STAGE PLANS:
                           columns col
                           columns.types int
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -287,6 +288,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types int:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -372,6 +374,7 @@ STAGE PLANS:
                           columns key,value
                           columns.types int:string
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -625,6 +628,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2
                     columns.types int:string:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/udf_java_method.q.out b/src/ql/src/test/results/clientpositive/udf_java_method.q.out
index 15e71e6..775dcaf 100644
--- a/src/ql/src/test/results/clientpositive/udf_java_method.q.out
+++ b/src/ql/src/test/results/clientpositive/udf_java_method.q.out
@@ -81,6 +81,7 @@ STAGE PLANS:
                         columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                         columns.types string:string:string:string:string:string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/udf_reflect.q.out b/src/ql/src/test/results/clientpositive/udf_reflect.q.out
index 69eb5d9..76ef006 100644
--- a/src/ql/src/test/results/clientpositive/udf_reflect.q.out
+++ b/src/ql/src/test/results/clientpositive/udf_reflect.q.out
@@ -77,6 +77,7 @@ STAGE PLANS:
                         columns _col0,_col1,_col2,_col3,_col4,_col5,_col6
                         columns.types string:string:string:string:string:string:string
                         escape.delim \
+                        hive.serialization.extend.nesting.levels true
                         serialization.format 1
                   TotalFiles: 1
                   GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/udtf_explode.q.out b/src/ql/src/test/results/clientpositive/udtf_explode.q.out
index 1912cf1..327cead 100644
--- a/src/ql/src/test/results/clientpositive/udtf_explode.q.out
+++ b/src/ql/src/test/results/clientpositive/udtf_explode.q.out
@@ -48,6 +48,7 @@ STAGE PLANS:
                           columns col
                           columns.types int
                           escape.delim \
+                          hive.serialization.extend.nesting.levels true
                           serialization.format 1
                     TotalFiles: 1
                     GatherStats: false
@@ -287,6 +288,7 @@ STAGE PLANS:
                     columns _col0,_col1
                     columns.types int:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
@@ -565,6 +567,7 @@ STAGE PLANS:
                     columns _col0,_col1,_col2
                     columns.types int:string:bigint
                     escape.delim \
+                    hive.serialization.extend.nesting.levels true
                     serialization.format 1
               TotalFiles: 1
               GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/union24.q.out b/src/ql/src/test/results/clientpositive/union24.q.out
index 50ae7e3..e18667e 100644
--- a/src/ql/src/test/results/clientpositive/union24.q.out
+++ b/src/ql/src/test/results/clientpositive/union24.q.out
@@ -477,6 +477,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types string:bigint
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -965,6 +966,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types string:bigint
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
@@ -1518,6 +1520,7 @@ STAGE PLANS:
                   columns _col0,_col1
                   columns.types string:bigint
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/clientpositive/union_ppr.q.out b/src/ql/src/test/results/clientpositive/union_ppr.q.out
index 756a9cd..0598ae6 100644
--- a/src/ql/src/test/results/clientpositive/union_ppr.q.out
+++ b/src/ql/src/test/results/clientpositive/union_ppr.q.out
@@ -244,6 +244,7 @@ STAGE PLANS:
                   columns _col0,_col1,_col2,_col3
                   columns.types string:string:string:string
                   escape.delim \
+                  hive.serialization.extend.nesting.levels true
                   serialization.format 1
             TotalFiles: 1
             GatherStats: false
diff --git a/src/ql/src/test/results/compiler/plan/cast1.q.xml b/src/ql/src/test/results/compiler/plan/cast1.q.xml
index 4bb6df9..4a26ce4 100644
--- a/src/ql/src/test/results/compiler/plan/cast1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/cast1.q.xml
@@ -198,6 +198,10 @@
                         <void property="properties"> 
                          <object class="java.util.Properties"> 
                           <void method="put"> 
+                           <string>hive.serialization.extend.nesting.levels</string> 
+                           <string>true</string> 
+                          </void> 
+                          <void method="put"> 
                            <string>columns</string> 
                            <string>_col0,_col1,_col2,_col3,_col4,_col5,_col6</string> 
                           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby2.q.xml b/src/ql/src/test/results/compiler/plan/groupby2.q.xml
index abb394c..36c3da0 100755
--- a/src/ql/src/test/results/compiler/plan/groupby2.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby2.q.xml
@@ -1332,6 +1332,10 @@
                   <void property="properties"> 
                    <object class="java.util.Properties"> 
                     <void method="put"> 
+                     <string>hive.serialization.extend.nesting.levels</string> 
+                     <string>true</string> 
+                    </void> 
+                    <void method="put"> 
                      <string>columns</string> 
                      <string>_col0,_col1,_col2</string> 
                     </void> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby3.q.xml b/src/ql/src/test/results/compiler/plan/groupby3.q.xml
index e20c3bd..757eae5 100644
--- a/src/ql/src/test/results/compiler/plan/groupby3.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby3.q.xml
@@ -1531,6 +1531,10 @@
                   <void property="properties"> 
                    <object class="java.util.Properties"> 
                     <void method="put"> 
+                     <string>hive.serialization.extend.nesting.levels</string> 
+                     <string>true</string> 
+                    </void> 
+                    <void method="put"> 
                      <string>columns</string> 
                      <string>_col0,_col1,_col2,_col3,_col4</string> 
                     </void> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby4.q.xml b/src/ql/src/test/results/compiler/plan/groupby4.q.xml
index 1b1487e..3f1e041 100644
--- a/src/ql/src/test/results/compiler/plan/groupby4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby4.q.xml
@@ -981,6 +981,10 @@
                   <void property="properties"> 
                    <object class="java.util.Properties"> 
                     <void method="put"> 
+                     <string>hive.serialization.extend.nesting.levels</string> 
+                     <string>true</string> 
+                    </void> 
+                    <void method="put"> 
                      <string>columns</string> 
                      <string>_col0</string> 
                     </void> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby5.q.xml b/src/ql/src/test/results/compiler/plan/groupby5.q.xml
index f73e9f1..d696eb0 100644
--- a/src/ql/src/test/results/compiler/plan/groupby5.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby5.q.xml
@@ -1074,6 +1074,10 @@
                   <void property="properties"> 
                    <object class="java.util.Properties"> 
                     <void method="put"> 
+                     <string>hive.serialization.extend.nesting.levels</string> 
+                     <string>true</string> 
+                    </void> 
+                    <void method="put"> 
                      <string>columns</string> 
                      <string>_col0,_col1</string> 
                     </void> 
diff --git a/src/ql/src/test/results/compiler/plan/groupby6.q.xml b/src/ql/src/test/results/compiler/plan/groupby6.q.xml
index fdd18fd..9d9c866 100644
--- a/src/ql/src/test/results/compiler/plan/groupby6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/groupby6.q.xml
@@ -981,6 +981,10 @@
                   <void property="properties"> 
                    <object class="java.util.Properties"> 
                     <void method="put"> 
+                     <string>hive.serialization.extend.nesting.levels</string> 
+                     <string>true</string> 
+                    </void> 
+                    <void method="put"> 
                      <string>columns</string> 
                      <string>_col0</string> 
                     </void> 
diff --git a/src/ql/src/test/results/compiler/plan/input20.q.xml b/src/ql/src/test/results/compiler/plan/input20.q.xml
index 7e3b577..276c25a 100644
--- a/src/ql/src/test/results/compiler/plan/input20.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input20.q.xml
@@ -1093,6 +1093,10 @@
                       <void property="properties"> 
                        <object class="java.util.Properties"> 
                         <void method="put"> 
+                         <string>hive.serialization.extend.nesting.levels</string> 
+                         <string>true</string> 
+                        </void> 
+                        <void method="put"> 
                          <string>columns</string> 
                          <string>_col0,_col1</string> 
                         </void> 
diff --git a/src/ql/src/test/results/compiler/plan/input8.q.xml b/src/ql/src/test/results/compiler/plan/input8.q.xml
index 9767451..82a532a 100644
--- a/src/ql/src/test/results/compiler/plan/input8.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input8.q.xml
@@ -194,6 +194,10 @@
                     <void property="properties"> 
                      <object class="java.util.Properties"> 
                       <void method="put"> 
+                       <string>hive.serialization.extend.nesting.levels</string> 
+                       <string>true</string> 
+                      </void> 
+                      <void method="put"> 
                        <string>columns</string> 
                        <string>_col0,_col1,_col2</string> 
                       </void> 
diff --git a/src/ql/src/test/results/compiler/plan/input_part1.q.xml b/src/ql/src/test/results/compiler/plan/input_part1.q.xml
index 05ebbc5..5966c24 100644
--- a/src/ql/src/test/results/compiler/plan/input_part1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input_part1.q.xml
@@ -235,6 +235,10 @@
                         <void property="properties"> 
                          <object class="java.util.Properties"> 
                           <void method="put"> 
+                           <string>hive.serialization.extend.nesting.levels</string> 
+                           <string>true</string> 
+                          </void> 
+                          <void method="put"> 
                            <string>columns</string> 
                            <string>_col0,_col1,_col2,_col3</string> 
                           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml b/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml
index a055bbf..38c1557 100644
--- a/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input_testxpath.q.xml
@@ -202,6 +202,10 @@
                     <void property="properties"> 
                      <object class="java.util.Properties"> 
                       <void method="put"> 
+                       <string>hive.serialization.extend.nesting.levels</string> 
+                       <string>true</string> 
+                      </void> 
+                      <void method="put"> 
                        <string>columns</string> 
                        <string>_col0,_col1,_col2</string> 
                       </void> 
diff --git a/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml b/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
index 27b4ed6..d7e4d4a 100644
--- a/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
+++ b/src/ql/src/test/results/compiler/plan/input_testxpath2.q.xml
@@ -206,6 +206,10 @@
                         <void property="properties"> 
                          <object class="java.util.Properties"> 
                           <void method="put"> 
+                           <string>hive.serialization.extend.nesting.levels</string> 
+                           <string>true</string> 
+                          </void> 
+                          <void method="put"> 
                            <string>columns</string> 
                            <string>_col0,_col1,_col2</string> 
                           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/join4.q.xml b/src/ql/src/test/results/compiler/plan/join4.q.xml
index 138a8db..30dce99 100644
--- a/src/ql/src/test/results/compiler/plan/join4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join4.q.xml
@@ -1712,6 +1712,10 @@
                       <void property="properties"> 
                        <object class="java.util.Properties"> 
                         <void method="put"> 
+                         <string>hive.serialization.extend.nesting.levels</string> 
+                         <string>true</string> 
+                        </void> 
+                        <void method="put"> 
                          <string>columns</string> 
                          <string>_col0,_col1,_col2,_col3</string> 
                         </void> 
diff --git a/src/ql/src/test/results/compiler/plan/join5.q.xml b/src/ql/src/test/results/compiler/plan/join5.q.xml
index 59a64fd..d046394 100644
--- a/src/ql/src/test/results/compiler/plan/join5.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join5.q.xml
@@ -1712,6 +1712,10 @@
                       <void property="properties"> 
                        <object class="java.util.Properties"> 
                         <void method="put"> 
+                         <string>hive.serialization.extend.nesting.levels</string> 
+                         <string>true</string> 
+                        </void> 
+                        <void method="put"> 
                          <string>columns</string> 
                          <string>_col0,_col1,_col2,_col3</string> 
                         </void> 
diff --git a/src/ql/src/test/results/compiler/plan/join6.q.xml b/src/ql/src/test/results/compiler/plan/join6.q.xml
index 9602cbd..49080eb 100644
--- a/src/ql/src/test/results/compiler/plan/join6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join6.q.xml
@@ -1712,6 +1712,10 @@
                       <void property="properties"> 
                        <object class="java.util.Properties"> 
                         <void method="put"> 
+                         <string>hive.serialization.extend.nesting.levels</string> 
+                         <string>true</string> 
+                        </void> 
+                        <void method="put"> 
                          <string>columns</string> 
                          <string>_col0,_col1,_col2,_col3</string> 
                         </void> 
diff --git a/src/ql/src/test/results/compiler/plan/join7.q.xml b/src/ql/src/test/results/compiler/plan/join7.q.xml
index 9db5901..298ce5d 100644
--- a/src/ql/src/test/results/compiler/plan/join7.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join7.q.xml
@@ -2436,6 +2436,10 @@
                       <void property="properties"> 
                        <object class="java.util.Properties"> 
                         <void method="put"> 
+                         <string>hive.serialization.extend.nesting.levels</string> 
+                         <string>true</string> 
+                        </void> 
+                        <void method="put"> 
                          <string>columns</string> 
                          <string>_col0,_col1,_col2,_col3,_col4,_col5</string> 
                         </void> 
diff --git a/src/ql/src/test/results/compiler/plan/join8.q.xml b/src/ql/src/test/results/compiler/plan/join8.q.xml
index a7ea611..2900921 100644
--- a/src/ql/src/test/results/compiler/plan/join8.q.xml
+++ b/src/ql/src/test/results/compiler/plan/join8.q.xml
@@ -1798,6 +1798,10 @@
                           <void property="properties"> 
                            <object class="java.util.Properties"> 
                             <void method="put"> 
+                             <string>hive.serialization.extend.nesting.levels</string> 
+                             <string>true</string> 
+                            </void> 
+                            <void method="put"> 
                              <string>columns</string> 
                              <string>_col0,_col1,_col2,_col3</string> 
                             </void> 
diff --git a/src/ql/src/test/results/compiler/plan/sample1.q.xml b/src/ql/src/test/results/compiler/plan/sample1.q.xml
index 3374432..4cff02d 100644
--- a/src/ql/src/test/results/compiler/plan/sample1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/sample1.q.xml
@@ -235,6 +235,10 @@
                         <void property="properties"> 
                          <object class="java.util.Properties"> 
                           <void method="put"> 
+                           <string>hive.serialization.extend.nesting.levels</string> 
+                           <string>true</string> 
+                          </void> 
+                          <void method="put"> 
                            <string>columns</string> 
                            <string>_col0,_col1,_col2,_col3</string> 
                           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/udf1.q.xml b/src/ql/src/test/results/compiler/plan/udf1.q.xml
index 5a81d07..0a86c2e 100644
--- a/src/ql/src/test/results/compiler/plan/udf1.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf1.q.xml
@@ -198,6 +198,10 @@
                         <void property="properties"> 
                          <object class="java.util.Properties"> 
                           <void method="put"> 
+                           <string>hive.serialization.extend.nesting.levels</string> 
+                           <string>true</string> 
+                          </void> 
+                          <void method="put"> 
                            <string>columns</string> 
                            <string>_col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11,_col12,_col13,_col14,_col15,_col16</string> 
                           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/udf4.q.xml b/src/ql/src/test/results/compiler/plan/udf4.q.xml
index 091b16e..37282cc 100644
--- a/src/ql/src/test/results/compiler/plan/udf4.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf4.q.xml
@@ -174,6 +174,10 @@
                     <void property="properties"> 
                      <object class="java.util.Properties"> 
                       <void method="put"> 
+                       <string>hive.serialization.extend.nesting.levels</string> 
+                       <string>true</string> 
+                      </void> 
+                      <void method="put"> 
                        <string>columns</string> 
                        <string>_col0,_col1,_col2,_col3,_col4,_col5,_col6,_col7,_col8,_col9,_col10,_col11,_col12,_col13,_col14,_col15,_col16,_col17,_col18</string> 
                       </void> 
diff --git a/src/ql/src/test/results/compiler/plan/udf6.q.xml b/src/ql/src/test/results/compiler/plan/udf6.q.xml
index 9bf5e19..480af9d 100644
--- a/src/ql/src/test/results/compiler/plan/udf6.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf6.q.xml
@@ -194,6 +194,10 @@
                     <void property="properties"> 
                      <object class="java.util.Properties"> 
                       <void method="put"> 
+                       <string>hive.serialization.extend.nesting.levels</string> 
+                       <string>true</string> 
+                      </void> 
+                      <void method="put"> 
                        <string>columns</string> 
                        <string>_col0,_col1</string> 
                       </void> 
diff --git a/src/ql/src/test/results/compiler/plan/udf_case.q.xml b/src/ql/src/test/results/compiler/plan/udf_case.q.xml
index aba590c..26bbd89 100644
--- a/src/ql/src/test/results/compiler/plan/udf_case.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf_case.q.xml
@@ -198,6 +198,10 @@
                         <void property="properties"> 
                          <object class="java.util.Properties"> 
                           <void method="put"> 
+                           <string>hive.serialization.extend.nesting.levels</string> 
+                           <string>true</string> 
+                          </void> 
+                          <void method="put"> 
                            <string>columns</string> 
                            <string>_col0,_col1</string> 
                           </void> 
diff --git a/src/ql/src/test/results/compiler/plan/udf_when.q.xml b/src/ql/src/test/results/compiler/plan/udf_when.q.xml
index 7c0e3aa..07757d2 100644
--- a/src/ql/src/test/results/compiler/plan/udf_when.q.xml
+++ b/src/ql/src/test/results/compiler/plan/udf_when.q.xml
@@ -198,6 +198,10 @@
                         <void property="properties"> 
                          <object class="java.util.Properties"> 
                           <void method="put"> 
+                           <string>hive.serialization.extend.nesting.levels</string> 
+                           <string>true</string> 
+                          </void> 
+                          <void method="put"> 
                            <string>columns</string> 
                            <string>_col0,_col1</string> 
                           </void> 
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
index 2c6251f..2976289 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
@@ -21,6 +21,7 @@ package org.apache.hadoop.hive.serde2.lazy;
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyMapObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory;
@@ -46,9 +47,9 @@ import org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger;
 import org.apache.hadoop.hive.serde2.lazydio.LazyDioLong;
 import org.apache.hadoop.hive.serde2.lazydio.LazyDioShort;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
 import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;
@@ -200,10 +201,11 @@ public final class LazyFactory {
    * @param nullSequence
    *          The sequence of bytes representing NULL.
    * @return The ObjectInspector
+   * @throws SerDeException
    */
   public static ObjectInspector createLazyObjectInspector(TypeInfo typeInfo,
       byte[] separator, int separatorIndex, Text nullSequence, boolean escaped,
-      byte escapeChar) {
+      byte escapeChar) throws SerDeException {
     ObjectInspector.Category c = typeInfo.getCategory();
     switch (c) {
     case PRIMITIVE:
@@ -217,13 +219,14 @@ public final class LazyFactory {
           nullSequence, escaped, escapeChar), createLazyObjectInspector(
           ((MapTypeInfo) typeInfo).getMapValueTypeInfo(), separator,
           separatorIndex + 2, nullSequence, escaped, escapeChar),
-          separator[separatorIndex], separator[separatorIndex + 1],
+          LazyUtils.getSeparator(separator, separatorIndex),
+          LazyUtils.getSeparator(separator, separatorIndex+1),
           nullSequence, escaped, escapeChar);
     case LIST:
       return LazyObjectInspectorFactory.getLazySimpleListObjectInspector(
           createLazyObjectInspector(((ListTypeInfo) typeInfo)
           .getListElementTypeInfo(), separator, separatorIndex + 1,
-          nullSequence, escaped, escapeChar), separator[separatorIndex],
+          nullSequence, escaped, escapeChar), LazyUtils.getSeparator(separator, separatorIndex),
           nullSequence, escaped, escapeChar);
     case STRUCT:
       StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
@@ -238,7 +241,8 @@ public final class LazyFactory {
             escapeChar));
       }
       return LazyObjectInspectorFactory.getLazySimpleStructObjectInspector(
-          fieldNames, fieldObjectInspectors, separator[separatorIndex],
+          fieldNames, fieldObjectInspectors,
+          LazyUtils.getSeparator(separator, separatorIndex),
           nullSequence, false, escaped, escapeChar);
     case UNION:
       UnionTypeInfo unionTypeInfo = (UnionTypeInfo) typeInfo;
@@ -249,7 +253,8 @@ public final class LazyFactory {
             escapeChar));
       }
       return LazyObjectInspectorFactory.getLazyUnionObjectInspector(lazyOIs,
-          separator[separatorIndex], nullSequence, escaped, escapeChar);
+          LazyUtils.getSeparator(separator, separatorIndex),
+          nullSequence, escaped, escapeChar);
     }
 
     throw new RuntimeException("Hive LazySerDe Internal error.");
@@ -262,13 +267,14 @@ public final class LazyFactory {
    * @param lastColumnTakesRest
    *          whether the last column of the struct should take the rest of the
    *          row if there are extra fields.
+   * @throws SerDeException
    * @see LazyFactory#createLazyObjectInspector(TypeInfo, byte[], int, Text,
    *      boolean, byte)
    */
   public static ObjectInspector createLazyStructInspector(
       List<String> columnNames, List<TypeInfo> typeInfos, byte[] separators,
       Text nullSequence, boolean lastColumnTakesRest, boolean escaped,
-      byte escapeChar) {
+      byte escapeChar) throws SerDeException {
     ArrayList<ObjectInspector> columnObjectInspectors = new ArrayList<ObjectInspector>(
         typeInfos.size());
     for (int i = 0; i < typeInfos.size(); i++) {
@@ -283,13 +289,14 @@ public final class LazyFactory {
   /**
    * Create a hierarchical ObjectInspector for ColumnarStruct with the given
    * columnNames and columnTypeInfos.
+   * @throws SerDeException
    *
    * @see LazyFactory#createLazyObjectInspector(TypeInfo, byte[], int, Text,
    *      boolean, byte)
    */
   public static ObjectInspector createColumnarStructInspector(
       List<String> columnNames, List<TypeInfo> columnTypes, byte[] separators,
-      Text nullSequence, boolean escaped, byte escapeChar) {
+      Text nullSequence, boolean escaped, byte escapeChar) throws SerDeException {
     ArrayList<ObjectInspector> columnObjectInspectors = new ArrayList<ObjectInspector>(
         columnTypes.size());
     for (int i = 0; i < columnTypes.size(); i++) {
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
index 385c98d..d0fcb00 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java
@@ -64,6 +64,9 @@ public class LazySimpleSerDe implements SerDe {
   public static final Log LOG = LogFactory.getLog(LazySimpleSerDe.class
       .getName());
 
+  public static final String SERIALIZATION_EXTEND_NESTING_LEVELS
+    = "hive.serialization.extend.nesting.levels";
+
   public static final byte[] DefaultSeparators = {(byte) 1, (byte) 2, (byte) 3};
 
   private ObjectInspector cachedObjectInspector;
@@ -176,6 +179,7 @@ public class LazySimpleSerDe implements SerDe {
    *
    * @see SerDe#initialize(Configuration, Properties)
    */
+  @Override
   public void initialize(Configuration job, Properties tbl)
       throws SerDeException {
 
@@ -208,9 +212,13 @@ public class LazySimpleSerDe implements SerDe {
   public static SerDeParameters initSerdeParams(Configuration job,
       Properties tbl, String serdeName) throws SerDeException {
     SerDeParameters serdeParams = new SerDeParameters();
-    // Read the separators: We use 8 levels of separators by default, but we
-    // should change this when we allow users to specify more than 10 levels
-    // of separators through DDL.
+    // Read the separators: We use 8 levels of separators by default,
+    // and 24 if SERIALIZATION_EXTEND_NESTING_LEVELS is set to true
+    // The levels possible are the set of control chars that we can use as
+    // special delimiters, ie they should absent in the data or escaped.
+    // To increase this level further, we need to stop relying
+    // on single control chars delimiters
+
     serdeParams.separators = new byte[8];
     serdeParams.separators[0] = getByte(tbl.getProperty(serdeConstants.FIELD_DELIM,
         tbl.getProperty(serdeConstants.SERIALIZATION_FORMAT)), DefaultSeparators[0]);
@@ -218,8 +226,53 @@ public class LazySimpleSerDe implements SerDe {
         .getProperty(serdeConstants.COLLECTION_DELIM), DefaultSeparators[1]);
     serdeParams.separators[2] = getByte(
         tbl.getProperty(serdeConstants.MAPKEY_DELIM), DefaultSeparators[2]);
-    for (int i = 3; i < serdeParams.separators.length; i++) {
-      serdeParams.separators[i] = (byte) (i + 1);
+    String extendedNesting =
+        tbl.getProperty(SERIALIZATION_EXTEND_NESTING_LEVELS);
+    if(extendedNesting == null || !extendedNesting.equalsIgnoreCase("true")){
+      //use the default smaller set of separators for backward compatibility
+      for (int i = 3; i < serdeParams.separators.length; i++) {
+        serdeParams.separators[i] = (byte) (i + 1);
+      }
+    }
+    else{
+      //If extended nesting is enabled, set the extended set of separator chars
+
+      final int MAX_CTRL_CHARS = 29;
+      byte[] extendedSeparators = new byte[MAX_CTRL_CHARS];
+      int extendedSeparatorsIdx = 0;
+
+      //get the first 3 separators that have already been set (defaults to 1,2,3)
+      for(int i = 0; i < 3; i++){
+        extendedSeparators[extendedSeparatorsIdx++] = serdeParams.separators[i];
+      }
+
+      for (byte asciival = 4; asciival <= MAX_CTRL_CHARS; asciival++) {
+
+        //use only control chars that are very unlikely to be part of the string
+        // the following might/likely to be used in text files for strings
+        // 9 (horizontal tab, HT, \t, ^I)
+        // 10 (line feed, LF, \n, ^J),
+        // 12 (form feed, FF, \f, ^L),
+        // 13 (carriage return, CR, \r, ^M),
+        // 27 (escape, ESC, \e [GCC only], ^[).
+
+        //reserving the following values for future dynamic level impl
+        // 30
+        // 31
+
+        switch(asciival){
+        case 9:
+        case 10:
+        case 12:
+        case 13:
+        case 27:
+          continue;
+        }
+        extendedSeparators[extendedSeparatorsIdx++] = asciival;
+      }
+
+      serdeParams.separators =
+          Arrays.copyOfRange(extendedSeparators, 0, extendedSeparatorsIdx);
     }
 
     serdeParams.nullString = tbl.getProperty(
@@ -271,6 +324,7 @@ public class LazySimpleSerDe implements SerDe {
    * @return The deserialized row Object.
    * @see SerDe#deserialize(Writable)
    */
+  @Override
   public Object deserialize(Writable field) throws SerDeException {
     if (byteArrayRef == null) {
       byteArrayRef = new ByteArrayRef();
@@ -296,6 +350,7 @@ public class LazySimpleSerDe implements SerDe {
   /**
    * Returns the ObjectInspector for the row.
    */
+  @Override
   public ObjectInspector getObjectInspector() throws SerDeException {
     return cachedObjectInspector;
   }
@@ -305,6 +360,7 @@ public class LazySimpleSerDe implements SerDe {
    *
    * @see SerDe#getSerializedClass()
    */
+  @Override
   public Class<? extends Writable> getSerializedClass() {
     return Text.class;
   }
@@ -323,6 +379,7 @@ public class LazySimpleSerDe implements SerDe {
    * @throws IOException
    * @see SerDe#serialize(Object, ObjectInspector)
    */
+  @Override
   public Writable serialize(Object obj, ObjectInspector objInspector)
       throws SerDeException {
 
@@ -409,11 +466,12 @@ public class LazySimpleSerDe implements SerDe {
    *          128. Negative byte values (or byte values >= 128) are never
    *          escaped.
    * @throws IOException
+   * @throws SerDeException
    */
   public static void serialize(ByteStream.Output out, Object obj,
       ObjectInspector objInspector, byte[] separators, int level,
       Text nullSequence, boolean escaped, byte escapeChar, boolean[] needsEscape)
-      throws IOException {
+      throws IOException, SerDeException {
 
     if (obj == null) {
       out.write(nullSequence.getBytes(), 0, nullSequence.getLength());
@@ -429,7 +487,7 @@ public class LazySimpleSerDe implements SerDe {
           needsEscape);
       return;
     case LIST:
-      separator = (char) separators[level];
+      separator = (char) LazyUtils.getSeparator(separators, level);
       ListObjectInspector loi = (ListObjectInspector) objInspector;
       list = loi.getList(obj);
       ObjectInspector eoi = loi.getListElementObjectInspector();
@@ -446,8 +504,10 @@ public class LazySimpleSerDe implements SerDe {
       }
       return;
     case MAP:
-      separator = (char) separators[level];
-      char keyValueSeparator = (char) separators[level + 1];
+      separator = (char) LazyUtils.getSeparator(separators, level);
+      char keyValueSeparator =
+           (char) LazyUtils.getSeparator(separators, level + 1);
+
       MapObjectInspector moi = (MapObjectInspector) objInspector;
       ObjectInspector koi = moi.getMapKeyObjectInspector();
       ObjectInspector voi = moi.getMapValueObjectInspector();
@@ -471,7 +531,7 @@ public class LazySimpleSerDe implements SerDe {
       }
       return;
     case STRUCT:
-      separator = (char) separators[level];
+      separator = (char) LazyUtils.getSeparator(separators, level);
       StructObjectInspector soi = (StructObjectInspector) objInspector;
       List<? extends StructField> fields = soi.getAllStructFieldRefs();
       list = soi.getStructFieldsDataAsList(obj);
@@ -489,7 +549,7 @@ public class LazySimpleSerDe implements SerDe {
       }
       return;
     case UNION:
-      separator = (char) separators[level];
+      separator = (char) LazyUtils.getSeparator(separators, level);
       UnionObjectInspector uoi = (UnionObjectInspector) objInspector;
       List<? extends ObjectInspector> ois = uoi.getObjectInspectors();
       if (ois == null) {
@@ -516,6 +576,7 @@ public class LazySimpleSerDe implements SerDe {
    * Returns the statistics after (de)serialization)
    */
 
+  @Override
   public SerDeStats getSerDeStats() {
     // must be different
     assert (lastOperationSerialize != lastOperationDeserialize);
diff --git a/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java b/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
index b93709e..0c45a38 100644
--- a/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
+++ b/src/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
@@ -367,6 +367,30 @@ public final class LazyUtils {
     return Arrays.copyOf(sourceBw.getBytes(), sourceBw.getLength());
   }
 
+  /**
+   * Utility function to get separator for current level used in serialization.
+   * Used to get a better log message when out of bound lookup happens
+   * @param separators - array of separators byte, byte at index x indicates
+   *  separator used at that level
+   * @param level - nesting level
+   * @return separator at given level
+   * @throws SerDeException
+   */
+  static byte getSeparator(byte[] separators, int level) throws SerDeException {
+    try{
+      return separators[level];
+    }catch(ArrayIndexOutOfBoundsException e){
+      String msg = "Number of levels of nesting supported for " +
+          "LazySimpleSerde is " + (separators.length - 1) +
+          " Unable to work with level " + level;
+      if(separators.length < 9){
+        msg += ". Use " + LazySimpleSerDe.SERIALIZATION_EXTEND_NESTING_LEVELS +
+            " serde property for tables using LazySimpleSerde.";
+      }
+      throw new SerDeException(msg, e);
+    }
+  }
+
   private LazyUtils() {
     // prevent instantiation
   }
diff --git a/src/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java b/src/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java
index 7d6aaf8..388ed8a 100644
--- a/src/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java
+++ b/src/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazyArrayMapStruct.java
@@ -19,13 +19,21 @@ package org.apache.hadoop.hive.serde2.lazy;
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Iterator;
 import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Properties;
 
 import junit.framework.TestCase;
 
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.io.IntWritable;
@@ -37,6 +45,10 @@ import org.apache.hadoop.io.Text;
  */
 public class TestLazyArrayMapStruct extends TestCase {
 
+  // nesting level limits
+  static final int EXTENDED_LEVEL_THRESHOLD = 24;
+  static final int DEFAULT_LEVEL_THRESHOLD = 8;
+
   /**
    * Test the LazyArray class.
    */
@@ -311,4 +323,287 @@ public class TestLazyArrayMapStruct extends TestCase {
       throw e;
     }
   }
+
+  /**
+   * Test the LazyArray class with multiple levels of nesting
+   */
+  public void testLazyArrayNested() throws Throwable {
+    for(int i = 2; i < EXTENDED_LEVEL_THRESHOLD; i++ ){
+      testNestedinArrayAtLevelExtended(i, ObjectInspector.Category.LIST);
+    }
+  }
+
+  /**
+   * Test the LazyArray class with multiple levels of nesting
+   */
+  public void testLazyArrayNestedExceedLimit() throws Throwable {
+    checkExtendedLimitExceeded(EXTENDED_LEVEL_THRESHOLD, ObjectInspector.Category.LIST);
+  }
+
+  private void checkExtendedLimitExceeded(int maxLevel, Category type) {
+    boolean foundException = false;
+    try {
+      testNestedinArrayAtLevelExtended(maxLevel, type);
+    }catch (SerDeException serdeEx){
+      foundException = true;
+    }
+    assertTrue("Got exception for exceeding nesting limit" , foundException);
+  }
+
+  /**
+   * Test the LazyArray class with multiple levels of nesting, when nesting
+   * levels are not extended
+   */
+  public void testLazyArrayNestedExceedLimitNotExtended() throws Throwable {
+    checkNotExtendedLimitExceeded(DEFAULT_LEVEL_THRESHOLD,
+        ObjectInspector.Category.LIST);
+  }
+
+  /**
+   * Test the LazyMap class with multiple levels of nesting, when nesting
+   * levels are not extended
+   */
+  public void testLazyMapNestedExceedLimitNotExtended() throws Throwable {
+    checkNotExtendedLimitExceeded(DEFAULT_LEVEL_THRESHOLD-1,
+        ObjectInspector.Category.MAP);
+  }
+
+  /**
+   * Test the LazyMap class with multiple levels of nesting, when nesting
+   * levels are not extended
+   */
+  public void testLazyStructNestedExceedLimitNotExtended() throws Throwable {
+    checkNotExtendedLimitExceeded(DEFAULT_LEVEL_THRESHOLD,
+        ObjectInspector.Category.STRUCT);
+  }
+
+  /**
+   * Test the LazyMap class with multiple levels of nesting, when nesting
+   * levels are not extended
+   */
+  public void testLazyUnionNestedExceedLimitNotExtended() throws Throwable {
+    checkNotExtendedLimitExceeded(DEFAULT_LEVEL_THRESHOLD,
+        ObjectInspector.Category.UNION);
+  }
+
+  private void checkNotExtendedLimitExceeded(int maxLevel, Category type) {
+    boolean foundException = false;
+    try {
+      testNestedinArrayAtLevel(maxLevel, type, new Properties());
+    }catch (SerDeException serdeEx){
+      foundException = true;
+    }
+    assertTrue("Expected exception for exceeding nesting limit" , foundException);
+  }
+
+  /**
+   * Test the LazyMap class with multiple levels of nesting
+   */
+  public void testLazyMapNested() throws Throwable {
+    //map max nesting level is one less because it uses an additional separator
+    for(int i = 2; i < EXTENDED_LEVEL_THRESHOLD - 1; i++ ){
+     testNestedinArrayAtLevelExtended(i, ObjectInspector.Category.MAP);
+    }
+  }
+
+  /**
+   * Test the LazyMap class with multiple levels of nesting
+   */
+  public void testLazyMapNestedExceedLimit() throws Throwable {
+    //map max nesting level is one less because it uses an additional separator
+    checkExtendedLimitExceeded(EXTENDED_LEVEL_THRESHOLD - 1, ObjectInspector.Category.MAP);
+  }
+
+  /**
+   * Test the LazyUnion class with multiple levels of nesting
+   */
+  public void testLazyUnionNested() throws Throwable {
+    for(int i = 2; i < EXTENDED_LEVEL_THRESHOLD; i++ ){
+     testNestedinArrayAtLevelExtended(i, ObjectInspector.Category.UNION);
+    }
+  }
+
+  /**
+   * Test the LazyUnion class with multiple levels of nesting
+   */
+  public void testLazyUnionNestedExceedLimit() throws Throwable {
+    checkExtendedLimitExceeded(EXTENDED_LEVEL_THRESHOLD, ObjectInspector.Category.UNION);
+  }
+
+  /**
+   * Test the LazyStruct class with multiple levels of nesting
+   */
+  public void testLazyStructNested() throws Throwable {
+    for(int i = 2; i < EXTENDED_LEVEL_THRESHOLD; i++ ){
+     testNestedinArrayAtLevelExtended(i, ObjectInspector.Category.STRUCT);
+    }
+  }
+
+  /**
+   * Verify the serialized format for given type dtype, when it is nested in an
+   * array with nestingLevel levels. with extended nesting enabled.
+   * @param nestingLevel
+   * @param dtype
+   * @throws SerDeException
+   */
+  private void testNestedinArrayAtLevelExtended(int nestingLevel,
+      ObjectInspector.Category dtype) throws SerDeException {
+    Properties tableProp = new Properties();
+    tableProp.setProperty(LazySimpleSerDe.SERIALIZATION_EXTEND_NESTING_LEVELS, "true");
+    testNestedinArrayAtLevel(nestingLevel, dtype, tableProp);
+  }
+
+  /**
+   * Test the LazyStruct class with multiple levels of nesting
+   */
+  public void testLazyStructNestedExceedLimit() throws Throwable {
+    checkExtendedLimitExceeded(EXTENDED_LEVEL_THRESHOLD, ObjectInspector.Category.STRUCT);
+  }
+
+  /**
+   * @param nestingLevel
+   * @param dtype
+   * @param tableProp
+   * @throws SerDeException
+   */
+  private void testNestedinArrayAtLevel(int nestingLevel,
+      ObjectInspector.Category dtype, Properties tableProp) throws SerDeException {
+
+    //create type with nestingLevel levels of nesting
+    //set inner schema for dtype
+    String inSchema = null;
+    switch(dtype){
+    case LIST:
+      inSchema = "array<tinyint>";
+      break;
+    case MAP:
+      inSchema = "map<string,int>";
+      break;
+    case STRUCT:
+      inSchema = "struct<s:string,i:tinyint>";
+      break;
+    case UNION:
+      inSchema = "uniontype<string,tinyint>";
+      break;
+    default :
+        fail("type not supported by test case");
+    }
+
+    StringBuilder schema = new StringBuilder(inSchema);
+    for(int i = 0; i < nestingLevel - 1; i++){
+      schema.insert(0, "array<");
+      schema.append(">");
+    }
+    System.err.println("Testing nesting level " + nestingLevel +
+        ". Using schema " + schema);
+
+
+    // Create the SerDe
+    LazySimpleSerDe serDe = new LazySimpleSerDe();
+    Configuration conf = new Configuration();
+    tableProp.setProperty("columns", "narray");
+    tableProp.setProperty("columns.types", schema.toString());
+    serDe.initialize(conf, tableProp);
+
+    //create the serialized string for type
+    byte[] separators = serDe.serdeParams.getSeparators();
+    System.err.println("Using separator " +  (char)separators[nestingLevel]);
+    byte [] serializedRow = null;
+    switch(dtype){
+    case LIST:
+      serializedRow = new byte[] {'8',separators[nestingLevel],'9'};
+      break;
+    case MAP:
+      byte kvSep = separators[nestingLevel+1];
+      byte kvPairSep = separators[nestingLevel];
+      serializedRow = new byte[] {'1', kvSep, '1', kvPairSep, '2', kvSep, '2'};
+      break;
+    case STRUCT:
+      serializedRow = new byte[] {'8',separators[nestingLevel],'9'};
+      break;
+    case UNION:
+      serializedRow = new byte[] {'0',separators[nestingLevel],'9'};
+      break;
+    default :
+        fail("type not supported by test case");
+    }
+
+
+    //create LazyStruct with serialized string with expected separators
+    StructObjectInspector oi = (StructObjectInspector) serDe
+        .getObjectInspector();
+    LazyStruct struct = (LazyStruct) LazyFactory.createLazyObject(oi);
+
+    TestLazyPrimitive.initLazyObject(struct, serializedRow, 0, serializedRow.length);
+
+
+    //Get fields out of the lazy struct and check if they match expected
+    // results
+    //Get first level array
+    LazyArray array = (LazyArray) struct.getField(0);
+
+    //Peel off the n-1 levels to get to the underlying array
+    for(int i = 0; i < nestingLevel - 2; i++){
+      array = (LazyArray) array.getListElementObject(0);
+    }
+
+    //verify the serialized format for dtype
+    switch(dtype){
+    case LIST:
+      LazyArray array1 = (LazyArray) array.getListElementObject(0);
+      //check elements of the innermost array
+      assertEquals(2, array1.getListLength());
+      assertEquals(new ByteWritable((byte) 8), ((LazyByte) array1
+          .getListElementObject(0)).getWritableObject());
+      assertEquals(new ByteWritable((byte) 9), ((LazyByte) array1
+          .getListElementObject(1)).getWritableObject());
+      break;
+
+    case MAP:
+      LazyMap lazyMap = (LazyMap) array.getListElementObject(0);
+      Map map = lazyMap.getMap();
+      System.err.println(map);
+      assertEquals(2, map.size());
+      Iterator<Map.Entry<LazyString, LazyInteger>> it = map.entrySet().iterator();
+
+      Entry<LazyString, LazyInteger> e1 = it.next();
+      assertEquals(e1.getKey().getWritableObject(), new Text(new byte[]{'1'}) );
+      assertEquals(e1.getValue().getWritableObject(), new IntWritable(1) );
+
+      Entry<LazyString, LazyInteger> e2 = it.next();
+      assertEquals(e2.getKey().getWritableObject(), new Text(new byte[]{'2'}) );
+      assertEquals(e2.getValue().getWritableObject(), new IntWritable(2) );
+      break;
+
+    case STRUCT:
+      LazyStruct innerStruct = (LazyStruct) array.getListElementObject(0);
+      //check elements of the innermost struct
+      assertEquals(2, innerStruct.getFieldsAsList().size());
+      assertEquals(new Text(new byte[]{'8'}),
+          ((LazyString) innerStruct.getField(0)).getWritableObject());
+      assertEquals(new ByteWritable((byte) 9),
+          ((LazyByte) innerStruct.getField(1)).getWritableObject());
+      break;
+
+    case UNION:
+      LazyUnion lazyUnion = (LazyUnion) array.getListElementObject(0);
+      //check elements of the innermost union
+      assertEquals(new Text(new byte[]{'9'}),
+          ((LazyString)lazyUnion.getField()).getWritableObject());
+      break;
+
+
+    default :
+        fail("type not supported by test case");
+    }
+
+    //test serialization
+    Text serializedText =
+        (Text) serDe.serialize(struct.getObject(), serDe.getObjectInspector());
+    org.junit.Assert.assertArrayEquals(serializedRow, serializedText.getBytes());
+
+  }
+
+
+
 }
-- 
1.7.0.4

