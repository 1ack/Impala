From e8bfbbe00f2eda4be69058f1463a8e1a03b4ed57 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Wed, 25 Sep 2013 22:59:23 +0000
Subject: [PATCH 205/218] CDH-14484:Backport HIVE-5318: Import Throws Error when Importing from a table export Hive 0.9 to Hive 0.10

---
 build-common.xml                                   |    2 +-
 data/files/exported_table/_metadata                |    1 +
 data/files/exported_table/data/data                |    2 +
 .../hadoop/hive/ql/plan/CreateTableDesc.java       |   24 +++++++++++--------
 .../queries/clientpositive/import_exported_table.q |   10 ++++++++
 .../clientpositive/import_exported_table.q.out     |   22 ++++++++++++++++++
 6 files changed, 50 insertions(+), 11 deletions(-)
 create mode 100644 data/files/exported_table/_metadata
 create mode 100644 data/files/exported_table/data/data
 create mode 100644 ql/src/test/queries/clientpositive/import_exported_table.q
 create mode 100644 ql/src/test/results/clientpositive/import_exported_table.q.out

diff --git a/src/build-common.xml b/src/build-common.xml
index 1f47386..c576eef 100644
--- a/src/build-common.xml
+++ b/src/build-common.xml
@@ -87,7 +87,7 @@
   <property name="test.output" value="true"/>
   <property name="test.junit.output.format" value="xml"/>
   <property name="test.junit.output.usefile" value="true"/>
-  <property name="minimr.query.files" value="input16_cc.q,scriptfile1.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,remote_script.q,load_fs2.q,load_hdfs_file_with_space_in_the_name.q"/>
+  <property name="minimr.query.files" value="input16_cc.q,scriptfile1.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,remote_script.q,load_fs2.q,load_hdfs_file_with_space_in_the_name.q,import_exported_table.q"/>
   <property name="minimr.query.negative.files" value="cluster_tasklog_retrieval.q,minimr_broken_pipe.q,mapreduce_stack_trace.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff_hadoop20.q" />
   <property name="test.silent" value="true"/>
   <property name="hadoopVersion" value="${hadoop.version.ant-internal}"/>
diff --git a/src/data/files/exported_table/_metadata b/src/data/files/exported_table/_metadata
new file mode 100644
index 0000000..81fbf63
--- /dev/null
+++ b/src/data/files/exported_table/_metadata
@@ -0,0 +1 @@
+{"partitions":[],"table":"{\"1\":{\"str\":\"j1_41\"},\"2\":{\"str\":\"default\"},\"3\":{\"str\":\"johndee\"},\"4\":{\"i32\":1371900915},\"5\":{\"i32\":0},\"6\":{\"i32\":0},\"7\":{\"rec\":{\"1\":{\"lst\":[\"rec\",2,{\"1\":{\"str\":\"a\"},\"2\":{\"str\":\"string\"}},{\"1\":{\"str\":\"b\"},\"2\":{\"str\":\"int\"}}]},\"2\":{\"str\":\"hdfs://hivebase01:8020/user/hive/warehouse/j1_41\"},\"3\":{\"str\":\"org.apache.hadoop.mapred.TextInputFormat\"},\"4\":{\"str\":\"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"},\"5\":{\"tf\":0},\"6\":{\"i32\":-1},\"7\":{\"rec\":{\"2\":{\"str\":\"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\"},\"3\":{\"map\":[\"str\",\"str\",2,{\"serialization.format\":\",\",\"field.delim\":\",\"}]}}},\"8\":{\"lst\":[\"str\",0]},\"9\":{\"lst\":[\"rec\",0]},\"10\":{\"map\":[\"str\",\"str\",0,{}]}}},\"8\":{\"lst\":[\"rec\",0]},\"9\":{\"map\":[\"str\",\"str\",1,{\"transient_lastDdlTime\":\"1371900931\"}]},\"12\":{\"str\":\"MANAGED_TABLE\"}}","version":"0.1"}
\ No newline at end of file
diff --git a/src/data/files/exported_table/data/data b/src/data/files/exported_table/data/data
new file mode 100644
index 0000000..40a75ac
--- /dev/null
+++ b/src/data/files/exported_table/data/data
@@ -0,0 +1,2 @@
+johndee,1
+burks,2
diff --git a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
index 25d04e1..c4c9d0b 100644
--- a/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
+++ b/src/ql/src/java/org/apache/hadoop/hive/ql/plan/CreateTableDesc.java
@@ -48,10 +48,10 @@ public class CreateTableDesc extends DDLDesc implements Serializable {
   String databaseName;
   String tableName;
   boolean isExternal;
-  ArrayList<FieldSchema> cols;
-  ArrayList<FieldSchema> partCols;
-  ArrayList<String> bucketCols;
-  ArrayList<Order> sortCols;
+  List<FieldSchema> cols;
+  List<FieldSchema> partCols;
+  List<String> bucketCols;
+  List<Order> sortCols;
   int numBuckets;
   String fieldDelim;
   String fieldEscape;
@@ -125,8 +125,12 @@ public class CreateTableDesc extends DDLDesc implements Serializable {
     this.serdeProps = serdeProps;
     this.tblProps = tblProps;
     this.ifNotExists = ifNotExists;
-    this.skewedColNames = new ArrayList<String>(skewedColNames);
-    this.skewedColValues = new ArrayList<List<String>>(skewedColValues);
+    this.skewedColNames = copyList(skewedColNames);
+    this.skewedColValues = copyList(skewedColValues);
+  }
+
+  private static <T> List<T> copyList(List<T> copy) {
+    return copy == null ? null : new ArrayList<T>(copy);
   }
 
   @Explain(displayName = "columns")
@@ -161,7 +165,7 @@ public class CreateTableDesc extends DDLDesc implements Serializable {
     this.tableName = tableName;
   }
 
-  public ArrayList<FieldSchema> getCols() {
+  public List<FieldSchema> getCols() {
     return cols;
   }
 
@@ -169,7 +173,7 @@ public class CreateTableDesc extends DDLDesc implements Serializable {
     this.cols = cols;
   }
 
-  public ArrayList<FieldSchema> getPartCols() {
+  public List<FieldSchema> getPartCols() {
     return partCols;
   }
 
@@ -178,7 +182,7 @@ public class CreateTableDesc extends DDLDesc implements Serializable {
   }
 
   @Explain(displayName = "bucket columns")
-  public ArrayList<String> getBucketCols() {
+  public List<String> getBucketCols() {
     return bucketCols;
   }
 
@@ -298,7 +302,7 @@ public class CreateTableDesc extends DDLDesc implements Serializable {
    * @return the sortCols
    */
   @Explain(displayName = "sort columns")
-  public ArrayList<Order> getSortCols() {
+  public List<Order> getSortCols() {
     return sortCols;
   }
 
diff --git a/src/ql/src/test/queries/clientpositive/import_exported_table.q b/src/ql/src/test/queries/clientpositive/import_exported_table.q
new file mode 100644
index 0000000..8496ec1
--- /dev/null
+++ b/src/ql/src/test/queries/clientpositive/import_exported_table.q
@@ -0,0 +1,10 @@
+dfs ${system:test.dfs.mkdir} hdfs:///tmp/test/;
+
+dfs -copyFromLocal ../data/files/exported_table hdfs:///tmp/test/;
+
+IMPORT FROM '/tmp/test/exported_table';
+DESCRIBE j1_41;
+SELECT * from j1_41;
+
+dfs -rmr hdfs:///tmp/test/exported_table;
+
diff --git a/src/ql/src/test/results/clientpositive/import_exported_table.q.out b/src/ql/src/test/results/clientpositive/import_exported_table.q.out
new file mode 100644
index 0000000..35e5637
--- /dev/null
+++ b/src/ql/src/test/results/clientpositive/import_exported_table.q.out
@@ -0,0 +1,22 @@
+#### A masked pattern was here ####
+PREHOOK: type: IMPORT
+#### A masked pattern was here ####
+POSTHOOK: type: IMPORT
+POSTHOOK: Output: default@j1_41
+PREHOOK: query: DESCRIBE j1_41
+PREHOOK: type: DESCTABLE
+POSTHOOK: query: DESCRIBE j1_41
+POSTHOOK: type: DESCTABLE
+a                   	string              	None                
+b                   	int                 	None                
+PREHOOK: query: SELECT * from j1_41
+PREHOOK: type: QUERY
+PREHOOK: Input: default@j1_41
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * from j1_41
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@j1_41
+#### A masked pattern was here ####
+johndee	1
+burks	2
+#### A masked pattern was here ####
-- 
1.7.0.4

