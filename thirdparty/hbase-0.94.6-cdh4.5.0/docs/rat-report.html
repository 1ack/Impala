<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<!-- Generated by Apache Maven Doxia at Nov 18, 2013 -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>HBase - RAT (Release Audit Tool) results</title>
    <style type="text/css" media="all">
      @import url("./css/maven-base.css");
      @import url("./css/maven-theme.css");
      @import url("./css/site.css");
    </style>
    <link rel="stylesheet" href="./css/print.css" type="text/css" media="print" />
    <link rel="shortcut icon" href="/images/favicon.ico" />
    <meta name="Date-Revision-yyyymmdd" content="20131118" />
    <meta http-equiv="Content-Language" content="en" />
        <!--Google Analytics-->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-30210968-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
  </head>
  <body class="composite">
    <div id="banner">
                  <a href="./" id="bannerLeft">
                                                <img src="images/hbase_logo.png" alt="$alt" />
                </a>
            <!-- Commented out since we do not use it.  St.Ack 20110906
       -->
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
      <div class="xright" style="padding-left: 8px; margin-top: -4px;">
        <form method="GET" action="http://search-hadoop.com/">
          <input type="text" style="width: 192px; height: 15px; font-size: inherit; border: 1px solid darkgray" name="q" value="Search wiki, mailing lists & more" onfocus="this.value=''"/>
          <input type="hidden" name="fc_project" value="HBase"/>
          <button style="height: 20px; width: 60px;">Search</button>
        </form>
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                
                                <h5>Apache HBase Project</h5>
                  <ul>
                  <li class="none">
                          <a href="index.html" title="Overview">Overview</a>
            </li>
                  <li class="none">
                          <a href="license.html" title="License">License</a>
            </li>
                  <li class="none">
                          <a href="http://www.apache.org/dyn/closer.cgi/hbase/" class="externalLink" title="Downloads">Downloads</a>
            </li>
                  <li class="none">
                          <a href="https://issues.apache.org/jira/browse/HBASE?report=com.atlassian.jira.plugin.system.project:changelog-panel#selectedTab=com.atlassian.jira.plugin.system.project%3Achangelog-panel" class="externalLink" title="Release Notes">Release Notes</a>
            </li>
                  <li class="none">
                          <a href="issue-tracking.html" title="Issue Tracking">Issue Tracking</a>
            </li>
                  <li class="none">
                          <a href="mail-lists.html" title="Mailing Lists">Mailing Lists</a>
            </li>
                  <li class="none">
                          <a href="source-repository.html" title="Source Repository">Source Repository</a>
            </li>
                  <li class="none">
                          <a href="https://reviews.apache.org" class="externalLink" title="ReviewBoard">ReviewBoard</a>
            </li>
                  <li class="none">
                          <a href="team-list.html" title="Team">Team</a>
            </li>
                  <li class="none">
                          <a href="sponsors.html" title="Thanks">Thanks</a>
            </li>
                  <li class="none">
                          <a href="http://blogs.apache.org/hbase/" class="externalLink" title="Blog">Blog</a>
            </li>
                  <li class="none">
                          <a href="resources.html" title="Other resources">Other resources</a>
            </li>
          </ul>
                       <h5>0.94 Documentation</h5>
                  <ul>
                  <li class="none">
                          <a href="book/quickstart.html" title="Getting Started">Getting Started</a>
            </li>
                  <li class="none">
                          <a href="apidocs/index.html" title="API">API</a>
            </li>
                  <li class="none">
                          <a href="xref/index.html" title="X-Ref">X-Ref</a>
            </li>
                  <li class="none">
                          <a href="book/book.html" title="Ref Guide (multi-page)">Ref Guide (multi-page)</a>
            </li>
                  <li class="none">
                          <a href="book.html" title="Ref Guide (single-page)">Ref Guide (single-page)</a>
            </li>
                  <li class="none">
                          <a href="http://abloz.com/hbase/book.html" class="externalLink" title="中文参考指南(单页)">中文参考指南(单页)</a>
            </li>
                  <li class="none">
                          <a href="book/faq.html" title="FAQ">FAQ</a>
            </li>
                  <li class="none">
                          <a href="book.html#other.info" title="Videos/Presentations">Videos/Presentations</a>
            </li>
                  <li class="none">
                          <a href="http://wiki.apache.org/hadoop/Hbase" class="externalLink" title="Wiki">Wiki</a>
            </li>
                  <li class="none">
                          <a href="acid-semantics.html" title="ACID Semantics">ACID Semantics</a>
            </li>
                  <li class="none">
                          <a href="book.html#arch.bulk.load" title="Bulk Loads">Bulk Loads</a>
            </li>
                  <li class="none">
                          <a href="metrics.html" title="Metrics">Metrics</a>
            </li>
                  <li class="none">
                          <a href="cygwin.html" title="HBase on Windows">HBase on Windows</a>
            </li>
                  <li class="none">
                          <a href="replication.html" title="Cluster replication">Cluster replication</a>
            </li>
          </ul>
                       <h5>ASF</h5>
                  <ul>
                  <li class="none">
                          <a href="http://www.apache.org/foundation/" class="externalLink" title="Apache Software Foundation">Apache Software Foundation</a>
            </li>
                  <li class="none">
                          <a href="http://www.apache.org/foundation/how-it-works.html" class="externalLink" title="How Apache Works">How Apache Works</a>
            </li>
                  <li class="none">
                          <a href="http://www.apache.org/foundation/sponsorship.html" class="externalLink" title="Sponsoring Apache">Sponsoring Apache</a>
            </li>
          </ul>
                             <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
        <img class="poweredBy" alt="Built by Maven" src="./images/logos/maven-feather.png" />
      </a>
                   
                
            </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <div class="section"><h2>RAT (Release Audit Tool) results<a name="RAT_Release_Audit_Tool_results"></a></h2><p>The following document contains the results of <a class="externalLink" href="http://incubator.apache.org/rat/apache-rat-plugin">RAT (Release Audit Tool)</a>.</p><p></p><div class="source"><pre>
*****************************************************
Summary
-------
Generated at: 2013-11-18T15:17:47-08:00
Notes: 5
Binaries: 35
Archives: 0
Standards: 1515

Apache Licensed: 1438
Generated Documents: 0

JavaDocs are generated and so license header is optional
Generated files do not required license headers

77 Unknown Licenses

*******************************

Unapproved licenses:

  .arcconfig
  conf/log4j.properties
  conf/regionservers
  conf/hadoop-metrics.properties
  .git/packed-refs
  .git/FETCH_HEAD
  .git/hooks/update.sample
  .git/hooks/post-receive.sample
  .git/hooks/applypatch-msg.sample
  .git/hooks/pre-commit.sample
  .git/hooks/prepare-commit-msg.sample
  .git/hooks/post-commit.sample
  .git/hooks/pre-applypatch.sample
  .git/hooks/post-update.sample
  .git/hooks/pre-rebase.sample
  .git/hooks/commit-msg.sample
  .git/logs/refs/remotes/origin/cdh5-0.96.0
  .git/logs/refs/heads/master
  .git/logs/HEAD
  .git/config
  .git/refs/remotes/origin/cdh5-0.96.0
  .git/refs/remotes/origin/HEAD
  .git/refs/heads/master
  .git/refs/tags/jenkins-Impala-HBase-dependency-21
  .git/refs/tags/jenkins-Impala-HBase-dependency-15
  .git/refs/tags/jenkins-Impala-HBase-dependency-19
  .git/refs/tags/jenkins-Impala-HBase-dependency-17
  .git/refs/tags/jenkins-Impala-HBase-dependency-18
  .git/refs/tags/jenkins-Impala-HBase-dependency-20
  .git/refs/tags/jenkins-Impala-HBase-dependency-16
  .git/info/exclude
  .git/HEAD
  .git/description
  .gitignore
  CHANGES.txt
  src/site/resources/css/freebsd_docbook.css
  src/site/resources/images/hbase_logo.svg
  src/site/resources/images/big_h_logo.svg
  src/packages/deb/hbase.control/conffile
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellSetMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/StorageClusterStatusMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ScannerMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/VersionMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableInfoMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableListMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ColumnSchemaMessage.java
  src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableSchemaMessage.java
  src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java
  src/main/java/org/apache/hadoop/hbase/protobuf/generated/ErrorHandlingProtos.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/TCell.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/TScan.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/TIncrement.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/TRowResult.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/IllegalArgument.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/ColumnDescriptor.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/IOError.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/Mutation.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/TColumn.java
  src/main/java/org/apache/hadoop/hbase/thrift/generated/AlreadyExists.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TResult.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TScan.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TPut.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIllegalArgument.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnValue.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIOError.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDelete.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnIncrement.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TGet.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDeleteType.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumn.java
  src/main/java/org/apache/hadoop/hbase/thrift2/generated/TTimeRange.java

*******************************

Archives:

*****************************************************
  Files with Apache License headers will be marked AL
  Binary files (which do not require AL headers) will be marked B
  Compressed archives will be marked A
  Notices, licenses etc will be marked N
 !????? .arcconfig
  AL    bin/get-active-master.rb
  AL    bin/replication/copy_tables_desc.rb
  AL    bin/graceful_stop.sh
  AL    bin/hbase
  AL    bin/start-hbase.sh
  AL    bin/hbase-daemons.sh
  AL    bin/region_mover.rb
  AL    bin/stop-hbase.sh
  AL    bin/hbase-config.sh
  AL    bin/region_status.rb
  AL    bin/master-backup.sh
  AL    bin/hbase-daemon.sh
  AL    bin/regionservers.sh
  AL    bin/hbase-jruby
  AL    bin/local-master-backup.sh
  AL    bin/local-regionservers.sh
  AL    bin/zookeepers.sh
  AL    bin/hirb.rb
  AL    bin/rolling-restart.sh
  AL    conf/hbase-policy.xml
 !????? conf/log4j.properties
 !????? conf/regionservers
  AL    conf/hbase-site.xml
 !????? conf/hadoop-metrics.properties
  AL    conf/hbase-env.sh
 !????? .git/packed-refs
 !????? .git/FETCH_HEAD
 !????? .git/hooks/update.sample
 !????? .git/hooks/post-receive.sample
 !????? .git/hooks/applypatch-msg.sample
 !????? .git/hooks/pre-commit.sample
 !????? .git/hooks/prepare-commit-msg.sample
 !????? .git/hooks/post-commit.sample
 !????? .git/hooks/pre-applypatch.sample
 !????? .git/hooks/post-update.sample
 !????? .git/hooks/pre-rebase.sample
 !????? .git/hooks/commit-msg.sample
 !????? .git/logs/refs/remotes/origin/cdh5-0.96.0
 !????? .git/logs/refs/heads/master
 !????? .git/logs/HEAD
 !????? .git/config
 !????? .git/refs/remotes/origin/cdh5-0.96.0
 !????? .git/refs/remotes/origin/HEAD
 !????? .git/refs/heads/master
 !????? .git/refs/tags/jenkins-Impala-HBase-dependency-21
 !????? .git/refs/tags/jenkins-Impala-HBase-dependency-15
 !????? .git/refs/tags/jenkins-Impala-HBase-dependency-19
 !????? .git/refs/tags/jenkins-Impala-HBase-dependency-17
 !????? .git/refs/tags/jenkins-Impala-HBase-dependency-18
 !????? .git/refs/tags/jenkins-Impala-HBase-dependency-20
 !????? .git/refs/tags/jenkins-Impala-HBase-dependency-16
  B     .git/objects/64/2ebbefb22ddf4fd90813510bb38f37df122b53
  B     .git/objects/c9/495ce4a8be0c2617207dbcc0decec2e4cb8a07
  B     .git/objects/cb/c3fca248b7b69c8e4392b04ef3f538d5f59038
  B     .git/objects/ec/31fdaaca38762369fc0003ceb741035f801b50
  B     .git/objects/54/1265b97a5421ce1d1d13e5e6628f55b96fa357
  B     .git/objects/a2/801db1700bb641a251a050d2be4437bbb76b97
  B     .git/objects/09/14eab2da227c6d215d088c0e77825248e469c5
  B     .git/objects/12/dbfea1224f801bb26605c4050d9eb5e9020733
  B     .git/objects/cc/d69918f6a2d7c6122e95414b94c52fb07fed72
  B     .git/objects/2c/40f8da5c0be90ff0c902181c159a264186c284
  B     .git/objects/d4/1c6601edee085d55f15ecbb691c8401e7e08a0
  B     .git/objects/1d/9df4caad4d1d045e0a165a6956e860cf9f365d
  B     .git/objects/36/34426b3cf8ae68648b2dbe97d3c8df8ebe37d1
  B     .git/objects/87/4afbbd0e94ffe7cd53ec76c6e4c48edd0b946e
  B     .git/objects/c6/7f1d9f1ce3a32a868b24bef67202d25fab3623
  B     .git/objects/ef/c5bc7b3eb54a6d08a4c19d37f76cbc4c9a1559
  B     .git/objects/06/6e1ec360772dfec0169ce64e0227d573a33ae0
  B     .git/objects/66/7f19b7b53039307e466e3ae2d096aa0914d4dc
  B     .git/objects/pack/pack-3b004effa918d118c6be61c6f9bc7cbdcf9c48a2.pack
  B     .git/objects/pack/pack-3b004effa918d118c6be61c6f9bc7cbdcf9c48a2.idx
  B     .git/objects/cf/88b2fa9819754a25a1f463f28e398bba837d96
  B     .git/index
 !????? .git/info/exclude
 !????? .git/HEAD
 !????? .git/description
  N     LICENSE.txt
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/TablePermission.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadProtocol.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/UserPermission.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlFilter.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/AuthResult.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/AccessControllerProtocol.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/Permission.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/HBasePolicyProvider.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcServer.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/AccessDeniedException.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/TokenProvider.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSelector.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenIdentifier.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationProtocol.java
  AL    security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationKey.java
  AL    security/src/main/java/org/apache/hadoop/hbase/ipc/SecureConnectionHeader.java
  AL    security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java
  AL    security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java
  AL    security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java
  AL    security/src/test/resources/hbase-site.xml
  AL    security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFilesSplitRecovery.java
  AL    security/src/test/java/org/apache/hadoop/hbase/mapreduce/TestSecureLoadIncrementalHFiles.java
  AL    security/src/test/java/org/apache/hadoop/hbase/security/access/TestZKPermissionsWatcher.java
  AL    security/src/test/java/org/apache/hadoop/hbase/security/access/TestTablePermissions.java
  AL    security/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
  AL    security/src/test/java/org/apache/hadoop/hbase/security/access/SecureTestUtil.java
  AL    security/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessControlFilter.java
  AL    security/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java
  AL    security/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java
 !????? .gitignore
  AL    dev-support/hbasetests.sh
  AL    dev-support/test-patch.properties
  AL    dev-support/findHangingTest.sh
  AL    dev-support/smart-apply-patch.sh
  AL    dev-support/test-util.sh
  AL    dev-support/test-patch.sh
  AL    pom.xml
 !????? CHANGES.txt
  N     NOTICE.txt
  N     README.txt
  AL    src/saveVersion.sh
  AL    src/examples/healthcheck/healthcheck.sh
  AL    src/examples/mapreduce/org/apache/hadoop/hbase/mapreduce/IndexBuilder.java
  AL    src/examples/mapreduce/org/apache/hadoop/hbase/mapreduce/SampleUploader.java
  AL    src/examples/mapreduce/index-builder-setup.rb
  AL    src/examples/thrift/DemoClient.java
  AL    src/examples/thrift/DemoClient.py
  AL    src/examples/thrift/DemoClient.rb
  AL    src/examples/thrift/DemoClient.cpp
  AL    src/examples/thrift/DemoClient.pl
  N     src/examples/thrift/README.txt
  AL    src/examples/thrift/DemoClient.php
  AL    src/examples/thrift/Makefile
  AL    src/examples/thrift2/DemoClient.java
  AL    src/examples/thrift2/DemoClient.py
  N     src/examples/README.txt
  AL    src/site/resources/css/site.css
 !????? src/site/resources/css/freebsd_docbook.css
  AL    src/site/resources/doap_Hbase.rdf
  B     src/site/resources/images/favicon.ico
 !????? src/site/resources/images/hbase_logo.svg
  B     src/site/resources/images/hadoop-logo.jpg
  B     src/site/resources/images/hfilev2.png
  B     src/site/resources/images/hfile.png
 !????? src/site/resources/images/big_h_logo.svg
  B     src/site/resources/images/architecture.gif
  B     src/site/resources/images/big_h_logo.png
  B     src/site/resources/images/hbase_logo.png
  B     src/site/resources/images/replication_overview.png
  AL    src/site/site.vm
  AL    src/site/xdoc/bulk-loads.xml
  AL    src/site/xdoc/sponsors.xml
  AL    src/site/xdoc/index.xml
  AL    src/site/xdoc/pseudo-distributed.xml
  AL    src/site/xdoc/cygwin.xml
  AL    src/site/xdoc/metrics.xml
  AL    src/site/xdoc/resources.xml
  AL    src/site/xdoc/replication.xml
  AL    src/site/xdoc/acid-semantics.xml
  AL    src/site/xdoc/old_news.xml
  AL    src/site/site.xml
  AL    src/assembly/all.xml
  AL    src/packages/update-hbase-env.sh
  AL    src/packages/deb/hbase.control/preinst
  AL    src/packages/deb/hbase.control/prerm
  AL    src/packages/deb/hbase.control/postinst
  AL    src/packages/deb/hbase.control/control
 !????? src/packages/deb/hbase.control/conffile
  AL    src/packages/deb/hbase.control/postrm
  AL    src/packages/deb/conf-pseudo.control/prerm
  AL    src/packages/deb/conf-pseudo.control/postinst
  AL    src/packages/deb/conf-pseudo.control/control
  AL    src/packages/deb/conf-pseudo.control/conffile
  AL    src/packages/deb/init.d/hbase-master
  AL    src/packages/deb/init.d/hbase-regionserver
  AL    src/packages/conf-pseudo/hbase-site.xml
  AL    src/packages/rpm/spec/conf-pseudo.spec
  AL    src/packages/rpm/spec/hbase.spec
  AL    src/packages/rpm/init.d/hbase-master
  AL    src/packages/rpm/init.d/hbase-regionserver
  AL    src/packages/build.xml
  AL    src/main/ruby/irb/hirb.rb
  AL    src/main/ruby/hbase.rb
  AL    src/main/ruby/hbase/admin.rb
  AL    src/main/ruby/hbase/hbase.rb
  AL    src/main/ruby/hbase/replication_admin.rb
  AL    src/main/ruby/hbase/security.rb
  AL    src/main/ruby/hbase/table.rb
  AL    src/main/ruby/shell.rb
  AL    src/main/ruby/shell/commands.rb
  AL    src/main/ruby/shell/formatter.rb
  AL    src/main/ruby/shell/commands/exists.rb
  AL    src/main/ruby/shell/commands/snapshot.rb
  AL    src/main/ruby/shell/commands/zk_dump.rb
  AL    src/main/ruby/shell/commands/restore_snapshot.rb
  AL    src/main/ruby/shell/commands/clone_snapshot.rb
  AL    src/main/ruby/shell/commands/compact.rb
  AL    src/main/ruby/shell/commands/unassign.rb
  AL    src/main/ruby/shell/commands/hlog_roll.rb
  AL    src/main/ruby/shell/commands/disable_all.rb
  AL    src/main/ruby/shell/commands/start_replication.rb
  AL    src/main/ruby/shell/commands/balancer.rb
  AL    src/main/ruby/shell/commands/status.rb
  AL    src/main/ruby/shell/commands/disable.rb
  AL    src/main/ruby/shell/commands/drop_all.rb
  AL    src/main/ruby/shell/commands/show_filters.rb
  AL    src/main/ruby/shell/commands/get_table.rb
  AL    src/main/ruby/shell/commands/create.rb
  AL    src/main/ruby/shell/commands/alter_async.rb
  AL    src/main/ruby/shell/commands/is_enabled.rb
  AL    src/main/ruby/shell/commands/table_help.rb
  AL    src/main/ruby/shell/commands/incr.rb
  AL    src/main/ruby/shell/commands/is_disabled.rb
  AL    src/main/ruby/shell/commands/alter.rb
  AL    src/main/ruby/shell/commands/disable_peer.rb
  AL    src/main/ruby/shell/commands/remove_peer.rb
  AL    src/main/ruby/shell/commands/grant.rb
  AL    src/main/ruby/shell/commands/truncate.rb
  AL    src/main/ruby/shell/commands/split.rb
  AL    src/main/ruby/shell/commands/put.rb
  AL    src/main/ruby/shell/commands/stop_replication.rb
  AL    src/main/ruby/shell/commands/version.rb
  AL    src/main/ruby/shell/commands/delete_snapshot.rb
  AL    src/main/ruby/shell/commands/user_permission.rb
  AL    src/main/ruby/shell/commands/deleteall.rb
  AL    src/main/ruby/shell/commands/get.rb
  AL    src/main/ruby/shell/commands/delete.rb
  AL    src/main/ruby/shell/commands/revoke.rb
  AL    src/main/ruby/shell/commands/describe.rb
  AL    src/main/ruby/shell/commands/scan.rb
  AL    src/main/ruby/shell/commands/enable_all.rb
  AL    src/main/ruby/shell/commands/list_peers.rb
  AL    src/main/ruby/shell/commands/major_compact.rb
  AL    src/main/ruby/shell/commands/add_peer.rb
  AL    src/main/ruby/shell/commands/count.rb
  AL    src/main/ruby/shell/commands/enable.rb
  AL    src/main/ruby/shell/commands/assign.rb
  AL    src/main/ruby/shell/commands/get_counter.rb
  AL    src/main/ruby/shell/commands/list_snapshots.rb
  AL    src/main/ruby/shell/commands/enable_peer.rb
  AL    src/main/ruby/shell/commands/alter_status.rb
  AL    src/main/ruby/shell/commands/drop.rb
  AL    src/main/ruby/shell/commands/balance_switch.rb
  AL    src/main/ruby/shell/commands/list.rb
  AL    src/main/ruby/shell/commands/move.rb
  AL    src/main/ruby/shell/commands/whoami.rb
  AL    src/main/ruby/shell/commands/flush.rb
  AL    src/main/ruby/shell/commands/close_region.rb
  AL    src/main/resources/hbase-default.xml
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/VersionMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/TableListMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/ScannerMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/StorageClusterStatusMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/TableSchemaMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/ColumnSchemaMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/TableInfoMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/CellSetMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/protobuf/CellMessage.proto
  AL    src/main/resources/org/apache/hadoop/hbase/rest/XMLSchema.xsd
  AL    src/main/resources/org/apache/hadoop/hbase/mapreduce/RowCounter_Counters.properties
  AL    src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift
  AL    src/main/resources/org/apache/hadoop/hbase/thrift2/hbase.thrift
  AL    src/main/resources/org/apache/hadoop/hbase/mapred/RowCounter_Counters.properties
  AL    src/main/resources/hbase-webapps/rest/index.html
  AL    src/main/resources/hbase-webapps/rest/rest.jsp
  AL    src/main/resources/hbase-webapps/static/hbase.css
  B     src/main/resources/hbase-webapps/static/favicon.ico
  B     src/main/resources/hbase-webapps/static/hbase_logo.png
  B     src/main/resources/hbase-webapps/static/hbase_logo_med.gif
  AL    src/main/resources/hbase-webapps/regionserver/index.html
  AL    src/main/resources/hbase-webapps/regionserver/regionserver.jsp
  AL    src/main/resources/hbase-webapps/thrift/index.html
  AL    src/main/resources/hbase-webapps/thrift/thrift.jsp
  AL    src/main/resources/hbase-webapps/master/index.html
  AL    src/main/resources/hbase-webapps/master/snapshot.jsp
  AL    src/main/resources/hbase-webapps/master/zk.jsp
  AL    src/main/resources/hbase-webapps/master/table.jsp
  AL    src/main/resources/hbase-webapps/master/master.jsp
  AL    src/main/resources/hbase-webapps/master/tablesDetailed.jsp
  AL    src/main/protobuf/ErrorHandling.proto
  AL    src/main/protobuf/hbase.proto
  AL    src/main/python/hbase/merge_conf.py
  AL    src/main/native/CMakeLists.txt
  AL    src/main/native/src/mlockall_agent/mlockall_agent.c
  AL    src/main/java/org/apache/hadoop/hbase/RegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTableReadOnly.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/HQuorumPeer.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/RootRegionTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/MetaNodeTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ClusterId.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperListener.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/DrainingServerTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ClusterStatusTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKServerTool.java
  AL    src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java
  AL    src/main/java/org/apache/hadoop/hbase/MasterAddressTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ResourceConfig.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ResultGenerator.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/filter/GZIPRequestWrapper.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/filter/GZIPResponseWrapper.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/filter/GZIPRequestStream.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/filter/GzipFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/filter/GZIPResponseStream.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/consumer/ProtobufMessageBodyConsumer.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/producer/PlainTextMessageBodyProducer.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/producer/ProtobufMessageBodyProducer.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/provider/JAXBContextResolver.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/StorageClusterVersionResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/Response.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/RemoteAdmin.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/Client.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/client/Cluster.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableSchemaModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/ScannerModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableInfoModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/ColumnSchemaModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/VersionModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/CellModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableRegionModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterVersionModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableListModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/TableModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/StorageClusterStatusModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/RowModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/model/CellSetModel.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ResourceBase.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/metrics/RESTStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/metrics/RESTMetrics.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellSetMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/StorageClusterStatusMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ScannerMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/VersionMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableInfoMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableListMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ColumnSchemaMessage.java
 !????? src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableSchemaMessage.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/package.html
  AL    src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/VersionResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ExistsResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/Constants.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/Main.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/ProtobufMessageHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/TableResource.java
  AL    src/main/java/org/apache/hadoop/hbase/rest/RootResource.java
  AL    src/main/java/org/apache/hadoop/hbase/RemoteExceptionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/TableNotDisabledException.java
  AL    src/main/java/org/apache/hadoop/hbase/NotServingRegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/tool/Canary.java
  AL    src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSinkMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeer.java
  AL    src/main/java/org/apache/hadoop/hbase/InvalidFamilyOperationException.java
  AL    src/main/java/org/apache/hadoop/hbase/HealthReport.java
  AL    src/main/java/org/apache/hadoop/hbase/io/WritableWithSize.java
  AL    src/main/java/org/apache/hadoop/hbase/io/DataOutputOutputStream.java
  AL    src/main/java/org/apache/hadoop/hbase/io/Reference.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HbaseMapWritable.java
  AL    src/main/java/org/apache/hadoop/hbase/io/FileLink.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheColumnFamilySummary.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CachedBlock.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/SimpleBlockCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCacheKey.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/ChecksumUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/slab/Slab.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/slab/SlabItemActionWatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/slab/SlabCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/slab/SingleSizeCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/InvalidHFileException.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CacheStats.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/InlineBlockWriter.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/Cacheable.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CacheableDeserializer.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BlockWithScanInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CachedBlockQueue.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileWriter.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BoundedRangeFileInputStream.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/DoubleBlockCache.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/CorruptHFileException.java
  AL    src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java
  AL    src/main/java/org/apache/hadoop/hbase/io/CodeToClassAndBack.java
  AL    src/main/java/org/apache/hadoop/hbase/io/TimeRange.java
  AL    src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java
  AL    src/main/java/org/apache/hadoop/hbase/io/DoubleOutputStream.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HLogLink.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HFileLink.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HeapSize.java
  AL    src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/EncoderBufferTooSmallException.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/CompressionState.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java
  AL    src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
  AL    src/main/java/org/apache/hadoop/hbase/BaseConfigurable.java
  AL    src/main/java/org/apache/hadoop/hbase/catalog/RootLocationEditor.java
  AL    src/main/java/org/apache/hadoop/hbase/catalog/MetaMigrationRemovingHTD.java
  AL    src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
  AL    src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java
  AL    src/main/java/org/apache/hadoop/hbase/ClockOutOfSyncException.java
  AL    src/main/java/org/apache/hadoop/hbase/ZooKeeperConnectionException.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SubstringComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FilterList.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FilterBase.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/PageFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ParseFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/RandomRowFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ValueFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ColumnRangeFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/Filter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/WritableByteArrayComparable.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/SingleColumnValueExcludeFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ColumnCountGetFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/BitComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/CompareFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ParseConstants.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/QualifierFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/ColumnPaginationFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/TimestampsFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/RowFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FamilyFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/BinaryComparator.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/InclusiveStopFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/InvalidRowFilterException.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/filter/IncompatibleFilterException.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/Driver.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/TotalOrderPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/HRegionPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/Export.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/MultithreadedTableMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/SimpleTotalOrderPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/IdentityTableReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/HLogInputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/WALPlayer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableOutputCommitter.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReader.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java
  AL    src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java
  AL    src/main/java/org/apache/hadoop/hbase/executor/RegionTransitionData.java
  AL    src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
  AL    src/main/java/org/apache/hadoop/hbase/migration/HRegionInfo090x.java
  AL    src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java
  AL    src/main/java/org/apache/hadoop/hbase/KeyValue.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Get.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Increment.java
  AL    src/main/java/org/apache/hadoop/hbase/client/IsolationLevel.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HConnection.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RowLock.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
  AL    src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationAdmin.java
  AL    src/main/java/org/apache/hadoop/hbase/client/UnmodifyableHRegionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RowMutations.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MultiPutResponse.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Put.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RegionOfflineException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Result.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MultiAction.java
  AL    src/main/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MultiResponse.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTableInterfaceFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Delete.java
  AL    src/main/java/org/apache/hadoop/hbase/client/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ScannerTimeoutException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Append.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Scan.java
  AL    src/main/java/org/apache/hadoop/hbase/client/metrics/ScanMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Row.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTableUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Operation.java
  AL    src/main/java/org/apache/hadoop/hbase/client/NoServerForRegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/coprocessor/LongColumnInterpreter.java
  AL    src/main/java/org/apache/hadoop/hbase/client/coprocessor/ExecResult.java
  AL    src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java
  AL    src/main/java/org/apache/hadoop/hbase/client/coprocessor/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/client/coprocessor/Batch.java
  AL    src/main/java/org/apache/hadoop/hbase/client/coprocessor/AggregationClient.java
  AL    src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java
  AL    src/main/java/org/apache/hadoop/hbase/client/OperationWithAttributes.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTable.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTableFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Action.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ResultScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/client/AbstractClientScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/client/MultiPut.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Attributes.java
  AL    src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java
  AL    src/main/java/org/apache/hadoop/hbase/client/Mutation.java
  AL    src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java
  AL    src/main/java/org/apache/hadoop/hbase/constraint/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/constraint/Constraints.java
  AL    src/main/java/org/apache/hadoop/hbase/constraint/BaseConstraint.java
  AL    src/main/java/org/apache/hadoop/hbase/constraint/Constraint.java
  AL    src/main/java/org/apache/hadoop/hbase/constraint/ConstraintException.java
  AL    src/main/java/org/apache/hadoop/hbase/constraint/ConstraintProcessor.java
  AL    src/main/java/org/apache/hadoop/hbase/TableNotEnabledException.java
  AL    src/main/java/org/apache/hadoop/hbase/UnknownScannerException.java
  AL    src/main/java/org/apache/hadoop/hbase/ServerName.java
  AL    src/main/java/org/apache/hadoop/hbase/Coprocessor.java
  AL    src/main/java/org/apache/hadoop/hbase/ClusterStatus.java
  AL    src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java
  AL    src/main/java/org/apache/hadoop/hbase/NotAllMetaRegionsOnlineException.java
  AL    src/main/java/org/apache/hadoop/hbase/security/KerberosInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/security/TokenInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/security/User.java
  AL    src/main/java/org/apache/hadoop/hbase/Server.java
  AL    src/main/java/org/apache/hadoop/hbase/HealthCheckChore.java
  AL    src/main/java/org/apache/hadoop/hbase/DoNotRetryIOException.java
  AL    src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java
  AL    src/main/java/org/apache/hadoop/hbase/MasterNotRunningException.java
  AL    src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionOpeningState.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseMetaHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRootHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenMetaHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRootHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/Store.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MultiVersionConsistencyControl.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/NoSuchColumnFamilyException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitPolicy.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerAccounting.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegionThriftServer.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RSStatusServlet.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationSinkService.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/OperationStatus.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/CompactionTool.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MXBeanImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/FlushRequester.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionAlreadyInTransitionException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/LruHashMap.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/SplitRequest.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/InternalScan.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MXBean.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueSkipListSet.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/LeaseListener.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/OperationMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionMetricsStorage.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/NonLazyKeyValueScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerStoppedException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/OnlineRegions.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerRunningException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreLAB.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationService.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/WrongRegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationSourceService.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/LeaseException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFileSystem.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/Dictionary.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/LRUDictionary.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALActionsListener.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/FailedLogCloseException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogPrettyPrinter.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/OrphanHLogAfterSplitException.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/CompressionContext.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/KeyValueCompression.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/GetClosestRowBeforeTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/DebugPrint.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/Leases.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ChangedReadersObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ScanType.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionProgress.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactSelection.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java
  AL    src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/HBaseInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/PersistentMetricsTimeVaryingRate.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/MetricsString.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/file/TimeStampingFileContext.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/MetricsRate.java
  AL    src/main/java/org/apache/hadoop/hbase/metrics/ExactCounterMetric.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/Subprocedure.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMemberRpcs.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/SubprocedureFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/ProcedureMember.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinatorRpcs.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/ProcedureCoordinator.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/ZKProcedureCoordinatorRpcs.java
  AL    src/main/java/org/apache/hadoop/hbase/procedure/Procedure.java
 !????? src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java
 !????? src/main/java/org/apache/hadoop/hbase/protobuf/generated/ErrorHandlingProtos.java
  AL    src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/HBaseFileSystem.java
  AL    src/main/java/org/apache/hadoop/hbase/UnknownRegionException.java
  AL    src/main/java/org/apache/hadoop/hbase/TableDescriptors.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Merge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/RegionSplitCalculator.java
  AL    src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Bytes.java
  AL    src/main/java/org/apache/hadoop/hbase/util/BloomFilterFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/util/AbstractHBaseTool.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ChecksumFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Objects.java
  AL    src/main/java/org/apache/hadoop/hbase/util/JvmVersion.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CoprocessorClassLoader.java
  AL    src/main/java/org/apache/hadoop/hbase/util/IdLock.java
  AL    src/main/java/org/apache/hadoop/hbase/util/GetJavaProperty.java
  AL    src/main/java/org/apache/hadoop/hbase/util/MD5Hash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Threads.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CompoundBloomFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/PairOfSameType.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ProtoUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/util/IncrementingEnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ManualEnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CollectionBackedScanner.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ServerCommandLine.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Pair.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java
  AL    src/main/java/org/apache/hadoop/hbase/util/RetryCounterFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/util/HashedBytes.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Base64.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Writables.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CompoundBloomFilterWriter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/KeyRange.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/HasThread.java
  AL    src/main/java/org/apache/hadoop/hbase/util/BloomFilterBase.java
  AL    src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FSVisitor.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Keying.java
  AL    src/main/java/org/apache/hadoop/hbase/util/PoolMap.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ByteBufferOutputStream.java
  AL    src/main/java/org/apache/hadoop/hbase/util/JenkinsHash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Sleeper.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ModifyRegionUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/HMerge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/BloomFilter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ClassSize.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ShutdownHookManager.java
  AL    src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRepair.java
  AL    src/main/java/org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.java
  AL    src/main/java/org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandlerImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/util/HBaseConfTool.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/MurmurHash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/DirectMemoryUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/SoftValueSortedMap.java
  AL    src/main/java/org/apache/hadoop/hbase/util/SortedCopyOnWriteSet.java
  AL    src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ClassLoaderBase.java
  AL    src/main/java/org/apache/hadoop/hbase/util/SizeBasedThrottler.java
  AL    src/main/java/org/apache/hadoop/hbase/util/VersionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/util/DynamicClassLoader.java
  AL    src/main/java/org/apache/hadoop/hbase/util/DefaultEnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/ChecksumType.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Classes.java
  AL    src/main/java/org/apache/hadoop/hbase/util/InfoServer.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Strings.java
  AL    src/main/java/org/apache/hadoop/hbase/util/RetryCounter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CancelableProgressable.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Methods.java
  AL    src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdge.java
  AL    src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FSMapRUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Hash.java
  AL    src/main/java/org/apache/hadoop/hbase/util/Addressing.java
  AL    src/main/java/org/apache/hadoop/hbase/util/HFileArchiveUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/util/BloomFilterWriter.java
  AL    src/main/java/org/apache/hadoop/hbase/util/FileSystemVersionException.java
  AL    src/main/java/org/apache/hadoop/hbase/util/CompoundBloomFilterBase.java
  AL    src/main/java/org/apache/hadoop/hbase/Abortable.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateProtocol.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/example/ZooKeeperScanPolicyObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteResponse.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteProtocol.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/RegionCoprocessorEnvironment.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorException.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/WALObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/MasterCoprocessorEnvironment.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/BaseEndpointCoprocessor.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/RegionServerCoprocessorEnvironment.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/WALCoprocessorEnvironment.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/ColumnInterpreter.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/BaseMasterObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/AggregateImplementation.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/MultiRowMutationProtocol.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/ObserverContext.java
  AL    src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java
  AL    src/main/java/org/apache/hadoop/hbase/HRegionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/MemoryBoundedLogMessageBuffer.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/ThreadMonitoring.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/LogMonitoring.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredRPCHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/MonitoredTask.java
  AL    src/main/java/org/apache/hadoop/hbase/monitoring/StateDumpServlet.java
  AL    src/main/java/org/apache/hadoop/hbase/HServerInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/Chore.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceServerWALsTask.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshotException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotExistsException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/TakeSnapshotUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDoesNotExistException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/ReferenceRegionHFilesTask.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotTask.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/CorruptedSnapshotException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/TablePartiallyOpenException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/CopyRecoveredEditsTask.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/TableInfoCopyTask.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/RestoreSnapshotException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/UnknownSnapshotException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/HBaseSnapshotException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotLogSplitter.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/SnapshotCreationException.java
  AL    src/main/java/org/apache/hadoop/hbase/snapshot/HSnapshotDescription.java
  AL    src/main/java/org/apache/hadoop/hbase/YouAreDeadException.java
  AL    src/main/java/org/apache/hadoop/hbase/HConstants.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/HbaseHandlerMetricsProxy.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/IncrementCoalescer.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/CallQueue.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/IncrementCoalescerMBean.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/TCell.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/TScan.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/TIncrement.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/TRowResult.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/IllegalArgument.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/ColumnDescriptor.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/IOError.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/Mutation.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/TColumn.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift/generated/AlreadyExists.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/ThriftMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/HThreadedSelectorServerArgs.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
  AL    src/main/java/org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector.java
  AL    src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignException.java
  AL    src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/errorhandling/TimeoutException.java
  AL    src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionSnare.java
  AL    src/main/java/org/apache/hadoop/hbase/errorhandling/ForeignExceptionListener.java
  AL    src/main/java/org/apache/hadoop/hbase/master/DeadServer.java
  AL    src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/SplitRegionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/MetaServerShutdownHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/OpenedRegionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/ClosedRegionHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/handler/TotesHRegionInfo.java
  AL    src/main/java/org/apache/hadoop/hbase/master/LoadBalancerFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ServerAndLoad.java
  AL    src/main/java/org/apache/hadoop/hbase/master/SnapshotSentinel.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
  AL    src/main/java/org/apache/hadoop/hbase/master/BulkReOpen.java
  AL    src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MXBeanImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MXBean.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java
  AL    src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/BaseFileCleanerDelegate.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/TimeToLiveLogCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/BaseLogCleanerDelegate.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/FileCleanerDelegate.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/TimeToLiveHFileCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/LogCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/BaseHFileCleanerDelegate.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java
  AL    src/main/java/org/apache/hadoop/hbase/master/AssignCallable.java
  AL    src/main/java/org/apache/hadoop/hbase/master/metrics/MasterMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/master/metrics/MasterStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
  AL    src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java
  AL    src/main/java/org/apache/hadoop/hbase/master/BulkAssigner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/HMaster.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/RestoreSnapshotHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotLogCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotHFileCleaner.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.java
  AL    src/main/java/org/apache/hadoop/hbase/master/snapshot/EnabledTableSnapshotHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java
  AL    src/main/java/org/apache/hadoop/hbase/master/UnAssignCallable.java
  AL    src/main/java/org/apache/hadoop/hbase/master/RegionPlan.java
  AL    src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java
  AL    src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
  AL    src/main/java/org/apache/hadoop/hbase/PleaseHoldException.java
  AL    src/main/java/org/apache/hadoop/hbase/EmptyWatcher.java
  AL    src/main/java/org/apache/hadoop/hbase/TableNotFoundException.java
  AL    src/main/java/org/apache/hadoop/hbase/DroppedSnapshotException.java
  AL    src/main/java/org/apache/hadoop/hbase/TableExistsException.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TResult.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TScan.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TPut.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIllegalArgument.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnValue.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIOError.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDelete.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnIncrement.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TGet.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDeleteType.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumn.java
 !????? src/main/java/org/apache/hadoop/hbase/thrift2/generated/TTimeRange.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift2/package.html
  AL    src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java
  AL    src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/VersionAnnotation.java
  AL    src/main/java/org/apache/hadoop/hbase/CoprocessorEnvironment.java
  AL    src/main/java/org/apache/hadoop/hbase/HBaseIOException.java
  AL    src/main/java/org/apache/hadoop/hbase/Stoppable.java
  AL    src/main/java/org/apache/hadoop/hbase/RegionTooBusyException.java
  AL    src/main/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
  AL    src/main/java/org/apache/hadoop/hbase/TableInfoMissingException.java
  AL    src/main/java/org/apache/hadoop/hbase/HServerLoad.java
  AL    src/main/java/org/apache/hadoop/hbase/HServerAddress.java
  AL    src/main/java/org/apache/hadoop/hbase/DaemonThreadFactory.java
  AL    src/main/java/org/apache/hadoop/hbase/HRegionLocation.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableSplit.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/Driver.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/HRegionPartitioner.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableMap.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/IdentityTableReduce.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/package-info.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormat.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableReduce.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReaderImpl.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/IdentityTableMap.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableRecordReader.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/RowCounter.java
  AL    src/main/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java
  AL    src/main/java/org/apache/hadoop/hbase/HealthChecker.java
  AL    src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java
  AL    src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPCStatistics.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/ProtocolSignature.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/RpcCallContext.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/CoprocessorProtocol.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/RpcEngine.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/VersionedProtocol.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/ResponseFlag.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/ExecRPCInvoker.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/ServerNotRunningYetException.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/Status.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/MasterExecRPCInvoker.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/CallerDisconnectedException.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/Delayable.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPCErrorHandler.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/ConnectionHeader.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/HMasterRegionInterface.java
  AL    src/main/java/org/apache/hadoop/hbase/ipc/RequestContext.java
  AL    src/main/javadoc/org/apache/hadoop/hbase/replication/package.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/io/hfile/package.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/index.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/style.css
  AL    src/main/javadoc/org/apache/hadoop/hbase/thrift/doc-files/Hbase.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/thrift/package.html
  AL    src/main/javadoc/org/apache/hadoop/hbase/ipc/package.html
  AL    src/main/javadoc/overview.html
  AL    src/main/xslt/configuration_to_docbook_section.xsl
  AL    src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon
  AL    src/main/jamon/org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.jamon
  AL    src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
  AL    src/main/jamon/org/apache/hadoop/hbase/tmpl/master/AssignmentManagerStatusTmpl.jamon
  AL    src/docbkx/upgrading.xml
  AL    src/docbkx/book.xml
  AL    src/docbkx/troubleshooting.xml
  AL    src/docbkx/shell.xml
  AL    src/docbkx/community.xml
  AL    src/docbkx/performance.xml
  AL    src/docbkx/developer.xml
  AL    src/docbkx/getting_started.xml
  AL    src/docbkx/customization.xsl
  AL    src/docbkx/preface.xml
  AL    src/docbkx/zookeeper.xml
  AL    src/docbkx/security.xml
  AL    src/docbkx/configuration.xml
  AL    src/docbkx/case_studies.xml
  AL    src/docbkx/ops_mgt.xml
  AL    src/docbkx/external_apis.xml
  AL    src/test/ruby/hbase/admin_test.rb
  AL    src/test/ruby/hbase/table_test.rb
  AL    src/test/ruby/hbase/hbase_test.rb
  AL    src/test/ruby/tests_runner.rb
  AL    src/test/ruby/test_helper.rb
  AL    src/test/ruby/shell/commands_test.rb
  AL    src/test/ruby/shell/formatter_test.rb
  AL    src/test/ruby/shell/shell_test.rb
  B     src/test/resources/org/apache/hadoop/hbase/io/hfile/8e8ab58dcf39412da19833fcd8f687ac
  AL    src/test/resources/org/apache/hadoop/hbase/PerformanceEvaluation_Counter.properties
  AL    src/test/resources/log4j.properties
  AL    src/test/resources/mapred-queues.xml
  AL    src/test/resources/hdfs-site.xml
  AL    src/test/resources/hbase-site.xml
  B     src/test/data/hbase-4388-root.dir.tgz
  AL    src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestHQuorumPeer.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperACL.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestRecoverableZooKeeper.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKTable.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperMainServerArg.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestZooKeeperNodeTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKTableReadOnly.java
  AL    src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKMulti.java
  AL    src/test/java/org/apache/hadoop/hbase/LargeTests.java
  AL    src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestRowResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestScannerResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestSchemaResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/PerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestTableResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestScannersWithFilters.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteAdmin.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestRowModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableRegionModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestVersionModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableInfoModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableListModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestTableSchemaModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestStorageClusterVersionModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestCellModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestColumnSchemaModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestCellSetModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestScannerModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/model/TestStorageClusterStatusModel.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestGzipFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/HBaseRESTTestingUtility.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestStatusResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestMultiRowResource.java
  AL    src/test/java/org/apache/hadoop/hbase/rest/TestVersionResource.java
  AL    src/test/java/org/apache/hadoop/hbase/TestClusterBootOrder.java
  AL    src/test/java/org/apache/hadoop/hbase/TestFullLogReconstruction.java
  AL    src/test/java/org/apache/hadoop/hbase/PerformanceEvaluationCommons.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestMasterReplication.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationDisableInactivePeer.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationZookeeper.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationQueueFailover.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSource.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSink.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/ReplicationSourceDummy.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationQueueFailoverCompressed.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestMultiSlaveReplication.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
  AL    src/test/java/org/apache/hadoop/hbase/replication/TestReplicationBase.java
  AL    src/test/java/org/apache/hadoop/hbase/IngestIntegrationTestBase.java
  AL    src/test/java/org/apache/hadoop/hbase/TestCheckTestClasses.java
  AL    src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/MediumTests.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestFileLink.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestHeapSize.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileReaderV1.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/slab/TestSlab.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/slab/TestSlabCache.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/slab/TestSingleSizeCache.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/RandomSeek.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/KeySampler.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/NanoTimer.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileInlineToRootChunkConversion.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/KVGenerator.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFilePerformance.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestScannerSelectionUsingTTL.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestChecksum.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileSeek.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFile.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/RandomDistribution.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestBlockCacheColumnFamilySummary.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestLruBlockCache.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestForceCacheImportantBlocks.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestReseekTo.java
  AL    src/test/java/org/apache/hadoop/hbase/io/hfile/TestCachedBlockQueue.java
  AL    src/test/java/org/apache/hadoop/hbase/io/TestImmutableBytesWritable.java
  AL    src/test/java/org/apache/hadoop/hbase/io/encoding/TestChangingEncoding.java
  AL    src/test/java/org/apache/hadoop/hbase/io/encoding/TestUpgradeFromHFileV1ToEncoding.java
  AL    src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
  AL    src/test/java/org/apache/hadoop/hbase/io/encoding/TestLoadAndSwitchEncodeOnDisk.java
  AL    src/test/java/org/apache/hadoop/hbase/io/encoding/RedundantKVGenerator.java
  AL    src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
  AL    src/test/java/org/apache/hadoop/hbase/io/encoding/TestBufferedDataBlockEncoder.java
  AL    src/test/java/org/apache/hadoop/hbase/catalog/TestCatalogTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditor.java
  AL    src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditorNoCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestColumnPrefixFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestFilterList.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestDependentColumnFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestParseFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestColumnPaginationFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestRandomRowFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestMultipleColumnPrefixFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestPrefixFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestInclusiveStopFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestBitComparator.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestColumnCountGetFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestSingleColumnValueExcludeFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/filter/TestPageFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/ClusterManager.java
  AL    src/test/java/org/apache/hadoop/hbase/SmallTests.java
  AL    src/test/java/org/apache/hadoop/hbase/TestSerialization.java
  AL    src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
  AL    src/test/java/org/apache/hadoop/hbase/IntegrationTestsDriver.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormat.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/MapreduceTestingShim.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestSimpleTotalOrderPartitioner.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/NMapInputFormat.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableInputFormatScan.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableSplit.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TsvImporterCustomTestMapper.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java
  AL    src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHRegionLocation.java
  AL    src/test/java/org/apache/hadoop/hbase/executor/TestExecutorService.java
  AL    src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
  AL    src/test/java/org/apache/hadoop/hbase/migration/TestMigrationFrom090To092.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestAttributes.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestShell.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestScan.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestSnapshotsFromAdmin.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestHTableUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestResult.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestConnectionUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
  AL    src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdmin.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestHCM.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestMetaScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromAdmin.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestCloneSnapshotFromClient.java
  AL    src/test/java/org/apache/hadoop/hbase/client/HConnectionTestingUtility.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestGet.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideWithCoprocessor.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestHTablePool.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestSnapshotFromClient.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestMultipleTimestamps.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestPutDotHas.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestMetaMigrationRemovingHTD.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestOperation.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestHConnection.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestScannerTimeout.java
  AL    src/test/java/org/apache/hadoop/hbase/client/TestRestoreSnapshotFromClient.java
  AL    src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHServerAddress.java
  AL    src/test/java/org/apache/hadoop/hbase/constraint/AllPassConstraint.java
  AL    src/test/java/org/apache/hadoop/hbase/constraint/TestConstraints.java
  AL    src/test/java/org/apache/hadoop/hbase/constraint/CheckConfigurationConstraint.java
  AL    src/test/java/org/apache/hadoop/hbase/constraint/AllFailConstraint.java
  AL    src/test/java/org/apache/hadoop/hbase/constraint/TestConstraint.java
  AL    src/test/java/org/apache/hadoop/hbase/constraint/RuntimeFailConstraint.java
  AL    src/test/java/org/apache/hadoop/hbase/constraint/WorksConstraint.java
  AL    src/test/java/org/apache/hadoop/hbase/backup/TestHFileArchiving.java
  AL    src/test/java/org/apache/hadoop/hbase/HServerLoad092.java
  AL    src/test/java/org/apache/hadoop/hbase/ClassTestFinder.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java
  AL    src/test/java/org/apache/hadoop/hbase/security/TestUser.java
  AL    src/test/java/org/apache/hadoop/hbase/ClassFinder.java
  AL    src/test/java/org/apache/hadoop/hbase/MultithreadedTestUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiVersionConsistencyControl.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransaction.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMXBean.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/CreateRandomStoreFile.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestRpcMetrics.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestOpenRegionHandler.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/handler/TestCloseRegionHandler.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactSelection.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestRSKilledWhenMasterInitializing.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestServerCustomProtocol.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMiniBatchOperationInProgress.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionServerBulkLoad.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionInfo.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestCacheOnWriteInSchema.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/HFileReadWriteTest.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStore.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/metrics/TestSchemaConfigured.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/metrics/TestSchemaMetrics.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueSkipListSet.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionWithCoprocessor.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWildcardColumnTracker.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/NoOpScanPolicyObserver.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionOnCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMasterAddressManager.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHBase7051.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestColumnSeeking.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestCompactionState.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLRUDictionary.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/InstrumentedSequenceFileLogWriter.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogMethods.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogBench.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplitCompressed.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtilsForTests.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplayCompressed.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestKeyValueCompression.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestCompressor.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/wal/HLogPerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFileBlockCacheSummary.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestScanWithBloomError.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestMemStoreLAB.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksScanned.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestKeyValueScanFixture.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestBatchHRegionLockingAndWrites.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/KeyValueScanFixture.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/EncodedSeekPerformanceTest.java
  AL    src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionBusyWait.java
  AL    src/test/java/org/apache/hadoop/hbase/metrics/TestExponentiallyDecayingSample.java
  AL    src/test/java/org/apache/hadoop/hbase/metrics/TestExactCounterMetric.java
  AL    src/test/java/org/apache/hadoop/hbase/metrics/TestMetricsMBeanBase.java
  AL    src/test/java/org/apache/hadoop/hbase/metrics/TestMetricsHistogram.java
  AL    src/test/java/org/apache/hadoop/hbase/procedure/TestProcedure.java
  AL    src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedureControllers.java
  AL    src/test/java/org/apache/hadoop/hbase/procedure/TestZKProcedure.java
  AL    src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureCoordinator.java
  AL    src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureMember.java
  AL    src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
  AL    src/test/java/org/apache/hadoop/hbase/HBaseClusterManager.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHBaseTestingUtility.java
  AL    src/test/java/org/apache/hadoop/hbase/TestServerName.java
  AL    src/test/java/org/apache/hadoop/hbase/TestClassFinder.java
  AL    src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
  AL    src/test/java/org/apache/hadoop/hbase/IntegrationTestDataIngestWithChaosMonkey.java
  AL    src/test/java/org/apache/hadoop/hbase/TestDrainingServer.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestDynamicClassLoader.java
  AL    src/test/java/org/apache/hadoop/hbase/util/ChaosMonkey.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestPoolMap.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestHFileArchiveUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/util/HFileArchiveTestingUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestFSTableDescriptors.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitter.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadSequential.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadParallel.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestIdLock.java
  AL    src/test/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManagerTestHelper.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestIncrementingEnvironmentEdge.java
  AL    src/test/java/org/apache/hadoop/hbase/util/MockServer.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestByteBloomFilter.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestMiniClusterLoadEncoded.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestThreads.java
  AL    src/test/java/org/apache/hadoop/hbase/util/MultiThreadedAction.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestSizeBasedThrottler.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java
  AL    src/test/java/org/apache/hadoop/hbase/util/MultiThreadedReader.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestMergeTable.java
  AL    src/test/java/org/apache/hadoop/hbase/util/RestartMetaTest.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestFSVisitor.java
  AL    src/test/java/org/apache/hadoop/hbase/util/StoppableImplementation.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestRootPath.java
  AL    src/test/java/org/apache/hadoop/hbase/util/ClassLoaderTestHelper.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestFSHDFSUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsckComparator.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestRegionSplitCalculator.java
  AL    src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildHole.java
  AL    src/test/java/org/apache/hadoop/hbase/util/hbck/HbckTestingUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildOverlap.java
  AL    src/test/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRebuildTestCore.java
  AL    src/test/java/org/apache/hadoop/hbase/util/hbck/TestOfflineMetaRebuildBase.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestLoadTestKVGenerator.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestCompressionTest.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestDefaultEnvironmentEdge.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestByteBufferUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/util/MockRegionServerServices.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestCoprocessorScanPolicy.java
  AL    src/test/java/org/apache/hadoop/hbase/util/MultiThreadedWriter.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestEnvironmentEdgeManager.java
  AL    src/test/java/org/apache/hadoop/hbase/util/LoadTestKVGenerator.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestSortedCopyOnWriteSet.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestBase64.java
  AL    src/test/java/org/apache/hadoop/hbase/util/TestKeying.java
  AL    src/test/java/org/apache/hadoop/hbase/util/ProcessBasedLocalHBaseCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/TestCompare.java
  AL    src/test/java/org/apache/hadoop/hbase/TestInfoServers.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverBypass.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/example/TestBulkDeleteProtocol.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/example/TestZooKeeperScanPolicyObserver.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestBigDecimalColumnInterpreter.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/GenericEndpoint.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithAbort.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/GenericProtocol.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationProtocol.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorExceptionWithRemove.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestWALObserver.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/SampleRegionWALObserver.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/ColumnAggregationEndpoint.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithRemove.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestAggregateProtocol.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
  AL    src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorExceptionWithAbort.java
  AL    src/test/java/org/apache/hadoop/hbase/monitoring/TestTaskMonitor.java
  AL    src/test/java/org/apache/hadoop/hbase/monitoring/TestMemoryBoundedLogMessageBuffer.java
  AL    src/test/java/org/apache/hadoop/hbase/KeyValueTestUtil.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestReferenceRegionHFilesTask.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotLogSplitter.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestFlushSnapshotFromClient.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotTask.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreSnapshotHelper.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestExportSnapshot.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestRestoreFlushSnapshotFromClient.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestSnapshotDescriptionUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestWALReferenceTask.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
  AL    src/test/java/org/apache/hadoop/hbase/snapshot/TestCopyRecoveredEditsTask.java
  AL    src/test/java/org/apache/hadoop/hbase/TestFSTableDescriptorForceCreation.java
  AL    src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServer.java
  AL    src/test/java/org/apache/hadoop/hbase/thrift/TestThriftServerCmdLine.java
  AL    src/test/java/org/apache/hadoop/hbase/thrift/TestCallQueue.java
  AL    src/test/java/org/apache/hadoop/hbase/errorhandling/TestForeignExceptionDispatcher.java
  AL    src/test/java/org/apache/hadoop/hbase/errorhandling/TestTimeoutExceptionInjector.java
  AL    src/test/java/org/apache/hadoop/hbase/errorhandling/TestForeignExceptionSerialization.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMXBean.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
  AL    src/test/java/org/apache/hadoop/hbase/master/handler/TestTableDeleteFamilyHandler.java
  AL    src/test/java/org/apache/hadoop/hbase/master/handler/TestTableDescriptorModification.java
  AL    src/test/java/org/apache/hadoop/hbase/master/handler/TestCreateTableHandler.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestRestartCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMaster.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestHMasterRPCException.java
  AL    src/test/java/org/apache/hadoop/hbase/master/cleaner/TestCleanerChore.java
  AL    src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileCleaner.java
  AL    src/test/java/org/apache/hadoop/hbase/master/cleaner/TestHFileLinkCleaner.java
  AL    src/test/java/org/apache/hadoop/hbase/master/cleaner/TestLogsCleaner.java
  AL    src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
  AL    src/test/java/org/apache/hadoop/hbase/master/Mocking.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestZKBasedOpenCloseRegion.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestOpenedRegionHandler.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestDeadServer.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestClockSkewDetection.java
  AL    src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotHFileCleaner.java
  AL    src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotManager.java
  AL    src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotLogCleaner.java
  AL    src/test/java/org/apache/hadoop/hbase/master/snapshot/TestSnapshotFileCache.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterFileSystem.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterRestartAfterDisablingTable.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestDefaultLoadBalancer.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterZKSessionRecovery.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterTransitions.java
  AL    src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHTableDescriptor.java
  AL    src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java
  AL    src/test/java/org/apache/hadoop/hbase/HBaseCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
  AL    src/test/java/org/apache/hadoop/hbase/HFilePerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/thrift2/TestThriftHBaseServiceHandler.java
  AL    src/test/java/org/apache/hadoop/hbase/TestNodeHealthCheckChore.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHServerInfo.java
  AL    src/test/java/org/apache/hadoop/hbase/IntegrationTestingUtility.java
  AL    src/test/java/org/apache/hadoop/hbase/IntegrationTests.java
  AL    src/test/java/org/apache/hadoop/hbase/ResourceCheckerJUnitRule.java
  AL    src/test/java/org/apache/hadoop/hbase/TestHBaseFileSystem.java
  AL    src/test/java/org/apache/hadoop/hbase/TestLocalHBaseCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
  AL    src/test/java/org/apache/hadoop/hbase/ResourceChecker.java
  AL    src/test/java/org/apache/hadoop/hbase/MapFilePerformanceEvaluation.java
  AL    src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
  AL    src/test/java/org/apache/hadoop/hbase/mapred/TestTableInputFormat.java
  AL    src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
  AL    src/test/java/org/apache/hadoop/hbase/test/IntegrationTestBigLinkedList.java
  AL    src/test/java/org/apache/hadoop/hbase/ipc/TestProtocolExtension.java
  AL    src/test/java/org/apache/hadoop/hbase/ipc/TestPBOnWritableRpc.java
  AL    src/test/java/org/apache/hadoop/hbase/ipc/TestDelayedRpc.java
 
 *****************************************************
 Printing headers for files without AL header...
 
 
 =======================================================================
 ==.arcconfig
 =======================================================================
{
  &quot;project_id&quot; : &quot;hbase&quot;,
  &quot;conduit_uri&quot; : &quot;https://reviews.facebook.net/&quot;,
  &quot;copyright_holder&quot; : &quot;Apache Software Foundation&quot;,
  &quot;phutil_libraries&quot; : {
    &quot;arclib&quot; : &quot;.arc_jira_lib&quot;
  },
  &quot;arcanist_configuration&quot; : &quot;ArcJIRAConfiguration&quot;,
  &quot;jira_project&quot; : &quot;HBASE&quot;,
  &quot;jira_api_url&quot; : &quot;https://issues.apache.org/jira/si/&quot;
}

 =======================================================================
 ==conf/log4j.properties
 =======================================================================
# Define some default values that can be overridden by system properties
hbase.root.logger=INFO,console
hbase.security.logger=INFO,console
hbase.log.dir=.
hbase.log.file=hbase.log

# Define the root logger to the system property &quot;hbase.root.logger&quot;.
log4j.rootLogger=${hbase.root.logger}

# Logging Threshold
log4j.threshold=ALL

#
# Daily Rolling File Appender
#
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}

# Rollver at midnight
log4j.appender.DRFA.DatePattern=.yyyy-MM-dd

# 30-day backup
#log4j.appender.DRFA.MaxBackupIndex=30
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout

# Pattern format: Date LogLevel LoggerName LogMessage
log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

# Rolling File Appender properties
hbase.log.maxfilesize=256MB
hbase.log.maxbackupindex=20

# Rolling File Appender
log4j.appender.RFA=org.apache.log4j.RollingFileAppender
log4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}

log4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}
log4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}

log4j.appender.RFA.layout=org.apache.log4j.PatternLayout
log4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n

# Debugging Pattern format
#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n

#
# Security audit appender
#
hbase.security.log.file=SecurityAuth.audit
log4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender 

 =======================================================================
 ==conf/regionservers
 =======================================================================
localhost

 =======================================================================
 ==conf/hadoop-metrics.properties
 =======================================================================
# See http://wiki.apache.org/hadoop/GangliaMetrics
# Make sure you know whether you are using ganglia 3.0 or 3.1.
# If 3.1, you will have to patch your hadoop instance with HADOOP-4675
# And, yes, this file is named hadoop-metrics.properties rather than
# hbase-metrics.properties because we're leveraging the hadoop metrics
# package and hadoop-metrics.properties is an hardcoded-name, at least
# for the moment.
#
# See also http://hadoop.apache.org/hbase/docs/current/metrics.html
# GMETADHOST_IP is the hostname (or) IP address of the server on which the ganglia 
# meta daemon (gmetad) service is running

# Configuration of the &quot;hbase&quot; context for NullContextWithUpdateThread
# NullContextWithUpdateThread is a  null context which has a thread calling
# periodically when monitoring is started. This keeps the data sampled
# correctly.
hbase.class=org.apache.hadoop.metrics.spi.NullContextWithUpdateThread
hbase.period=10

# Configuration of the &quot;hbase&quot; context for file
# hbase.class=org.apache.hadoop.hbase.metrics.file.TimeStampingFileContext
# hbase.fileName=/tmp/metrics_hbase.log

# HBase-specific configuration to reset long-running stats (e.g. compactions)
# If this variable is left out, then the default is no expiration.
hbase.extendedperiod = 3600

# Configuration of the &quot;hbase&quot; context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# hbase.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# hbase.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# hbase.period=10
# hbase.servers=GMETADHOST_IP:8649

# Configuration of the &quot;jvm&quot; context for null
jvm.class=org.apache.hadoop.metrics.spi.NullContextWithUpdateThread
jvm.period=10

# Configuration of the &quot;jvm&quot; context for file
# jvm.class=org.apache.hadoop.hbase.metrics.file.TimeStampingFileContext
# jvm.fileName=/tmp/metrics_jvm.log

# Configuration of the &quot;jvm&quot; context for ganglia
# Pick one: Ganglia 3.0 (former) or Ganglia 3.1 (latter)
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext
# jvm.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
# jvm.period=10
# jvm.servers=GMETADHOST_IP:8649

# Configuration of the &quot;rpc&quot; context for null

 =======================================================================
 ==.git/packed-refs
 =======================================================================
# pack-refs with: peeled 
7a24ae7c1d18b2a404c5926e572db5c33193aaf2 refs/tags/release-0.2.1
^5d62dec3f327e848f341b56e4ed40018fab42b3a
364bab127d9afc42ba3d1100e8e91f76c6a6c3e3 refs/tags/release-0.2.0
^33f84956068207a2ef55fc81a5bdb6017ec844a1
859b2401b94036348f59821dc50c5b5901f1d749 refs/tags/release-0.18.0
^573831e549a870f62de619fcbeda8d451d351dcd
039a26b3c8b023cf2e1e5f57ebcd0fde510d74f2 refs/tags/pre-cdh4-base
1f2c6e82f5ed7ca895c67f8106e69bd3fee56ca6 refs/tags/original/cdh3u2-patch-base
cd5ea56ed028d2807f1e4c08fe755e22a481272f refs/tags/mvn-release-test-01
^2ebb5703964c3575140237c1ed6ce81bbb5c6dba
fa300c664d5004bfc2e9e35b55ef5196f2f3e689 refs/tags/hbase-0.95.2-hadoop2-testing
^1e85a0cba21529c3547be745d14a2c61848ac87c
45f71c1d23044431e149397eff3021adccec9aad refs/tags/hbase-0.95.2-hadoop2
^a7e9be80f33e9142dbbb79ec999336d0eefb3aa5
9c5196fa901ad240b9740b963609091b9705218b refs/tags/hbase-0.95.1-hadoop2.by.mvn3
^2a2b9fd7148182d4ed6de3acd53e47f0dcb70f76
3e34823d07d88422a8cd73e1933c7a9cf69d25de refs/tags/hbase-0.95.1-hadoop2.by.mvn
^4225e9563323622df59bf13b6056c420812514ce
eaa724c4104d8b74c441021146a48dfa83283b49 refs/tags/hbase-0.95.1-hadoop1
^ed65efd1b8b478653acaabd53433e6f3a383eb9b
3702326d022fd03256fab70d61ba3f5da686a5f7 refs/tags/hbase-0.95.0
^cd7d3efd567ee9c239f318b33db1e5addb78763c
4fd8ef39cabcc31e8d9b17c62ac1d8055187899c refs/tags/hbase-0.94.6.1
^164a57a9a3ea834361b6b9b1184d491f8149225e
e6dc806eaf7edd7b766f936a0c1c5f8c6590095f refs/tags/hbase-0.90.6RC1
^841f682637812500a3f47e72016373d5cb5e9d2e
dab3bc588fd01fd4a98dbc7ae2df87968b955bbe refs/tags/hbase-0.90.6-rc2
^904865052560d61aafcc70c856972ef6bb04536d
7a9d7b8edc099c5f099bf0ad547da6869b08ec4b refs/tags/hbase-0.90.6-RC3
^39d47d924bacb5a21cbf92fd6102d58d85b6104c
25e6a67b18e554ddab7fe39d003129a0cf3508ff refs/tags/hbase-0.20.0-RC2
^627d801e06499ed7028a856334e095a4db4c6705
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-release
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc9
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc8
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc7
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc6
af0889f0801b9ba5ca77b24567c53c982946ca34 refs/tags/cdh5.0.0b1-rc5
7d45dbc995e800a002e271ddd0838be3a320f3f4 refs/tags/cdh5.0.0b1-rc4
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc13
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc12
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc11
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 refs/tags/cdh5.0.0b1-rc10
142c55e115f1e2c7c7a42ea255e0455349873039 refs/tags/cdh5.0.0b1-rc1
f6f07ec6d0bc6958dfc1988625206ea26795df0f refs/tags/cdh5-base-0.95.2
^ebbf9d85843b421794bb55687af6964ff88d416f
5a0c8582b2fa5b31461a99f110740d44bb5145df refs/tags/cdh5-base-0.95.1
914d80ef94dc665bb1920ee8665a808c95e15284 refs/tags/cdh4b1-rc4
aa4ee07dc0255c1c74f22099eba8a2d0b8323fc5 refs/tags/cdh4b1-rc3

 =======================================================================
 ==.git/FETCH_HEAD
 =======================================================================
0cca221b34695776338749a3b020d30c69954f18		branch '0.1' of git://github.sf.cloudera.com/CDH/hbase
fb28a30e62cbee315421b435e7e97e0afe990642		branch '0.18' of git://github.sf.cloudera.com/CDH/hbase
e8e336df2dd751ac120815e20dad5e4e5560c24e		branch '0.19' of git://github.sf.cloudera.com/CDH/hbase
c13d5b09a6f17f893bcd451a61766a1298d82756		branch '0.19_on_hadoop_0.18' of git://github.sf.cloudera.com/CDH/hbase
97440fa69a7b6ce070be4548a6791fd3558ddc11		branch '0.2' of git://github.sf.cloudera.com/CDH/hbase
432ac58f49809264f7face72d511f7d3acd7399a		branch '0.20' of git://github.sf.cloudera.com/CDH/hbase
8654a23a95664de25712dfe8f4b2cf12fae9970e		branch '0.20-bulkload' of git://github.sf.cloudera.com/CDH/hbase
82fd71a47f2411f1b04d14f355a3ca48c155cc95		branch '0.20-testacidguarantees' of git://github.sf.cloudera.com/CDH/hbase
01a27794653294817a4e18484c1c72e4d2ab4ffa		branch '0.20_on_hadoop-0.18.3' of git://github.sf.cloudera.com/CDH/hbase
15ea14b60067e18a338dd93e209a23f56250fb5d		branch '0.20_on_hadoop-0.21' of git://github.sf.cloudera.com/CDH/hbase
df9083ae8ccec7d7a7ab9f0624442ba0a0e71e35		branch '0.89.20100621' of git://github.sf.cloudera.com/CDH/hbase
d8db5ee277809167cd2ffeb19d75a378ec90545d		branch '94rebase3' of git://github.sf.cloudera.com/CDH/hbase
1847b1b01f312ef134da8b9893afffab46df468e		branch 'andrew/sonar-test' of git://github.sf.cloudera.com/CDH/hbase
351fc40fdb77ce1e1c9c0a0f51782d106e988af0		branch 'apb-maven-proto' of git://github.sf.cloudera.com/CDH/hbase
bf9116c54b903efbedff1ea7ae5c1cb02b19076c		branch 'busted-cdh-0.90.4+49' of git://github.sf.cloudera.com/CDH/hbase
9a7b1c1676a054777f1f0c8179470cc5c51f8a24		branch 'cdh-0.89.20100621' of git://github.sf.cloudera.com/CDH/hbase
141178fbd79e618aff9dc8c328872d929a97bb01		branch 'cdh-0.89.20100924' of git://github.sf.cloudera.com/CDH/hbase
d7afdc6ad5bf3809630c121ecf8d7fdf1c1170d6		branch 'cdh-0.90' of git://github.sf.cloudera.com/CDH/hbase
b28b19919fad2fc899d574fdc82ab62ecdbd612b		branch 'cdh-0.90.0' of git://github.sf.cloudera.com/CDH/hbase
62870aca1f5886ea235b9177f57d720805f02986		branch 'cdh-0.90.1' of git://github.sf.cloudera.com/CDH/hbase
6142f8aa0d53af90be2291cea2eb751b51b5924e		branch 'cdh-0.90.1+15' of git://github.sf.cloudera.com/CDH/hbase
050e0e91e779b5909ce23932ad210e9eddba58cc		branch 'cdh-0.90.1+15-broken' of git://github.sf.cloudera.com/CDH/hbase
8b487c6cd5eb6bcca5d2a81108fcb70820a3dfed		branch 'cdh-0.90.1+15-orig' of git://github.sf.cloudera.com/CDH/hbase
e73f551ae288265ddfcb4b04b877f0c04e65053d		branch 'cdh-0.90.1+15.18' of git://github.sf.cloudera.com/CDH/hbase
c8ecf3b80a4a98962355d1cfec41f93f8aad0a1b		branch 'cdh-0.90.1+15.18+PATCH9' of git://github.sf.cloudera.com/CDH/hbase
0140046cd7dd29d72ae372c047f974b48c8b4e32		branch 'cdh-0.90.1-cdh3b4-rc' of git://github.sf.cloudera.com/CDH/hbase
769aa41e9cfec0e09ec35f8f9b251c2a439d3bf4		branch 'cdh-0.90.3' of git://github.sf.cloudera.com/CDH/hbase
644ed208303aa7fb514c216e8a2cbee57c1e2e29		branch 'cdh-0.90.3+15' of git://github.sf.cloudera.com/CDH/hbase
50421e7161ae9bb8d2846fac2e67cd2601efd7dc		branch 'cdh-0.90.3+15.3' of git://github.sf.cloudera.com/CDH/hbase
8cd98d9e794265e7554b31da9bb5ab1d784ca934		branch 'cdh-0.90.4' of git://github.sf.cloudera.com/CDH/hbase
433eb35f07408dcbcacad6720864f29fca948853		branch 'cdh-0.90.4+23' of git://github.sf.cloudera.com/CDH/hbase
3f6e90c6fd65daa3a139af1d2b69185bc9d03cac		branch 'cdh-0.90.4+23+ebaycassini' of git://github.sf.cloudera.com/CDH/hbase
e778739e9f4186787e8d3b85d31fb433ab355527		branch 'cdh-0.90.4+49' of git://github.sf.cloudera.com/CDH/hbase
515865c07f0b3dbe01322bc429b74d9b503292e3		branch 'cdh-0.90.4+49.1' of git://github.sf.cloudera.com/CDH/hbase
4b97cbfad4f75b26119f9e34bcb21471a863f2f4		branch 'cdh-0.90.4+49.1+PATCH10' of git://github.sf.cloudera.com/CDH/hbase
018c4b674589db83271171cdb90e631cd9f86f90		branch 'cdh-0.90.4+49.1+PATCH17' of git://github.sf.cloudera.com/CDH/hbase
ec4997f35769bfd338815fea789e2b7440849f8e		branch 'cdh-0.90.4+49.1+PATCH17+PATCH22' of git://github.sf.cloudera.com/CDH/hbase
b5ff71ccbf793cb00cb1411076e60dce04a5cb8e		branch 'cdh-0.90.4+49.1+PATCH17+PATCH22+PATCH23' of git://github.sf.cloudera.com/CDH/hbase
3b33dbe0f62a9929ccdd36dff5d37d0d1bf9d9b2		branch 'cdh-0.90.4+49.137' of git://github.sf.cloudera.com/CDH/hbase
234f55583463677014dd542fb73110d78e359cf9		branch 'cdh-0.90.4+49.137+PATCH26' of git://github.sf.cloudera.com/CDH/hbase
de209bc895bba085a839a72ebc89c442002276ea		branch 'cdh-0.90.4+49.137+PATCH30' of git://github.sf.cloudera.com/CDH/hbase
b7d6db6883253443be1839909b8cf22d3f902685		branch 'cdh-0.90.4+49.137+PATCH31' of git://github.sf.cloudera.com/CDH/hbase
cf268e35009fc206ac1f62fae57604c008ebe4c5		branch 'cdh-0.90.4+49_backup' of git://github.sf.cloudera.com/CDH/hbase
71aa7dd8d317a18803b42846b4edcc3f08d4b674		branch 'cdh-0.90.4+49_bda' of git://github.sf.cloudera.com/CDH/hbase
3495f6431dedce37e5105ae402437ee2ef0e2ea7		branch 'cdh-0.90.6' of git://github.sf.cloudera.com/CDH/hbase
be3870e46255611b98861c812fa510a2084a336f		branch 'cdh-0.90.6+84' of git://github.sf.cloudera.com/CDH/hbase
bc5f693f0efd0da86f4b72f07d4477564f74dc69		branch 'cdh-0.90.6+84.29' of git://github.sf.cloudera.com/CDH/hbase
284971f9617b28aa3ad62745bf97f8162e998f38		branch 'cdh-0.90.6+84.29+PATCH32' of git://github.sf.cloudera.com/CDH/hbase
5b897c2a27c0956ff10dd1960f616916e7c97184		branch 'cdh-0.90.6+84.29+PATCH32+PATCH33' of git://github.sf.cloudera.com/CDH/hbase
96b0f87ce20d46589330027366280ad9334020e1		branch 'cdh-0.90.6+84.29+PATCH32+PATCH33+PATCH39' of git://github.sf.cloudera.com/CDH/hbase

 =======================================================================
 ==.git/hooks/update.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to blocks unannotated tags from entering.
# Called by &quot;git receive-pack&quot; with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to &quot;update&quot;.
#
# Config
# ------
# hooks.allowunannotated
#   This boolean sets whether unannotated tags will be allowed into the
#   repository.  By default they won't be.
# hooks.allowdeletetag
#   This boolean sets whether deleting tags will be allowed in the
#   repository.  By default they won't be.
# hooks.allowmodifytag
#   This boolean sets whether a tag may be modified after creation. By default
#   it won't be.
# hooks.allowdeletebranch
#   This boolean sets whether deleting branches will be allowed in the
#   repository.  By default they won't be.
# hooks.denycreatebranch
#   This boolean sets whether remotely creating branches will be denied
#   in the repository.  By default this is allowed.
#

# --- Command line
refname=&quot;$1&quot;
oldrev=&quot;$2&quot;
newrev=&quot;$3&quot;

# --- Safety check
if [ -z &quot;$GIT_DIR&quot; ]; then
	echo &quot;Don't run this script from the command line.&quot; &gt;&amp;2
	echo &quot; (if you want, you could supply GIT_DIR then run&quot; &gt;&amp;2
	echo &quot;  $0 &lt;ref&gt; &lt;oldrev&gt; &lt;newrev&gt;)&quot; &gt;&amp;2
	exit 1
fi

if [ -z &quot;$refname&quot; -o -z &quot;$oldrev&quot; -o -z &quot;$newrev&quot; ]; then
	echo &quot;Usage: $0 &lt;ref&gt; &lt;oldrev&gt; &lt;newrev&gt;&quot; &gt;&amp;2
	exit 1
fi

# --- Config
allowunannotated=$(git config --bool hooks.allowunannotated)
allowdeletebranch=$(git config --bool hooks.allowdeletebranch)
denycreatebranch=$(git config --bool hooks.denycreatebranch)
allowdeletetag=$(git config --bool hooks.allowdeletetag)
allowmodifytag=$(git config --bool hooks.allowmodifytag)

 =======================================================================
 ==.git/hooks/post-receive.sample
 =======================================================================
#!/bin/sh
#
# An example hook script for the &quot;post-receive&quot; event.
#
# The &quot;post-receive&quot; script is run after receive-pack has accepted a pack
# and the repository has been updated.  It is passed arguments in through
# stdin in the form
#  &lt;oldrev&gt; &lt;newrev&gt; &lt;refname&gt;
# For example:
#  aa453216d1b3e49e7f6f98441fa56946ddcd6a20 68f7abf4e6f922807889f52bc043ecd31b79f814 refs/heads/master
#
# see contrib/hooks/ for a sample, or uncomment the next line and
# rename the file to &quot;post-receive&quot;.

#. /usr/share/git-core/contrib/hooks/post-receive-email

 =======================================================================
 ==.git/hooks/applypatch-msg.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to &quot;applypatch-msg&quot;.

. git-sh-setup
test -x &quot;$GIT_DIR/hooks/commit-msg&quot; &amp;&amp;
	exec &quot;$GIT_DIR/hooks/commit-msg&quot; ${1+&quot;$@&quot;}
:

 =======================================================================
 ==.git/hooks/pre-commit.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by &quot;git commit&quot; with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to &quot;pre-commit&quot;.

if git rev-parse --verify HEAD &gt;/dev/null 2&gt;&amp;1
then
	against=HEAD
else
	# Initial commit: diff against an empty tree object
	against=4b825dc642cb6eb9a060e54bf8d69288fbee4904
fi

# If you want to allow non-ascii filenames set this variable to true.
allownonascii=$(git config hooks.allownonascii)

# Cross platform projects tend to avoid non-ascii filenames; prevent
# them from being added to the repository. We exploit the fact that the
# printable range starts at the space character and ends with tilde.
if [ &quot;$allownonascii&quot; != &quot;true&quot; ] &amp;&amp;
	# Note that the use of brackets around a tr range is ok here, (it's
	# even required, for portability to Solaris 10's /usr/bin/tr), since
	# the square bracket bytes happen to fall in the designated range.
	test &quot;$(git diff --cached --name-only --diff-filter=A -z $against |
	  LC_ALL=C tr -d '[ -~]\0')&quot;
then
	echo &quot;Error: Attempt to add a non-ascii file name.&quot;
	echo
	echo &quot;This can cause problems if you want to work&quot;
	echo &quot;with people on other platforms.&quot;
	echo
	echo &quot;To be portable it is advisable to rename the file ...&quot;
	echo
	echo &quot;If you know what you are doing you can disable this&quot;
	echo &quot;check using:&quot;
	echo
	echo &quot;  git config hooks.allownonascii true&quot;
	echo
	exit 1
fi

exec git diff-index --check --cached $against --

 =======================================================================
 ==.git/hooks/prepare-commit-msg.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to prepare the commit log message.
# Called by &quot;git commit&quot; with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to &quot;prepare-commit-msg&quot;.

# This hook includes three examples.  The first comments out the
# &quot;Conflicts:&quot; part of a merge commit.
#
# The second includes the output of &quot;git diff --name-status -r&quot;
# into the message, just before the &quot;git status&quot; output.  It is
# commented because it doesn't cope with --amend or with squashed
# commits.
#
# The third example adds a Signed-off-by line to the message, that can
# still be edited.  This is rarely a good idea.

case &quot;$2,$3&quot; in
  merge,)
    /usr/bin/perl -i.bak -ne 's/^/# /, s/^# #/#/ if /^Conflicts/ .. /#/; print' &quot;$1&quot; ;;

# ,|template,)
#   /usr/bin/perl -i.bak -pe '
#      print &quot;\n&quot; . `git diff --cached --name-status -r`
#	 if /^#/ &amp;&amp; $first++ == 0' &quot;$1&quot; ;;

  *) ;;
esac

# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*&gt;\).*$/Signed-off-by: \1/p')
# grep -qs &quot;^$SOB&quot; &quot;$1&quot; || echo &quot;$SOB&quot; &gt;&gt; &quot;$1&quot;

 =======================================================================
 ==.git/hooks/post-commit.sample
 =======================================================================
#!/bin/sh
#
# An example hook script that is called after a successful
# commit is made.
#
# To enable this hook, rename this file to &quot;post-commit&quot;.

: Nothing

 =======================================================================
 ==.git/hooks/pre-applypatch.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to &quot;pre-applypatch&quot;.

. git-sh-setup
test -x &quot;$GIT_DIR/hooks/pre-commit&quot; &amp;&amp;
	exec &quot;$GIT_DIR/hooks/pre-commit&quot; ${1+&quot;$@&quot;}
:

 =======================================================================
 ==.git/hooks/post-update.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to &quot;post-update&quot;.

exec git update-server-info

 =======================================================================
 ==.git/hooks/pre-rebase.sample
 =======================================================================
#!/bin/sh
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The &quot;pre-rebase&quot; hook is run just before &quot;git rebase&quot; starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#
# $1 -- the upstream the series was forked from.
# $2 -- the branch being rebased (or empty when rebasing the current branch).
#
# This sample shows how to prevent topic branches that are already
# merged to 'next' branch from getting rebased, because allowing it
# would result in rebasing already published history.

publish=next
basebranch=&quot;$1&quot;
if test &quot;$#&quot; = 2
then
	topic=&quot;refs/heads/$2&quot;
else
	topic=`git symbolic-ref HEAD` ||
	exit 0 ;# we do not interrupt rebasing detached HEAD
fi

case &quot;$topic&quot; in
refs/heads/??/*)
	;;
*)
	exit 0 ;# we do not interrupt others.
	;;
esac

# Now we are dealing with a topic branch being rebased
# on top of master.  Is it OK to rebase it?

# Does the topic really exist?
git show-ref -q &quot;$topic&quot; || {
	echo &gt;&amp;2 &quot;No such branch $topic&quot;
	exit 1
}

# Is topic fully merged to master?
not_in_master=`git rev-list --pretty=oneline ^master &quot;$topic&quot;`
if test -z &quot;$not_in_master&quot;
then
	echo &gt;&amp;2 &quot;$topic is fully merged to master; better remove it.&quot;
	exit 1 ;# we could allow it, but there is no point.

 =======================================================================
 ==.git/hooks/commit-msg.sample
 =======================================================================
#!/bin/sh
#
# An example hook script to check the commit log message.
# Called by &quot;git commit&quot; with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to &quot;commit-msg&quot;.

# Uncomment the below to add a Signed-off-by line to the message.
# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
# hook is more suited to it.
#
# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*&gt;\).*$/Signed-off-by: \1/p')
# grep -qs &quot;^$SOB&quot; &quot;$1&quot; || echo &quot;$SOB&quot; &gt;&gt; &quot;$1&quot;

# This example catches duplicate Signed-off-by lines.

test &quot;&quot; = &quot;$(grep '^Signed-off-by: ' &quot;$1&quot; |
	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')&quot; || {
	echo &gt;&amp;2 Duplicate Signed-off-by lines.
	exit 1
}

 =======================================================================
 ==.git/logs/refs/remotes/origin/cdh5-0.96.0
 =======================================================================
e6378511ea61f932cfd74a9f9d577af545b58b29 ec31fdaaca38762369fc0003ceb741035f801b50 Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384813653 -0800	fetch -t git://github.sf.cloudera.com/CDH/hbase.git +refs/heads/*:refs/remotes/origin/*: fast-forward

 =======================================================================
 ==.git/logs/refs/heads/master
 =======================================================================
0000000000000000000000000000000000000000 ddcb524fe45ab24c414c5009cbd84636dcbf55c2 Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384813056 -0800	clone: from git://github.sf.cloudera.com/CDH/hbase.git

 =======================================================================
 ==.git/logs/HEAD
 =======================================================================
0000000000000000000000000000000000000000 ddcb524fe45ab24c414c5009cbd84636dcbf55c2 Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384813056 -0800	clone: from git://github.sf.cloudera.com/CDH/hbase.git
ddcb524fe45ab24c414c5009cbd84636dcbf55c2 dfd266a87df8d8063e1310e2c9bdcec468be848a Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384813057 -0800	checkout: moving from master to dfd266a87df8d8063e1310e2c9bdcec468be848a
dfd266a87df8d8063e1310e2c9bdcec468be848a 3fb283446fed48c245cd4b7d2370a0ec5654eeb3 Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384814016 -0800	checkout: moving from dfd266a87df8d8063e1310e2c9bdcec468be848a to 3fb283446fed48c245cd4b7d2370a0ec5654eeb3
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 dfd266a87df8d8063e1310e2c9bdcec468be848a Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384814085 -0800	checkout: moving from 3fb283446fed48c245cd4b7d2370a0ec5654eeb3 to dfd266a87df8d8063e1310e2c9bdcec468be848a
dfd266a87df8d8063e1310e2c9bdcec468be848a 3fb283446fed48c245cd4b7d2370a0ec5654eeb3 Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384816519 -0800	checkout: moving from dfd266a87df8d8063e1310e2c9bdcec468be848a to 3fb283446fed48c245cd4b7d2370a0ec5654eeb3
3fb283446fed48c245cd4b7d2370a0ec5654eeb3 dfd266a87df8d8063e1310e2c9bdcec468be848a Jenkins &lt;dev-kitchen@cloudera.com&gt; 1384816525 -0800	checkout: moving from 3fb283446fed48c245cd4b7d2370a0ec5654eeb3 to dfd266a87df8d8063e1310e2c9bdcec468be848a

 =======================================================================
 ==.git/config
 =======================================================================
[core]
	repositoryformatversion = 0
	filemode = true
	bare = false
	logallrefupdates = true
[remote &quot;origin&quot;]
	fetch = +refs/heads/*:refs/remotes/origin/*
	url = git://github.sf.cloudera.com/CDH/hbase.git
[branch &quot;master&quot;]
	remote = origin
	merge = refs/heads/master

 =======================================================================
 ==.git/refs/remotes/origin/cdh5-0.96.0
 =======================================================================
ec31fdaaca38762369fc0003ceb741035f801b50

 =======================================================================
 ==.git/refs/remotes/origin/HEAD
 =======================================================================
ref: refs/remotes/origin/master

 =======================================================================
 ==.git/refs/heads/master
 =======================================================================
ddcb524fe45ab24c414c5009cbd84636dcbf55c2

 =======================================================================
 ==.git/refs/tags/jenkins-Impala-HBase-dependency-21
 =======================================================================
d41c6601edee085d55f15ecbb691c8401e7e08a0

 =======================================================================
 ==.git/refs/tags/jenkins-Impala-HBase-dependency-15
 =======================================================================
2c40f8da5c0be90ff0c902181c159a264186c284

 =======================================================================
 ==.git/refs/tags/jenkins-Impala-HBase-dependency-19
 =======================================================================
066e1ec360772dfec0169ce64e0227d573a33ae0

 =======================================================================
 ==.git/refs/tags/jenkins-Impala-HBase-dependency-17
 =======================================================================
c9495ce4a8be0c2617207dbcc0decec2e4cb8a07

 =======================================================================
 ==.git/refs/tags/jenkins-Impala-HBase-dependency-18
 =======================================================================
3634426b3cf8ae68648b2dbe97d3c8df8ebe37d1

 =======================================================================
 ==.git/refs/tags/jenkins-Impala-HBase-dependency-20
 =======================================================================
efc5bc7b3eb54a6d08a4c19d37f76cbc4c9a1559

 =======================================================================
 ==.git/refs/tags/jenkins-Impala-HBase-dependency-16
 =======================================================================
667f19b7b53039307e466e3ae2d096aa0914d4dc

 =======================================================================
 ==.git/info/exclude
 =======================================================================
# git ls-files --others --exclude-from=.git/info/exclude
# Lines that start with '#' are comments.
# For a project mostly in C, the following would be a good set of
# exclude patterns (uncomment them if you want to use them):
# *.[oa]
# *~

 =======================================================================
 ==.git/HEAD
 =======================================================================
dfd266a87df8d8063e1310e2c9bdcec468be848a

 =======================================================================
 ==.git/description
 =======================================================================
Unnamed repository; edit this file 'description' to name the repository.

 =======================================================================
 ==.gitignore
 =======================================================================
/.arc_jira_lib
/.classpath
/.externalToolBuilders
/.project
/.settings
/build
/.idea/
/logs
/target
*.iml
*.orig
*~

 =======================================================================
 ==CHANGES.txt
 =======================================================================
HBase Change Log

Release 0.94.6 - 3/14/2013
Sub-task

    [HBASE-7944] - Replication leaks file reader resource &amp; not reset currentNbOperations

Bug

    [HBASE-6132] - ColumnCountGetFilter &amp; PageFilter not working with FilterList
    [HBASE-6347] - -ROOT- and .META. are stale in table.jsp if they moved
    [HBASE-6748] - Endless recursive of deleteNode happened in SplitLogManager#DeleteAsyncCallback
    [HBASE-7111] - hbase zkcli will not start if the zookeeper server chosen to connect to is unavailable
    [HBASE-7153] - print gc option in hbase-env.sh affects hbase zkcli
    [HBASE-7507] - Make memstore flush be able to retry after exception
    [HBASE-7521] - fix HBASE-6060 (regions stuck in opening state) in 0.94
    [HBASE-7624] - Backport HBASE-5359 and HBASE-7596 to 0.94
    [HBASE-7671] - Flushing memstore again after last failure could cause data loss
    [HBASE-7700] - TestColumnSeeking is mathematically bound to fail
    [HBASE-7723] - Remove NameNode URI from ZK splitlogs
    [HBASE-7725] - Add ability to create custom compaction request
    [HBASE-7761] - MemStore.USEMSLAB_DEFAULT is false, hbase-default.xml says it's true
    [HBASE-7763] - Compactions not sorting based on size anymore.
    [HBASE-7768] - zkcluster in local mode not seeing configurations in hbase-{site|default}.xml
    [HBASE-7777] - HBCK check for lingering split parents should check for child regions
    [HBASE-7813] - Bug in BulkDeleteEndpoint kills entire rows on COLUMN/VERSION Deletes
    [HBASE-7814] - Port HBASE-6963 'unable to run hbck on a secure cluster' to 0.94
    [HBASE-7829] - zookeeper kerberos conf keytab and principal parameters interchanged
    [HBASE-7832] - Use User.getShortName() in FSUtils
    [HBASE-7833] - 0.94 does not compile with Hadoop-0.20.205 and 0.22.0
    [HBASE-7851] - Include the guava classes as a dependency for jobs using mapreduce.TableMapReduceUtil
    [HBASE-7866] - TestSplitTransactionOnCluster.testSplitBeforeSettingSplittingInZK failed 3 times in a row
    [HBASE-7867] - setPreallocSize is different with COMMENT in setupTestEnv in MiniZooKeeperCluster.java
    [HBASE-7869] - Provide way to not start LogSyncer thread
    [HBASE-7876] - Got exception when manually triggers a split on an empty region
    [HBASE-7883] - Update memstore size when removing the entries in append operation
    [HBASE-7884] - ByteBloomFilter's performance can be improved by avoiding multiplication when generating hash
    [HBASE-7913] - Secure Rest server should login before getting an instance of Rest servlet
    [HBASE-7914] - Port the fix of HBASE-6748 into 0.94 branch
    [HBASE-7915] - Secure ThriftServer needs to login before calling HBaseHandler
    [HBASE-7916] - HMaster uses wrong InetSocketAddress parameter to throw exception
    [HBASE-7919] - Wrong key is used in ServerManager#getServerConnection() to retrieve from Map serverConnections
    [HBASE-7920] - Move isFamilyEssential(byte[] name) out of Filter interface in 0.94
    [HBASE-7945] - Remove flaky TestCatalogTrackerOnCluster
    [HBASE-7986] - [REST] Make HTablePool size configurable
    [HBASE-7991] - Backport HBASE-6479 'HFileReaderV1 caching the same parent META block could cause server abort when splitting' to 0.94
    [HBASE-8007] - Adopt TestLoadAndVerify from BigTop
    [HBASE-8019] - Port HBASE-7779 '[snapshot 130201 merge] Fix TestMultiParallel' to 0.94
    [HBASE-8025] - zkcli fails when SERVER_GC_OPTS is enabled
    [HBASE-8040] - Race condition in AM after HBASE-7521 (only 0.94)

 =======================================================================
 ==src/site/resources/css/freebsd_docbook.css
 =======================================================================
/*
 * Copyright (c) 2001, 2003, 2010 The FreeBSD Documentation Project
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * $FreeBSD: doc/share/misc/docbook.css,v 1.15 2010/03/20 04:15:01 hrs Exp $
 */

BODY ADDRESS {
	line-height: 1.3;
	margin: .6em 0;
}

BODY BLOCKQUOTE {
	margin-top: .75em;
	line-height: 1.5;
	margin-bottom: .75em;
}

HTML BODY {
	margin: 1em 8% 1em 10%;
	line-height: 1.2;
}

.LEGALNOTICE {
	font-size: small;
	font-variant: small-caps;
}

BODY DIV {

 =======================================================================
 ==src/site/resources/images/hbase_logo.svg
 =======================================================================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;
&lt;!-- Generator: Adobe Illustrator 15.1.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  --&gt;

&lt;svg
   xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot;
   xmlns:cc=&quot;http://creativecommons.org/ns#&quot;
   xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;
   xmlns:svg=&quot;http://www.w3.org/2000/svg&quot;
   xmlns=&quot;http://www.w3.org/2000/svg&quot;
   xmlns:sodipodi=&quot;http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd&quot;
   xmlns:inkscape=&quot;http://www.inkscape.org/namespaces/inkscape&quot;
   version=&quot;1.1&quot;
   id=&quot;Layer_1&quot;
   x=&quot;0px&quot;
   y=&quot;0px&quot;
   width=&quot;792px&quot;
   height=&quot;612px&quot;
   viewBox=&quot;0 0 792 612&quot;
   enable-background=&quot;new 0 0 792 612&quot;
   xml:space=&quot;preserve&quot;
   inkscape:version=&quot;0.48.4 r9939&quot;
   sodipodi:docname=&quot;hbase_banner_logo.png&quot;
   inkscape:export-filename=&quot;hbase_logo_filledin.png&quot;
   inkscape:export-xdpi=&quot;90&quot;
   inkscape:export-ydpi=&quot;90&quot;&gt;&lt;metadata
   id=&quot;metadata3285&quot;&gt;&lt;rdf:RDF&gt;&lt;cc:Work
       rdf:about=&quot;&quot;&gt;&lt;dc:format&gt;image/svg+xml&lt;/dc:format&gt;&lt;dc:type
         rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;dc:title&gt;&lt;/dc:title&gt;&lt;/cc:Work&gt;&lt;/rdf:RDF&gt;&lt;/metadata&gt;&lt;defs
   id=&quot;defs3283&quot; /&gt;&lt;sodipodi:namedview
   pagecolor=&quot;#ffffff&quot;
   bordercolor=&quot;#666666&quot;
   borderopacity=&quot;1&quot;
   objecttolerance=&quot;10&quot;
   gridtolerance=&quot;10&quot;
   guidetolerance=&quot;10&quot;
   inkscape:pageopacity=&quot;0&quot;
   inkscape:pageshadow=&quot;2&quot;
   inkscape:window-width=&quot;1131&quot;
   inkscape:window-height=&quot;715&quot;
   id=&quot;namedview3281&quot;
   showgrid=&quot;false&quot;
   inkscape:zoom=&quot;4.3628026&quot;
   inkscape:cx=&quot;328.98554&quot;
   inkscape:cy=&quot;299.51695&quot;
   inkscape:window-x=&quot;752&quot;
   inkscape:window-y=&quot;456&quot;
   inkscape:window-maximized=&quot;0&quot;
   inkscape:current-layer=&quot;Layer_1&quot; /&gt;
&lt;path
   d=&quot;m 233.586,371.672 -9.895,0 0,-51.583 9.895,0 0,51.583 z m -9.77344,-51.59213 -0.12156,-31.94487 9.895,0 -0.0405,31.98539 z m -0.12156,51.59213 -9.896,0 0,-32.117 -63.584,0 0,32.117 -19.466,0 0,-83.537 19.466,0 0,31.954 55.128,0 8.457,0 9.896,0 0,51.583 z m 0,-83.537 -9.896,0 0,31.98539 10.01756,-0.0405 z&quot;

 =======================================================================
 ==src/site/resources/images/big_h_logo.svg
 =======================================================================
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;
&lt;!-- Generator: Adobe Illustrator 15.1.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  --&gt;

&lt;svg
   xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot;
   xmlns:cc=&quot;http://creativecommons.org/ns#&quot;
   xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;
   xmlns:svg=&quot;http://www.w3.org/2000/svg&quot;
   xmlns=&quot;http://www.w3.org/2000/svg&quot;
   xmlns:sodipodi=&quot;http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd&quot;
   xmlns:inkscape=&quot;http://www.inkscape.org/namespaces/inkscape&quot;
   version=&quot;1.1&quot;
   id=&quot;Layer_1&quot;
   x=&quot;0px&quot;
   y=&quot;0px&quot;
   width=&quot;792px&quot;
   height=&quot;612px&quot;
   viewBox=&quot;0 0 792 612&quot;
   enable-background=&quot;new 0 0 792 612&quot;
   xml:space=&quot;preserve&quot;
   inkscape:version=&quot;0.48.4 r9939&quot;
   sodipodi:docname=&quot;big_h_same_font_hbase3_logo.png&quot;
   inkscape:export-filename=&quot;big_h_bitmap.png&quot;
   inkscape:export-xdpi=&quot;90&quot;
   inkscape:export-ydpi=&quot;90&quot;&gt;&lt;metadata
   id=&quot;metadata3693&quot;&gt;&lt;rdf:RDF&gt;&lt;cc:Work
       rdf:about=&quot;&quot;&gt;&lt;dc:format&gt;image/svg+xml&lt;/dc:format&gt;&lt;dc:type
         rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;dc:title&gt;&lt;/dc:title&gt;&lt;/cc:Work&gt;&lt;/rdf:RDF&gt;&lt;/metadata&gt;&lt;defs
   id=&quot;defs3691&quot; /&gt;&lt;sodipodi:namedview
   pagecolor=&quot;#000000&quot;
   bordercolor=&quot;#666666&quot;
   borderopacity=&quot;1&quot;
   objecttolerance=&quot;10&quot;
   gridtolerance=&quot;10&quot;
   guidetolerance=&quot;10&quot;
   inkscape:pageopacity=&quot;0&quot;
   inkscape:pageshadow=&quot;2&quot;
   inkscape:window-width=&quot;1440&quot;
   inkscape:window-height=&quot;856&quot;
   id=&quot;namedview3689&quot;
   showgrid=&quot;false&quot;
   inkscape:zoom=&quot;2.1814013&quot;
   inkscape:cx=&quot;415.39305&quot;
   inkscape:cy=&quot;415.72702&quot;
   inkscape:window-x=&quot;1164&quot;
   inkscape:window-y=&quot;22&quot;
   inkscape:window-maximized=&quot;0&quot;
   inkscape:current-layer=&quot;Layer_1&quot; /&gt;



 =======================================================================
 ==src/packages/deb/hbase.control/conffile
 =======================================================================
/etc/hbase/hadoop-metrics.properties
/etc/hbase/hbase-env.sh
/etc/hbase/hbase-site.xml
/etc/hbase/log4j.properties
/etc/hbase/regionservers

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellSetMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: CellSetMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class CellSetMessage {
  private CellSetMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface CellSetOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // repeated .org.apache.hadoop.hbase.rest.protobuf.generated.CellSet.Row rows = 1;
    java.util.List&lt;org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.CellSet.Row&gt; 
        getRowsList();
    org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.CellSet.Row getRows(int index);
    int getRowsCount();
    java.util.List&lt;? extends org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.CellSet.RowOrBuilder&gt; 
        getRowsOrBuilderList();
    org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.CellSet.RowOrBuilder getRowsOrBuilder(
        int index);
  }
  public static final class CellSet extends
      com.google.protobuf.GeneratedMessage
      implements CellSetOrBuilder {
    // Use CellSet.newBuilder() to construct.
    private CellSet(Builder builder) {
      super(builder);
    }
    private CellSet(boolean noInit) {}
    
    private static final CellSet defaultInstance;
    public static CellSet getDefaultInstance() {
      return defaultInstance;
    }
    
    public CellSet getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_CellSet_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_CellSet_fieldAccessorTable;
    }

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/StorageClusterStatusMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: StorageClusterStatusMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class StorageClusterStatusMessage {
  private StorageClusterStatusMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface StorageClusterStatusOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // repeated .org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatus.Node liveNodes = 1;
    java.util.List&lt;org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.StorageClusterStatus.Node&gt; 
        getLiveNodesList();
    org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.StorageClusterStatus.Node getLiveNodes(int index);
    int getLiveNodesCount();
    java.util.List&lt;? extends org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.StorageClusterStatus.NodeOrBuilder&gt; 
        getLiveNodesOrBuilderList();
    org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.StorageClusterStatus.NodeOrBuilder getLiveNodesOrBuilder(
        int index);
    
    // repeated string deadNodes = 2;
    java.util.List&lt;String&gt; getDeadNodesList();
    int getDeadNodesCount();
    String getDeadNodes(int index);
    
    // optional int32 regions = 3;
    boolean hasRegions();
    int getRegions();
    
    // optional int32 requests = 4;
    boolean hasRequests();
    int getRequests();
    
    // optional double averageLoad = 5;
    boolean hasAverageLoad();
    double getAverageLoad();
  }
  public static final class StorageClusterStatus extends
      com.google.protobuf.GeneratedMessage
      implements StorageClusterStatusOrBuilder {
    // Use StorageClusterStatus.newBuilder() to construct.
    private StorageClusterStatus(Builder builder) {
      super(builder);
    }
    private StorageClusterStatus(boolean noInit) {}
    
    private static final StorageClusterStatus defaultInstance;

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/CellMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: CellMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class CellMessage {
  private CellMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface CellOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional bytes row = 1;
    boolean hasRow();
    com.google.protobuf.ByteString getRow();
    
    // optional bytes column = 2;
    boolean hasColumn();
    com.google.protobuf.ByteString getColumn();
    
    // optional int64 timestamp = 3;
    boolean hasTimestamp();
    long getTimestamp();
    
    // optional bytes data = 4;
    boolean hasData();
    com.google.protobuf.ByteString getData();
  }
  public static final class Cell extends
      com.google.protobuf.GeneratedMessage
      implements CellOrBuilder {
    // Use Cell.newBuilder() to construct.
    private Cell(Builder builder) {
      super(builder);
    }
    private Cell(boolean noInit) {}
    
    private static final Cell defaultInstance;
    public static Cell getDefaultInstance() {
      return defaultInstance;
    }
    
    public Cell getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.CellMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_Cell_descriptor;

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ScannerMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: ScannerMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class ScannerMessage {
  private ScannerMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface ScannerOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional bytes startRow = 1;
    boolean hasStartRow();
    com.google.protobuf.ByteString getStartRow();
    
    // optional bytes endRow = 2;
    boolean hasEndRow();
    com.google.protobuf.ByteString getEndRow();
    
    // repeated bytes columns = 3;
    java.util.List&lt;com.google.protobuf.ByteString&gt; getColumnsList();
    int getColumnsCount();
    com.google.protobuf.ByteString getColumns(int index);
    
    // optional int32 batch = 4;
    boolean hasBatch();
    int getBatch();
    
    // optional int64 startTime = 5;
    boolean hasStartTime();
    long getStartTime();
    
    // optional int64 endTime = 6;
    boolean hasEndTime();
    long getEndTime();
    
    // optional int32 maxVersions = 7;
    boolean hasMaxVersions();
    int getMaxVersions();
    
    // optional string filter = 8;
    boolean hasFilter();
    String getFilter();
  }
  public static final class Scanner extends
      com.google.protobuf.GeneratedMessage
      implements ScannerOrBuilder {
    // Use Scanner.newBuilder() to construct.

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/VersionMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: VersionMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class VersionMessage {
  private VersionMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface VersionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional string restVersion = 1;
    boolean hasRestVersion();
    String getRestVersion();
    
    // optional string jvmVersion = 2;
    boolean hasJvmVersion();
    String getJvmVersion();
    
    // optional string osVersion = 3;
    boolean hasOsVersion();
    String getOsVersion();
    
    // optional string serverVersion = 4;
    boolean hasServerVersion();
    String getServerVersion();
    
    // optional string jerseyVersion = 5;
    boolean hasJerseyVersion();
    String getJerseyVersion();
  }
  public static final class Version extends
      com.google.protobuf.GeneratedMessage
      implements VersionOrBuilder {
    // Use Version.newBuilder() to construct.
    private Version(Builder builder) {
      super(builder);
    }
    private Version(boolean noInit) {}
    
    private static final Version defaultInstance;
    public static Version getDefaultInstance() {
      return defaultInstance;
    }
    
    public Version getDefaultInstanceForType() {
      return defaultInstance;
    }

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableInfoMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: TableInfoMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class TableInfoMessage {
  private TableInfoMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface TableInfoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required string name = 1;
    boolean hasName();
    String getName();
    
    // repeated .org.apache.hadoop.hbase.rest.protobuf.generated.TableInfo.Region regions = 2;
    java.util.List&lt;org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.TableInfo.Region&gt; 
        getRegionsList();
    org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.TableInfo.Region getRegions(int index);
    int getRegionsCount();
    java.util.List&lt;? extends org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.TableInfo.RegionOrBuilder&gt; 
        getRegionsOrBuilderList();
    org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.TableInfo.RegionOrBuilder getRegionsOrBuilder(
        int index);
  }
  public static final class TableInfo extends
      com.google.protobuf.GeneratedMessage
      implements TableInfoOrBuilder {
    // Use TableInfo.newBuilder() to construct.
    private TableInfo(Builder builder) {
      super(builder);
    }
    private TableInfo(boolean noInit) {}
    
    private static final TableInfo defaultInstance;
    public static TableInfo getDefaultInstance() {
      return defaultInstance;
    }
    
    public TableInfo getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableInfo_descriptor;
    }
    

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableListMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: TableListMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class TableListMessage {
  private TableListMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface TableListOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // repeated string name = 1;
    java.util.List&lt;String&gt; getNameList();
    int getNameCount();
    String getName(int index);
  }
  public static final class TableList extends
      com.google.protobuf.GeneratedMessage
      implements TableListOrBuilder {
    // Use TableList.newBuilder() to construct.
    private TableList(Builder builder) {
      super(builder);
    }
    private TableList(boolean noInit) {}
    
    private static final TableList defaultInstance;
    public static TableList getDefaultInstance() {
      return defaultInstance;
    }
    
    public TableList getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableListMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableList_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hbase.rest.protobuf.generated.TableListMessage.internal_static_org_apache_hadoop_hbase_rest_protobuf_generated_TableList_fieldAccessorTable;
    }
    
    // repeated string name = 1;
    public static final int NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList name_;
    public java.util.List&lt;String&gt;

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/ColumnSchemaMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: ColumnSchemaMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class ColumnSchemaMessage {
  private ColumnSchemaMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface ColumnSchemaOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional string name = 1;
    boolean hasName();
    String getName();
    
    // repeated .org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchema.Attribute attrs = 2;
    java.util.List&lt;org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchema.Attribute&gt; 
        getAttrsList();
    org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchema.Attribute getAttrs(int index);
    int getAttrsCount();
    java.util.List&lt;? extends org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchema.AttributeOrBuilder&gt; 
        getAttrsOrBuilderList();
    org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchema.AttributeOrBuilder getAttrsOrBuilder(
        int index);
    
    // optional int32 ttl = 3;
    boolean hasTtl();
    int getTtl();
    
    // optional int32 maxVersions = 4;
    boolean hasMaxVersions();
    int getMaxVersions();
    
    // optional string compression = 5;
    boolean hasCompression();
    String getCompression();
  }
  public static final class ColumnSchema extends
      com.google.protobuf.GeneratedMessage
      implements ColumnSchemaOrBuilder {
    // Use ColumnSchema.newBuilder() to construct.
    private ColumnSchema(Builder builder) {
      super(builder);
    }
    private ColumnSchema(boolean noInit) {}
    
    private static final ColumnSchema defaultInstance;
    public static ColumnSchema getDefaultInstance() {

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/rest/protobuf/generated/TableSchemaMessage.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: TableSchemaMessage.proto

package org.apache.hadoop.hbase.rest.protobuf.generated;

public final class TableSchemaMessage {
  private TableSchemaMessage() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface TableSchemaOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional string name = 1;
    boolean hasName();
    String getName();
    
    // repeated .org.apache.hadoop.hbase.rest.protobuf.generated.TableSchema.Attribute attrs = 2;
    java.util.List&lt;org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.TableSchema.Attribute&gt; 
        getAttrsList();
    org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.TableSchema.Attribute getAttrs(int index);
    int getAttrsCount();
    java.util.List&lt;? extends org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.TableSchema.AttributeOrBuilder&gt; 
        getAttrsOrBuilderList();
    org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.TableSchema.AttributeOrBuilder getAttrsOrBuilder(
        int index);
    
    // repeated .org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchema columns = 3;
    java.util.List&lt;org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchema&gt; 
        getColumnsList();
    org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchema getColumns(int index);
    int getColumnsCount();
    java.util.List&lt;? extends org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchemaOrBuilder&gt; 
        getColumnsOrBuilderList();
    org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.ColumnSchemaOrBuilder getColumnsOrBuilder(
        int index);
    
    // optional bool inMemory = 4;
    boolean hasInMemory();
    boolean getInMemory();
    
    // optional bool readOnly = 5;
    boolean hasReadOnly();
    boolean getReadOnly();
  }
  public static final class TableSchema extends
      com.google.protobuf.GeneratedMessage
      implements TableSchemaOrBuilder {
    // Use TableSchema.newBuilder() to construct.
    private TableSchema(Builder builder) {

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: hbase.proto

package org.apache.hadoop.hbase.protobuf.generated;

public final class HBaseProtos {
  private HBaseProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface SnapshotDescriptionOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required string name = 1;
    boolean hasName();
    String getName();
    
    // optional string table = 2;
    boolean hasTable();
    String getTable();
    
    // optional int64 creationTime = 3 [default = 0];
    boolean hasCreationTime();
    long getCreationTime();
    
    // optional .SnapshotDescription.Type type = 4 [default = FLUSH];
    boolean hasType();
    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription.Type getType();
    
    // optional int32 version = 5;
    boolean hasVersion();
    int getVersion();
  }
  public static final class SnapshotDescription extends
      com.google.protobuf.GeneratedMessage
      implements SnapshotDescriptionOrBuilder {
    // Use SnapshotDescription.newBuilder() to construct.
    private SnapshotDescription(Builder builder) {
      super(builder);
    }
    private SnapshotDescription(boolean noInit) {}
    
    private static final SnapshotDescription defaultInstance;
    public static SnapshotDescription getDefaultInstance() {
      return defaultInstance;
    }
    
    public SnapshotDescription getDefaultInstanceForType() {
      return defaultInstance;
    }

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/protobuf/generated/ErrorHandlingProtos.java
 =======================================================================
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: ErrorHandling.proto

package org.apache.hadoop.hbase.protobuf.generated;

public final class ErrorHandlingProtos {
  private ErrorHandlingProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface StackTraceElementMessageOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional string declaringClass = 1;
    boolean hasDeclaringClass();
    String getDeclaringClass();
    
    // optional string methodName = 2;
    boolean hasMethodName();
    String getMethodName();
    
    // optional string fileName = 3;
    boolean hasFileName();
    String getFileName();
    
    // optional int32 lineNumber = 4;
    boolean hasLineNumber();
    int getLineNumber();
  }
  public static final class StackTraceElementMessage extends
      com.google.protobuf.GeneratedMessage
      implements StackTraceElementMessageOrBuilder {
    // Use StackTraceElementMessage.newBuilder() to construct.
    private StackTraceElementMessage(Builder builder) {
      super(builder);
    }
    private StackTraceElementMessage(boolean noInit) {}
    
    private static final StackTraceElementMessage defaultInstance;
    public static StackTraceElementMessage getDefaultInstance() {
      return defaultInstance;
    }
    
    public StackTraceElementMessage getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hbase.protobuf.generated.ErrorHandlingProtos.internal_static_StackTraceElementMessage_descriptor;

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/TCell.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * TCell - Used to transport a cell value (byte[]) and the timestamp it was
 * stored with together as a result for get and getRow methods. This promotes
 * the timestamp of a cell to a first-class value, making it easy to take
 * note of temporal data. Cell is used all the way from HStore up to HTable.
 */
public class TCell implements org.apache.thrift.TBase&lt;TCell, TCell._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TCell&quot;);

  private static final org.apache.thrift.protocol.TField VALUE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;value&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField TIMESTAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;timestamp&quot;, org.apache.thrift.protocol.TType.I64, (short)2);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TCellStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TCellTupleSchemeFactory());
  }


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/TScan.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.8.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A Scan object is used to specify scanner parameters when opening a scanner.
 */
public class TScan implements org.apache.thrift.TBase&lt;TScan, TScan._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TScan&quot;);

  private static final org.apache.thrift.protocol.TField START_ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;startRow&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField STOP_ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;stopRow&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField TIMESTAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;timestamp&quot;, org.apache.thrift.protocol.TType.I64, (short)3);
  private static final org.apache.thrift.protocol.TField COLUMNS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;columns&quot;, org.apache.thrift.protocol.TType.LIST, (short)4);
  private static final org.apache.thrift.protocol.TField CACHING_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;caching&quot;, org.apache.thrift.protocol.TType.I32, (short)5);
  private static final org.apache.thrift.protocol.TField FILTER_STRING_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;filterString&quot;, org.apache.thrift.protocol.TType.STRING, (short)6);
  private static final org.apache.thrift.protocol.TField BATCH_SIZE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;batchSize&quot;, org.apache.thrift.protocol.TType.I32, (short)7);
  private static final org.apache.thrift.protocol.TField SORT_COLUMNS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;sortColumns&quot;, org.apache.thrift.protocol.TType.BOOL, (short)8);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TScanStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TScanTupleSchemeFactory());
  }


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/TIncrement.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * For increments that are not incrementColumnValue
 * equivalents.
 */
public class TIncrement implements org.apache.thrift.TBase&lt;TIncrement, TIncrement._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TIncrement&quot;);

  private static final org.apache.thrift.protocol.TField TABLE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;table&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;row&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField COLUMN_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;column&quot;, org.apache.thrift.protocol.TType.STRING, (short)3);
  private static final org.apache.thrift.protocol.TField AMMOUNT_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;ammount&quot;, org.apache.thrift.protocol.TType.I64, (short)4);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TIncrementStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TIncrementTupleSchemeFactory());
  }


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/TRowResult.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Holds row name and then a map of columns to cells.
 */
public class TRowResult implements org.apache.thrift.TBase&lt;TRowResult, TRowResult._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TRowResult&quot;);

  private static final org.apache.thrift.protocol.TField ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;row&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField COLUMNS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;columns&quot;, org.apache.thrift.protocol.TType.MAP, (short)2);
  private static final org.apache.thrift.protocol.TField SORTED_COLUMNS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;sortedColumns&quot;, org.apache.thrift.protocol.TType.LIST, (short)3);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TRowResultStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TRowResultTupleSchemeFactory());
  }

  public ByteBuffer row; // required
  public Map&lt;ByteBuffer,TCell&gt; columns; // optional

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class Hbase {

  public interface Iface {

    /**
     * Brings a table on-line (enables it)
     * 
     * @param tableName name of the table
     */
    public void enableTable(ByteBuffer tableName) throws IOError, org.apache.thrift.TException;

    /**
     * Disables a table (takes it off-line) If it is being served, the master
     * will tell the servers to stop serving it.
     * 
     * @param tableName name of the table
     */
    public void disableTable(ByteBuffer tableName) throws IOError, org.apache.thrift.TException;

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/IllegalArgument.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * An IllegalArgument exception indicates an illegal or invalid
 * argument was passed into a procedure.
 */
public class IllegalArgument extends TException implements org.apache.thrift.TBase&lt;IllegalArgument, IllegalArgument._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;IllegalArgument&quot;);

  private static final org.apache.thrift.protocol.TField MESSAGE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;message&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new IllegalArgumentStandardSchemeFactory());
    schemes.put(TupleScheme.class, new IllegalArgumentTupleSchemeFactory());
  }

  public String message; // required

  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/ColumnDescriptor.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * An HColumnDescriptor contains information about a column family
 * such as the number of versions, compression settings, etc. It is
 * used as input when creating a table or adding a column.
 */
public class ColumnDescriptor implements org.apache.thrift.TBase&lt;ColumnDescriptor, ColumnDescriptor._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;ColumnDescriptor&quot;);

  private static final org.apache.thrift.protocol.TField NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;name&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField MAX_VERSIONS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;maxVersions&quot;, org.apache.thrift.protocol.TType.I32, (short)2);
  private static final org.apache.thrift.protocol.TField COMPRESSION_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;compression&quot;, org.apache.thrift.protocol.TType.STRING, (short)3);
  private static final org.apache.thrift.protocol.TField IN_MEMORY_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;inMemory&quot;, org.apache.thrift.protocol.TType.BOOL, (short)4);
  private static final org.apache.thrift.protocol.TField BLOOM_FILTER_TYPE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;bloomFilterType&quot;, org.apache.thrift.protocol.TType.STRING, (short)5);
  private static final org.apache.thrift.protocol.TField BLOOM_FILTER_VECTOR_SIZE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;bloomFilterVectorSize&quot;, org.apache.thrift.protocol.TType.I32, (short)6);
  private static final org.apache.thrift.protocol.TField BLOOM_FILTER_NB_HASHES_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;bloomFilterNbHashes&quot;, org.apache.thrift.protocol.TType.I32, (short)7);
  private static final org.apache.thrift.protocol.TField BLOCK_CACHE_ENABLED_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;blockCacheEnabled&quot;, org.apache.thrift.protocol.TType.BOOL, (short)8);
  private static final org.apache.thrift.protocol.TField TIME_TO_LIVE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;timeToLive&quot;, org.apache.thrift.protocol.TType.I32, (short)9);


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/IOError.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * An IOError exception signals that an error occurred communicating
 * to the Hbase master or an Hbase region server.  Also used to return
 * more general Hbase error conditions.
 */
public class IOError extends TException implements org.apache.thrift.TBase&lt;IOError, IOError._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;IOError&quot;);

  private static final org.apache.thrift.protocol.TField MESSAGE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;message&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new IOErrorStandardSchemeFactory());
    schemes.put(TupleScheme.class, new IOErrorTupleSchemeFactory());
  }

  public String message; // required


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A BatchMutation object is used to apply a number of Mutations to a single row.
 */
public class BatchMutation implements org.apache.thrift.TBase&lt;BatchMutation, BatchMutation._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;BatchMutation&quot;);

  private static final org.apache.thrift.protocol.TField ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;row&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField MUTATIONS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;mutations&quot;, org.apache.thrift.protocol.TType.LIST, (short)2);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new BatchMutationStandardSchemeFactory());
    schemes.put(TupleScheme.class, new BatchMutationTupleSchemeFactory());
  }

  public ByteBuffer row; // required
  public List&lt;Mutation&gt; mutations; // required


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A TRegionInfo contains information about an HTable region.
 */
public class TRegionInfo implements org.apache.thrift.TBase&lt;TRegionInfo, TRegionInfo._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TRegionInfo&quot;);

  private static final org.apache.thrift.protocol.TField START_KEY_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;startKey&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField END_KEY_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;endKey&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField ID_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;id&quot;, org.apache.thrift.protocol.TType.I64, (short)3);
  private static final org.apache.thrift.protocol.TField NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;name&quot;, org.apache.thrift.protocol.TType.STRING, (short)4);
  private static final org.apache.thrift.protocol.TField VERSION_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;version&quot;, org.apache.thrift.protocol.TType.BYTE, (short)5);
  private static final org.apache.thrift.protocol.TField SERVER_NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;serverName&quot;, org.apache.thrift.protocol.TType.STRING, (short)6);
  private static final org.apache.thrift.protocol.TField PORT_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;port&quot;, org.apache.thrift.protocol.TType.I32, (short)7);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TRegionInfoStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TRegionInfoTupleSchemeFactory());

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/Mutation.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A Mutation object is used to either update or delete a column-value.
 */
public class Mutation implements org.apache.thrift.TBase&lt;Mutation, Mutation._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;Mutation&quot;);

  private static final org.apache.thrift.protocol.TField IS_DELETE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;isDelete&quot;, org.apache.thrift.protocol.TType.BOOL, (short)1);
  private static final org.apache.thrift.protocol.TField COLUMN_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;column&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField VALUE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;value&quot;, org.apache.thrift.protocol.TType.STRING, (short)3);
  private static final org.apache.thrift.protocol.TField WRITE_TO_WAL_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;writeToWAL&quot;, org.apache.thrift.protocol.TType.BOOL, (short)4);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new MutationStandardSchemeFactory());
    schemes.put(TupleScheme.class, new MutationTupleSchemeFactory());
  }

  public boolean isDelete; // required

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/TColumn.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.8.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Holds column name and the cell.
 */
public class TColumn implements org.apache.thrift.TBase&lt;TColumn, TColumn._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TColumn&quot;);

  private static final org.apache.thrift.protocol.TField COLUMN_NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;columnName&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField CELL_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;cell&quot;, org.apache.thrift.protocol.TType.STRUCT, (short)2);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TColumnStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TColumnTupleSchemeFactory());
  }

  public ByteBuffer columnName; // required
  public TCell cell; // required

  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
  public enum _Fields implements org.apache.thrift.TFieldIdEnum {
    COLUMN_NAME((short)1, &quot;columnName&quot;),

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift/generated/AlreadyExists.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * An AlreadyExists exceptions signals that a table with the specified
 * name already exists
 */
public class AlreadyExists extends TException implements org.apache.thrift.TBase&lt;AlreadyExists, AlreadyExists._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;AlreadyExists&quot;);

  private static final org.apache.thrift.protocol.TField MESSAGE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;message&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new AlreadyExistsStandardSchemeFactory());
    schemes.put(TupleScheme.class, new AlreadyExistsTupleSchemeFactory());
  }

  public String message; // required

  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TResult.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * if no Result is found, row and columnValues will not be set.
 */
public class TResult implements org.apache.thrift.TBase&lt;TResult, TResult._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TResult&quot;);

  private static final org.apache.thrift.protocol.TField ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;row&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField COLUMN_VALUES_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;columnValues&quot;, org.apache.thrift.protocol.TType.LIST, (short)2);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TResultStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TResultTupleSchemeFactory());
  }

  public ByteBuffer row; // optional
  public List&lt;TColumnValue&gt; columnValues; // required


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TScan.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Any timestamps in the columns are ignored, use timeRange to select by timestamp.
 * Max versions defaults to 1.
 */
public class TScan implements org.apache.thrift.TBase&lt;TScan, TScan._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TScan&quot;);

  private static final org.apache.thrift.protocol.TField START_ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;startRow&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField STOP_ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;stopRow&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField COLUMNS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;columns&quot;, org.apache.thrift.protocol.TType.LIST, (short)3);
  private static final org.apache.thrift.protocol.TField CACHING_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;caching&quot;, org.apache.thrift.protocol.TType.I32, (short)4);
  private static final org.apache.thrift.protocol.TField MAX_VERSIONS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;maxVersions&quot;, org.apache.thrift.protocol.TType.I32, (short)5);
  private static final org.apache.thrift.protocol.TField TIME_RANGE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;timeRange&quot;, org.apache.thrift.protocol.TType.STRUCT, (short)6);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TScanStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TScanTupleSchemeFactory());

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Used to perform Increment operations for a single row.
 * 
 * You can specify if this Increment should be written
 * to the write-ahead Log (WAL) or not. It defaults to true.
 */
public class TIncrement implements org.apache.thrift.TBase&lt;TIncrement, TIncrement._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TIncrement&quot;);

  private static final org.apache.thrift.protocol.TField ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;row&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField COLUMNS_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;columns&quot;, org.apache.thrift.protocol.TType.LIST, (short)2);
  private static final org.apache.thrift.protocol.TField WRITE_TO_WAL_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;writeToWal&quot;, org.apache.thrift.protocol.TType.BOOL, (short)3);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TIncrementStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TIncrementTupleSchemeFactory());
  }

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TPut.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Used to perform Put operations for a single row.
 * 
 * Add column values to this object and they'll be added.
 * You can provide a default timestamp if the column values
 * don't have one. If you don't provide a default timestamp
 * the current time is inserted.
 * 
 * You can also specify if this Put should be written
 * to the write-ahead Log (WAL) or not. It defaults to true.
 */
public class TPut implements org.apache.thrift.TBase&lt;TPut, TPut._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TPut&quot;);

  private static final org.apache.thrift.protocol.TField ROW_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;row&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField COLUMN_VALUES_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;columnValues&quot;, org.apache.thrift.protocol.TType.LIST, (short)2);
  private static final org.apache.thrift.protocol.TField TIMESTAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;timestamp&quot;, org.apache.thrift.protocol.TType.I64, (short)3);
  private static final org.apache.thrift.protocol.TField WRITE_TO_WAL_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;writeToWal&quot;, org.apache.thrift.protocol.TType.BOOL, (short)4);

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIllegalArgument.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A TIllegalArgument exception indicates an illegal or invalid
 * argument was passed into a procedure.
 */
public class TIllegalArgument extends TException implements org.apache.thrift.TBase&lt;TIllegalArgument, TIllegalArgument._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TIllegalArgument&quot;);

  private static final org.apache.thrift.protocol.TField MESSAGE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;message&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TIllegalArgumentStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TIllegalArgumentTupleSchemeFactory());
  }

  public String message; // optional

  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnValue.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Represents a single cell and its value.
 */
public class TColumnValue implements org.apache.thrift.TBase&lt;TColumnValue, TColumnValue._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TColumnValue&quot;);

  private static final org.apache.thrift.protocol.TField FAMILY_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;family&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField QUALIFIER_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;qualifier&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField VALUE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;value&quot;, org.apache.thrift.protocol.TType.STRING, (short)3);
  private static final org.apache.thrift.protocol.TField TIMESTAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;timestamp&quot;, org.apache.thrift.protocol.TType.I64, (short)4);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TColumnValueStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TColumnValueTupleSchemeFactory());
  }

  public ByteBuffer family; // required

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIOError.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * A TIOError exception signals that an error occurred communicating
 * to the HBase master or a HBase region server. Also used to return
 * more general HBase error conditions.
 */
public class TIOError extends TException implements org.apache.thrift.TBase&lt;TIOError, TIOError._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TIOError&quot;);

  private static final org.apache.thrift.protocol.TField MESSAGE_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;message&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TIOErrorStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TIOErrorTupleSchemeFactory());
  }

  public String message; // optional


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDelete.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Used to perform Delete operations on a single row.
 * 
 * The scope can be further narrowed down by specifying a list of
 * columns or column families as TColumns.
 * 
 * Specifying only a family in a TColumn will delete the whole family.
 * If a timestamp is specified all versions with a timestamp less than
 * or equal to this will be deleted. If no timestamp is specified the
 * current time will be used.
 * 
 * Specifying a family and a column qualifier in a TColumn will delete only
 * this qualifier. If a timestamp is specified only versions equal
 * to this timestamp will be deleted. If no timestamp is specified the
 * most recent version will be deleted.  To delete all previous versions,
 * specify the DELETE_COLUMNS TDeleteType.
 * 
 * The top level timestamp is only used if a complete row should be deleted

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnIncrement.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Represents a single cell and the amount to increment it by
 */
public class TColumnIncrement implements org.apache.thrift.TBase&lt;TColumnIncrement, TColumnIncrement._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TColumnIncrement&quot;);

  private static final org.apache.thrift.protocol.TField FAMILY_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;family&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField QUALIFIER_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;qualifier&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField AMOUNT_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;amount&quot;, org.apache.thrift.protocol.TType.I64, (short)3);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TColumnIncrementStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TColumnIncrementTupleSchemeFactory());
  }

  public ByteBuffer family; // required
  public ByteBuffer qualifier; // required

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TGet.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Used to perform Get operations on a single row.
 * 
 * The scope can be further narrowed down by specifying a list of
 * columns or column families.
 * 
 * To get everything for a row, instantiate a Get object with just the row to get.
 * To further define the scope of what to get you can add a timestamp or time range
 * with an optional maximum number of versions to return.
 * 
 * If you specify a time range and a timestamp the range is ignored.
 * Timestamps on TColumns are ignored.
 * 
 * TODO: Filter, Locks
 */
public class TGet implements org.apache.thrift.TBase&lt;TGet, TGet._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TGet&quot;);


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDeleteType.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;


import java.util.Map;
import java.util.HashMap;
import org.apache.thrift.TEnum;

/**
 * Specify type of delete:
 *  - DELETE_COLUMN means exactly one version will be removed,
 *  - DELETE_COLUMNS means previous versions will also be removed.
 */
public enum TDeleteType implements org.apache.thrift.TEnum {
  DELETE_COLUMN(0),
  DELETE_COLUMNS(1);

  private final int value;

  private TDeleteType(int value) {
    this.value = value;
  }

  /**
   * Get the integer value of this enum value, as defined in the Thrift IDL.
   */
  public int getValue() {
    return value;
  }

  /**
   * Find a the enum type by its integer value, as defined in the Thrift IDL.
   * @return null if the value is not found.
   */
  public static TDeleteType findByValue(int value) { 
    switch (value) {
      case 0:
        return DELETE_COLUMN;
      case 1:
        return DELETE_COLUMNS;
      default:
        return null;
    }
  }
}

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class THBaseService {

  public interface Iface {

    /**
     * Test for the existence of columns in the table, as specified in the TGet.
     * 
     * @return true if the specified TGet matches one or more keys, false if not
     * 
     * @param table the table to check on
     * 
     * @param get the TGet to check for
     */
    public boolean exists(ByteBuffer table, TGet get) throws TIOError, org.apache.thrift.TException;

    /**
     * Method for getting data from a row.
     * 

 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumn.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Addresses a single cell or multiple cells
 * in a HBase table by column family and optionally
 * a column qualifier and timestamp
 */
public class TColumn implements org.apache.thrift.TBase&lt;TColumn, TColumn._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TColumn&quot;);

  private static final org.apache.thrift.protocol.TField FAMILY_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;family&quot;, org.apache.thrift.protocol.TType.STRING, (short)1);
  private static final org.apache.thrift.protocol.TField QUALIFIER_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;qualifier&quot;, org.apache.thrift.protocol.TType.STRING, (short)2);
  private static final org.apache.thrift.protocol.TField TIMESTAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;timestamp&quot;, org.apache.thrift.protocol.TType.I64, (short)3);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TColumnStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TColumnTupleSchemeFactory());
  }


 =======================================================================
 ==src/main/java/org/apache/hadoop/hbase/thrift2/generated/TTimeRange.java
 =======================================================================
/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package org.apache.hadoop.hbase.thrift2.generated;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class TTimeRange implements org.apache.thrift.TBase&lt;TTimeRange, TTimeRange._Fields&gt;, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(&quot;TTimeRange&quot;);

  private static final org.apache.thrift.protocol.TField MIN_STAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;minStamp&quot;, org.apache.thrift.protocol.TType.I64, (short)1);
  private static final org.apache.thrift.protocol.TField MAX_STAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(&quot;maxStamp&quot;, org.apache.thrift.protocol.TType.I64, (short)2);

  private static final Map&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt; schemes = new HashMap&lt;Class&lt;? extends IScheme&gt;, SchemeFactory&gt;();
  static {
    schemes.put(StandardScheme.class, new TTimeRangeStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TTimeRangeTupleSchemeFactory());
  }

  public long minStamp; // required
  public long maxStamp; // required

  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
  public enum _Fields implements org.apache.thrift.TFieldIdEnum {
    MIN_STAMP((short)1, &quot;minStamp&quot;),
</pre></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
       <div class="xright">      
                
                 <span id="publishDate">Last Published: 2013-11-18</span>
              &nbsp;| <span id="projectVersion">Version: 0.94.6-cdh4.5.0</span>
            &nbsp;
        </div>
        <div class="xright">Copyright &#169;<a href="http://www.apache.org">                    2013
                        <a href="http://www.cloudera.com">Cloudera</a>.
            </a>All Rights Reserved.  Apache Hadoop, Hadoop, HDFS, HBase and the HBase project logo are trademarks of the Apache Software Foundation.
        </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
