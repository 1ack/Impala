From bec5270e0a017a53f7331f8b8cce784f41c3223d Mon Sep 17 00:00:00 2001
From: David S. Wang <dsw@cloudera.com>
Date: Wed, 2 May 2012 13:12:27 -0700
Subject: [PATCH 060/151] CDH-5597 hbase web ui displays misleading message about hdfs append
 Change append checks to sync and hflush checks.

Reason: Bug
Author: David S. Wang
Ref: CDH-5597
---
 .../hbase/tmpl/master/MasterStatusTmpl.jamon       |    7 ++-
 .../hadoop/hbase/master/MasterStatusServlet.java   |   18 +++++--
 .../hbase/regionserver/wal/HLogSplitter.java       |    2 +-
 .../org/apache/hadoop/hbase/util/FSHDFSUtils.java  |    7 +--
 .../org/apache/hadoop/hbase/util/FSMapRUtils.java  |    4 +-
 .../hadoop/hbase/util/FSTableDescriptors.java      |    1 -
 .../java/org/apache/hadoop/hbase/util/FSUtils.java |   58 ++++++++++----------
 .../apache/hadoop/hbase/util/RegionSplitter.java   |    2 +-
 .../hadoop/hbase/regionserver/wal/TestHLog.java    |    3 +-
 .../hbase/regionserver/wal/TestLogRollAbort.java   |    6 +-
 .../hbase/regionserver/wal/TestLogRolling.java     |   12 ++--
 .../org/apache/hadoop/hbase/util/TestFSUtils.java  |    6 +--
 12 files changed, 65 insertions(+), 61 deletions(-)

diff --git a/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon b/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon
index aca20b2..dc75ed7 100644
--- a/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon
+++ b/src/main/jamon/org/apache/hbase/tmpl/master/MasterStatusTmpl.jamon
@@ -25,7 +25,8 @@ ServerName rootLocation = null;
 ServerName metaLocation = null;
 List<ServerName> servers = null;
 Set<ServerName> deadServers = null;
-boolean showAppendWarning = false;
+boolean showSyncWarning = false;
+boolean showHflushWarning = false;
 String filter = "general";
 String format = "html";
 </%args>
@@ -77,9 +78,9 @@ org.apache.hadoop.hbase.HTableDescriptor;
   for details.
   </div>
 </%if>
-<%if showAppendWarning %> 
+<%if showSyncWarning && showHflushWarning %>
   <div class="warning">
-  You are currently running the HMaster without HDFS append support enabled.
+  You are currently running the HMaster without either HDFS sync or hflush support enabled.
   This may result in data loss.
   Please see the <a href="http://wiki.apache.org/hadoop/Hbase/HdfsSyncSupport">HBase wiki</a>
   for details.
diff --git a/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java b/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
index ef3e28b..acea0ea 100644
--- a/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
+++ b/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
@@ -65,7 +65,8 @@ public class MasterStatusServlet extends HttpServlet {
     response.setContentType("text/html");
     MasterStatusTmpl tmpl = new MasterStatusTmpl()
       .setFrags(frags)
-      .setShowAppendWarning(shouldShowAppendWarning(conf))
+      .setShowSyncWarning(shouldShowSyncWarning(conf))
+      .setShowHflushWarning(shouldShowHflushWarning(conf))
       .setRootLocation(rootLocation)
       .setMetaLocation(metaLocation)
       .setServers(servers)
@@ -98,11 +99,20 @@ public class MasterStatusServlet extends HttpServlet {
     }
   }
 
-  static boolean shouldShowAppendWarning(Configuration conf) {
+  static boolean shouldShowSyncWarning(Configuration conf) {
     try {
-      return !FSUtils.isAppendSupported(conf) && FSUtils.isHDFS(conf);
+      return !FSUtils.isSyncSupported() && FSUtils.isHDFS(conf);
     } catch (IOException e) {
-      LOG.warn("Unable to determine if append is supported", e);
+      LOG.warn("Unable to determine if sync is supported", e);
+      return false;
+    }
+  }
+
+  static boolean shouldShowHflushWarning(Configuration conf) {
+    try {
+      return !FSUtils.isHflushSupported() && FSUtils.isHDFS(conf);
+    } catch (IOException e) {
+      LOG.warn("Unable to determine if hflush is supported", e);
       return false;
     }
   }
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
index ae04ba2..e630c9b 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
@@ -754,7 +754,7 @@ public class HLogSplitter {
     }
 
     try {
-      FSUtils.getInstance(fs, conf).recoverFileLease(fs, path, conf);
+      FSUtils.getInstance(fs, conf).recoverFileLease(fs, path);
       try {
         in = getReader(fs, path, conf);
       } catch (EOFException e) {
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java b/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java
index 3d77879..b39952e 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java
@@ -25,7 +25,6 @@ import java.lang.reflect.InvocationTargetException;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -51,10 +50,10 @@ public class FSHDFSUtils extends FSUtils{
    */
   public static final long LEASE_SOFTLIMIT_PERIOD = 60 * 1000;
 
-  public void recoverFileLease(final FileSystem fs, final Path p, Configuration conf)
+  public void recoverFileLease(final FileSystem fs, final Path p)
   throws IOException{
-    if (!isAppendSupported(conf)) {
-      LOG.warn("Running on HDFS without append enabled may result in data loss");
+    if (!isSyncSupported() && !isHflushSupported()) {
+      LOG.warn("Running on HDFS without sync or hflush enabled may result in data loss");
       return;
     }
     // lease recovery not needed for local file system case.
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSMapRUtils.java b/src/main/java/org/apache/hadoop/hbase/util/FSMapRUtils.java
index e70b0d4..609c1d4 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSMapRUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSMapRUtils.java
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hbase.util;
 
 import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.permission.FsPermission;
@@ -32,8 +31,7 @@ import org.apache.commons.logging.LogFactory;
 public class FSMapRUtils extends FSUtils {
   private static final Log LOG = LogFactory.getLog(FSMapRUtils.class);
   
-  public void recoverFileLease(final FileSystem fs, final Path p, 
-      Configuration conf) throws IOException {
+  public void recoverFileLease(final FileSystem fs, final Path p) throws IOException {
     LOG.info("Recovering file " + p.toString() + 
       " by changing permission to readonly");
     FsPermission roPerm = new FsPermission((short) 0444);
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
index 8fbd557..04d7894 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java
@@ -41,7 +41,6 @@ import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableDescriptors;
-import org.apache.hadoop.hbase.TableExistsException;
 
 
 /**
diff --git a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
index 7dfbc15..bb7aa3c 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
@@ -767,36 +767,37 @@ public abstract class FSUtils {
   }
 
   /**
-   * Heuristic to determine whether is safe or not to open a file for append
-   * Looks both for dfs.support.append and use reflection to search
-   * for SequenceFile.Writer.syncFs() or FSDataOutputStream.hflush()
-   * @param conf
-   * @return True if append support
+   * Heuristic to determine whether is safe or not to open a file for sync
+   * Uses reflection to search for SequenceFile.Writer.syncFs()
+   * @return True if sync is supported
    */
-  public static boolean isAppendSupported(final Configuration conf) {
-    boolean append = conf.getBoolean("dfs.support.append", false);
-    if (append) {
-      try {
-        // TODO: The implementation that comes back when we do a createWriter
-        // may not be using SequenceFile so the below is not a definitive test.
-        // Will do for now (hdfs-200).
-        SequenceFile.Writer.class.getMethod("syncFs", new Class<?> []{});
-        append = true;
-      } catch (SecurityException e) {
-      } catch (NoSuchMethodException e) {
-        append = false;
-      }
+  public static boolean isSyncSupported() {
+    boolean sync = true;
+    try {
+      // TODO: The implementation that comes back when we do a createWriter
+      // may not be using SequenceFile so the below is not a definitive test.
+      // Will do for now (hdfs-200).
+      SequenceFile.Writer.class.getMethod("syncFs", new Class<?> []{});
+    } catch (SecurityException e) {
+    } catch (NoSuchMethodException e) {
+      sync = false;
     }
-    if (!append) {
-      // Look for the 0.21, 0.22, new-style append evidence.
-      try {
-        FSDataOutputStream.class.getMethod("hflush", new Class<?> []{});
-        append = true;
-      } catch (NoSuchMethodException e) {
-        append = false;
-      }
+    return sync;
+  }
+
+  /**
+   * Heuristic to determine whether is safe or not to open a file for hflush
+   * Uses reflection to search for FSDataOutputStream.hflush()
+   * @return True if hflush is supported
+   */
+  public static boolean isHflushSupported() {
+    boolean hflush = true;
+    try {
+      FSDataOutputStream.class.getMethod("hflush", new Class<?> []{});
+    } catch (NoSuchMethodException e) {
+      hflush = false;
     }
-    return append;
+    return hflush;
   }
 
   /**
@@ -818,8 +819,7 @@ public abstract class FSUtils {
    * @param conf Configuration handle
    * @throws IOException
    */
-  public abstract void recoverFileLease(final FileSystem fs, final Path p,
-      Configuration conf) throws IOException;
+  public abstract void recoverFileLease(final FileSystem fs, final Path p) throws IOException;
   
   /**
    * @param fs
diff --git a/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java b/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
index bad2c8e..74f4091 100644
--- a/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
+++ b/src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java
@@ -681,7 +681,7 @@ public class RegionSplitter {
     } else {
       LOG.debug("_balancedSplit file found. Replay log to restore state...");
       FSUtils.getInstance(fs, table.getConfiguration())
-        .recoverFileLease(fs, splitFile, table.getConfiguration());
+        .recoverFileLease(fs, splitFile);
 
       // parse split file and process remaining splits
       FSDataInputStream tmpIn = fs.open(splitFile);
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
index 16f0ee6..b565664 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
@@ -50,7 +50,6 @@ import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hdfs.protocol.FSConstants;
 import org.apache.hadoop.hdfs.server.datanode.DataNode;
-import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
 import org.apache.hadoop.hdfs.server.namenode.LeaseManager;
 import org.apache.hadoop.io.SequenceFile;
 import org.apache.log4j.Level;
@@ -413,7 +412,7 @@ public class TestHLog  {
       public void run() {
           try {
             FSUtils.getInstance(fs, rlConf)
-              .recoverFileLease(recoveredFs, walPath, rlConf);
+              .recoverFileLease(recoveredFs, walPath);
           } catch (IOException e) {
             exception = e;
           }
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
index 44396b2..e8ac4f0 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java
@@ -141,9 +141,9 @@ public class TestLogRollAbort {
     HLog log = server.getWAL();
 
     assertTrue("Need HDFS-826 for this test", log.canGetCurReplicas());
-    // don't run this test without append support (HDFS-200 & HDFS-142)
-    assertTrue("Need append support for this test",
-        FSUtils.isAppendSupported(TEST_UTIL.getConfiguration()));
+    // don't run this test without sync or hflush support
+    assertTrue("Need sync or hflush support for this test",
+        FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
 
     Put p = new Put(Bytes.toBytes("row2001"));
     p.add(HConstants.CATALOG_FAMILY, Bytes.toBytes("col"), Bytes.toBytes(2001));
diff --git a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
index 5ae22a6..b12f5d5 100644
--- a/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
+++ b/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
@@ -326,9 +326,9 @@ public class TestLogRolling  {
     this.log = server.getWAL();
 
     assertTrue("Need HDFS-826 for this test", log.canGetCurReplicas());
-    // don't run this test without append support (HDFS-200 & HDFS-142)
-    assertTrue("Need append support for this test", FSUtils
-        .isAppendSupported(TEST_UTIL.getConfiguration()));
+    // don't run this test without sync or hflush support 
+    assertTrue("Need sync or hflush support for this test",
+        FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
 
     // add up the datanode count, to ensure proper replication when we kill 1
     // This function is synchronous; when it returns, the dfs cluster is active
@@ -445,9 +445,9 @@ public class TestLogRolling  {
     });
 
     assertTrue("Need HDFS-826 for this test", log.canGetCurReplicas());
-    // don't run this test without append support (HDFS-200 & HDFS-142)
-    assertTrue("Need append support for this test", FSUtils
-        .isAppendSupported(TEST_UTIL.getConfiguration()));
+    // don't run this test without sync or hflush support 
+    assertTrue("Need sync or hflush support for this test",
+        FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
 
     writeData(table, 1002);
 
diff --git a/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java b/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
index aaddeca..566581f 100644
--- a/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
+++ b/src/test/java/org/apache/hadoop/hbase/util/TestFSUtils.java
@@ -37,14 +37,12 @@ import org.junit.Test;
 public class TestFSUtils {
   @Test public void testIsHDFS() throws Exception {
     HBaseTestingUtility htu = new HBaseTestingUtility();
-    htu.getConfiguration().setBoolean("dfs.support.append", false);
     assertFalse(FSUtils.isHDFS(htu.getConfiguration()));
-    htu.getConfiguration().setBoolean("dfs.support.append", true);
+    assertTrue(FSUtils.isSyncSupported() || FSUtils.isHflushSupported());
     MiniDFSCluster cluster = null;
     try {
       cluster = htu.startMiniDFSCluster(1);
       assertTrue(FSUtils.isHDFS(htu.getConfiguration()));
-      assertTrue(FSUtils.isAppendSupported(htu.getConfiguration()));
     } finally {
       if (cluster != null) cluster.shutdown();
     }
@@ -145,4 +143,4 @@ public class TestFSUtils {
     
   }
   
-}
\ No newline at end of file
+}
-- 
1.7.0.4

